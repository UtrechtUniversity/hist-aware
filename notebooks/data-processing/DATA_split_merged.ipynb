{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ea3315",
   "metadata": {},
   "source": [
    "# Split merged clean label files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ca5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import alpino\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/dutch.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f06fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb980e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/leonardovida/dev/hist-aware/notebooks/data/labeled-full/split_labeled/merged\"\n",
    "SAVE_DIR = \"/home/leonardovida/dev/hist-aware/notebooks/data/labeled-full/split_labeled/merged_split\"\n",
    "PRE_TRAINED_MODEL_NAME = 'wietsedv/bert-base-dutch-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb6069",
   "metadata": {},
   "source": [
    "### Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "537cb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "class BasicTokenizer(object):\n",
    "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
    "\n",
    "    def __init__(self, do_lower_case=False):\n",
    "        \"\"\"Constructs a BasicTokenizer.\n",
    "        Args:\n",
    "          do_lower_case: Whether to lower case the input.\n",
    "        \"\"\"\n",
    "        self.do_lower_case = do_lower_case\n",
    "\n",
    "    def tokenize(self, text, return_str=False):\n",
    "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "        text = self._clean_text(text)\n",
    "\n",
    "        orig_tokens = text.strip().split()\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if self.do_lower_case:\n",
    "                token = token.lower()\n",
    "                token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_punc(token))\n",
    "\n",
    "        return split_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text):\n",
    "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "\n",
    "def _is_whitespace(char):\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat in (\"Cc\", \"Cf\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    return cat.startswith(\"P\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7223162",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f179f",
   "metadata": {},
   "source": [
    "### Split text and explode new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59af1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(r):\n",
    "    out = []\n",
    "    sents = sent_detector.tokenize(r)\n",
    "    sents = [' '.join(tokenizer.tokenize(s)) for s in sents if len(s)>80]\n",
    "    return sents\n",
    "\n",
    "def apply_split_text(df):\n",
    "    df[\"text_split\"] = df[\"text\"].apply(split_text)\n",
    "    df.text_split.replace([], np.nan, inplace=True)\n",
    "    df.dropna(subset=['text_split'], inplace=True)\n",
    "    # Cancel all text_split == 0\n",
    "    df.drop(df[df.text_split.map(len) == 0].index, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9ef0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(344, 4)\n",
      "(344, 5)\n",
      "(2980, 5)\n",
      "(179, 4)\n",
      "(179, 5)\n",
      "(1728, 5)\n",
      "(500, 4)\n",
      "(500, 5)\n",
      "(6008, 5)\n",
      "(325, 4)\n",
      "(325, 5)\n",
      "(2844, 5)\n",
      "(511, 4)\n",
      "(511, 5)\n",
      "(5409, 5)\n",
      "(484, 4)\n",
      "(484, 5)\n",
      "(5905, 5)\n",
      "(337, 4)\n",
      "(332, 5)\n",
      "(1916, 5)\n",
      "(222, 4)\n",
      "(221, 5)\n",
      "(1335, 5)\n",
      "(193, 4)\n",
      "(192, 5)\n",
      "(1218, 5)\n"
     ]
    }
   ],
   "source": [
    "DECADES = [\"1970s\", \"1980s\", \"1990s\"]\n",
    "TYPES = [\"coal\", \"gas\", \"oil\"]\n",
    "\n",
    "for DECADE in DECADES:\n",
    "    for TYPE in TYPES:\n",
    "        name = f\"{DECADE}_{TYPE}_merged.csv\"\n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, name))\n",
    "        print(df.shape)\n",
    "        df = apply_split_text(df)\n",
    "        print(df.shape)\n",
    "        df = df.explode('text_split')\n",
    "        print(df.shape)\n",
    "        df.to_csv(os.path.join(SAVE_DIR, f\"{DECADE}_{TYPE}_merged_split.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f995501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histaware-NidRwJ64-py3.8",
   "language": "python",
   "name": "histaware-nidrwj64-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
