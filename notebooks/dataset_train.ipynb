{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "roman-antarctica",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distributed-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset as DT\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import wandb\n",
    "\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import alpino\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intensive-magazine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "twenty-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'wietsedv/bert-base-dutch-cased'\n",
    "LEN_SENTS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "overall-arrangement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997d7cb041f4469a86e820cd1dccfcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=241440.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-brake",
   "metadata": {},
   "source": [
    "## Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spatial-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols = ['sentiment', 'text', 'energy', 'article_filepath', 'article_name', 'count', 'date', 'dir', 'index_article', 'index_metadata', 'metadata_filepath',\n",
    "                    'newspaper_language', 'newspaper_publisher', 'newspaper_source', 'newspaper_title', 'newspaper_volume', \n",
    "             'newspaper_issuenumber', 'newspaper_city', 'text_clean', 'type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "engaging-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    \"\"\"Function to clean df after concat\"\"\"\n",
    "    df.text.replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    df.labels.replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['labels'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-invalid",
   "metadata": {},
   "source": [
    "### Gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "revised-psychology",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gas_1980 = pd.read_csv(\"~/dev/hist-aware/notebooks/sentiment/edo_1980s_gas.csv\")\n",
    "gas_1990 = pd.read_csv(\"~/dev/hist-aware/notebooks/sentiment/edo_1990s_gas_labeled.csv\")\n",
    "\n",
    "gas_1980 = gas_1980[list_cols]\n",
    "gas_1990 = gas_1990[list_cols]\n",
    "\n",
    "gas = gas_1980.append(gas_1990, ignore_index=True)\n",
    "gas = gas[gas.energy == \"Y\"]\n",
    "gas = gas[gas.sentiment != None]\n",
    "gas.rename(columns = {\"sentiment\": \"labels\"}, inplace=True)\n",
    "gas = clean_df(gas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-execution",
   "metadata": {},
   "source": [
    "### Oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "threaded-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_1980 = pd.read_csv(\"~/dev/hist-aware/notebooks/sentiment/edo_1980s_oil.csv\")\n",
    "oil_1990 = pd.read_csv(\"~/dev/hist-aware/notebooks/sentiment/edo_1990s_olie_labeled.csv\")\n",
    "\n",
    "oil_1980 = oil_1980[list_cols]\n",
    "oil_1990 = oil_1980[list_cols]\n",
    "\n",
    "oil = oil_1980.append(oil_1990, ignore_index=True)\n",
    "oil = oil[oil.energy == \"Y\"]\n",
    "oil = oil[oil.sentiment != None]\n",
    "oil.rename(columns = {\"sentiment\": \"labels\"}, inplace=True)\n",
    "oil = clean_df(oil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-guard",
   "metadata": {},
   "source": [
    "### Coal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "downtown-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "coal_1980 = pd.read_csv(\"~/dev/hist-aware/notebooks/sentiment/edo_1980s_coal.csv\")\n",
    "coal_1990 = pd.read_csv(\"~/dev/hist-aware/notebooks/sentiment/edo_1990s_kool_labeled.csv\")\n",
    "coal_1990.drop([\"sentiment_gas\", \"sentiment_oil\"], axis=1, inplace=True)\n",
    "coal_1990.rename(columns = {\"sentiment\": \"accuracy_selection\", \"sentiment_coal\": \"sentiment\"}, inplace=True)\n",
    "\n",
    "coal_1980 = coal_1980[list_cols]\n",
    "coal_1990 = coal_1990[list_cols]\n",
    "\n",
    "coal = coal_1980.append(coal_1990, ignore_index=True)\n",
    "coal = coal[coal.energy == \"Y\"]\n",
    "coal = coal[coal.sentiment != None]\n",
    "coal.rename(columns = {\"sentiment\": \"labels\"}, inplace=True)\n",
    "coal = clean_df(coal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-portugal",
   "metadata": {},
   "source": [
    "### General df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "single-concrete",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2773, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([gas, oil, coal], ignore_index=True)\n",
    "df = clean_df(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-fountain",
   "metadata": {},
   "source": [
    "## Fix labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "grave-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_sentiment = {\"labels\": {\"VN\": 1, \"NG\": 2, \"NE\": 3, \"PO\": 4, \"VP\": 5}}\n",
    "oil = oil.replace(cleanup_sentiment)\n",
    "gas = gas.replace(cleanup_sentiment)\n",
    "coal = coal.replace(cleanup_sentiment)\n",
    "df = df.replace(cleanup_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "middle-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.countplot(df.sentiment)\n",
    "#plt.xlabel('review sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-newman",
   "metadata": {},
   "source": [
    "Reduce from 5 labels to 3 because of lack of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wound-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentiment(rating):\n",
    "    rating = int(rating)\n",
    "    if rating <= 2:\n",
    "        return 0\n",
    "    elif rating == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df['labels'] = df.labels.apply(to_sentiment)\n",
    "gas['labels'] = gas.labels.apply(to_sentiment)\n",
    "coal['labels'] = coal.labels.apply(to_sentiment)\n",
    "oil['labels'] = oil.labels.apply(to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "handmade-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.countplot(df.sentiment)\n",
    "#plt.xlabel('review sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-rebel",
   "metadata": {},
   "source": [
    "### Split text and explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elegant-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unite(l, n):\n",
    "    \"\"\"Unite sentences previously split using nltk.tokenize.\"\"\"\n",
    "    count = []\n",
    "    chunks = []\n",
    "    sents = []\n",
    "    for s in l:\n",
    "        count.append(len(s.split()))\n",
    "    value = 0\n",
    "    prev_idx = 0\n",
    "    for i in range(0, len(count)):\n",
    "        if value == 0:\n",
    "            value = value + count[i]\n",
    "        elif (i+1 == len(count)):\n",
    "            chunks.append(l[prev_idx:i])\n",
    "            value = 0\n",
    "        elif value >= n:\n",
    "            chunks.append(l[prev_idx:i])\n",
    "            prev_idx = i\n",
    "            value = 0\n",
    "        else:\n",
    "             value = value + count[i]\n",
    "    for c in chunks:\n",
    "        sents.append(' '.join(c))\n",
    "    return(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "located-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(s, n):\n",
    "    \"\"\"Split sentences only using the number of words.\"\"\"\n",
    "    pieces = s.split()\n",
    "    return [\" \".join(pieces[i:i+n]) for i in range(0, len(pieces), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "transsexual-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_split_text(df):\n",
    "    df[\"text_split\"] = df[\"text\"].apply(sent_tokenize)\n",
    "    df[\"text_split\"] = df[\"text_split\"].apply(unite, n = LEN_SENTS)\n",
    "    df.text_split.replace([], np.nan, inplace=True)\n",
    "    df.dropna(subset=['text_split'], inplace=True)\n",
    "    # Cancel all text_split == 0\n",
    "    df.drop(df[df.text_split.map(len) == 0].index, inplace=True)\n",
    "    # Currently not splitting the cleaned sentences\n",
    "    #df[\"text_clean_split\"] = df[\"text_clean\"].apply(splitter, n = LEN_SENTS)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "strange-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil = apply_split_text(oil)\n",
    "gas = apply_split_text(gas)\n",
    "coal = apply_split_text(coal)\n",
    "df = apply_split_text(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-shore",
   "metadata": {},
   "source": [
    "Explode the sentences that we created previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dramatic-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('text_split')\n",
    "gas = gas.explode('text_split')\n",
    "coal = coal.explode('text_split')\n",
    "oil = oil.explode('text_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "developed-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"~/dev/hist-aware/notebooks/sentiment/df.csv\")\n",
    "#gas.to_csv(\"~/dev/hist-aware/notebooks/sentiment/gas.csv\")\n",
    "#coal.to_csv(\"~/dev/hist-aware/notebooks/sentiment/coal.csv\")\n",
    "#oil.to_csv(\"~/dev/hist-aware/notebooks/sentiment/oil.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-premises",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-valve",
   "metadata": {},
   "source": [
    "#### Using Dataset from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "tough-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DT.from_pandas(oil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "settled-tablet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b95c484b4ef428595968aa2728c0de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3232c46a272847c489a349a5e2f1c2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset = dataset.map(lambda par: tokenizer(\n",
    "    par['text_split'],\n",
    "    truncation=True,\n",
    "    padding=True),\n",
    "                      batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "vulnerable-insertion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29c4523001040e98ec265dc22011a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2686.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2b11e504b6455e83add0289df424d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=672.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset = dataset.map(lambda par: tokenizer.encode_plus(\n",
    "            par['text_split'],\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            max_length=350,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-relationship",
   "metadata": {},
   "source": [
    "#### Manual dataset creation with Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "rolled-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = df\n",
    "MAX_LEN = 350\n",
    "\n",
    "# Parameters\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "worse-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Divide into train and val\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df[\"text_split\"], df[\"labels\"], test_size=.3)\n",
    "# Divide into val and test\n",
    "test_texts, val_texts, test_labels, val_labels = train_test_split(val_texts, val_labels, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "italic-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HADataset(Dataset):\n",
    "    # Characterizes a dataset for Pytorch\n",
    "    def __init__(self, articles, labels, tokenizer, max_len):\n",
    "        # Initialization\n",
    "        self.articles = articles\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Total number of articles\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Generates one sample of the data/article\n",
    "        article = str(self.articles[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            article,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "        return {\n",
    "          'article_text': article,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "optical-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HADataset(train_texts.to_numpy(), train_labels.to_numpy(), tokenizer, MAX_LEN)\n",
    "val_dataset = HADataset(val_texts.to_numpy(), val_labels.to_numpy(), tokenizer, MAX_LEN)\n",
    "test_dataset = HADataset(test_texts.to_numpy(), test_labels.to_numpy(), tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "conscious-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, batch_size, num_workers, MAX_LEN):\n",
    "    ds = HADataset(\n",
    "        articles=df.text_split.to_numpy(),\n",
    "        labels=df.labels.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "      )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size,\n",
    "        num_workers\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "minute-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_data_loader = create_data_loader(DF, tokenizer, batch_size, num_workers, MAX_LEN)\n",
    "val_data_loader = create_data_loader(DF, tokenizer, batch_size, num_workers, MAX_LEN)\n",
    "test_data_loader = create_data_loader(DF, tokenizer, batch_size, num_workers, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-launch",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "complimentary-waterproof",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\nTypeError: forward() got an unexpected keyword argument 'labels'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-07a632cac371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    773\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1110\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mSubclass\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moverride\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \"\"\"\n\u001b[0;32m-> 1136\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\nTypeError: forward() got an unexpected keyword argument 'labels'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"hist-aware/notebooks/models\",\n",
    "    overwrite_output_dir = False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=4, # default is 8\n",
    "    per_device_eval_batch_size=4, # default is 8\n",
    "    \n",
    "    logging_dir=\"hist-aware/notebooks/logging\",\n",
    "    logging_steps=10,\n",
    "    \n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler    \n",
    "    eval_steps=500,\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    \n",
    "    label_names=\"labels\", # check this\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-castle",
   "metadata": {},
   "source": [
    "### Automatic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "sexual-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoModel, AutoTokenizer, AutoConfig,\n",
    "                          Trainer, TrainingArguments)\n",
    "\n",
    "config = AutoConfig.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "bert_model = AutoModel.from_pretrained(\n",
    "    PRE_TRAINED_MODEL_NAME,\n",
    "    num_labels = 3)\n",
    "    #output_attentions = False, # Whether the model returns attentions weights.\n",
    "    #output_hidden_states = False, # Whether the model returns all hidden-states\n",
    "    #return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "strange-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"hist-aware/notebooks/models\",\n",
    "    overwrite_output_dir = False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=4, # default is 8\n",
    "    per_device_eval_batch_size=4, # default is 8\n",
    "    logging_dir=\"hist-aware/notebooks/logging\",\n",
    "    eval_steps=500,\n",
    "    seed=RANDOM_SEED,\n",
    "    label_names=\"labels\", # check this\n",
    "    disable_tqdm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "sapphire-marathon",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "type of [1, 0, 2358, 15730, 22307, 11, 6, 22693, 10537, 30, 23225, 29566, 11, 0, 15266, 11315, 13903, 12005, 8472, 13, 6, 3570, 11552, 13903, 11315, 15266, 22976, 10806, 25, 13041, 20722, 10537, 21953, 13644, 14742, 11, 0, 11281, 18883, 13, 7529, 11037, 13903, 22339, 10532, 5108, 13644, 11130, 17898, 17721, 28515, 21843, 14545, 16348, 22867, 11788, 13, 7778, 22795, 26709, 22384, 17795, 20722, 11130, 20722, 16760, 22331, 15963, 18312, 27102, 11, 13261, 8048, 11, 16804, 22300, 9645, 19732, 13, 3631, 13261, 16058, 20722, 3137, 17572, 11, 10516, 13903, 13261, 22339, 20722, 2511, 20407, 24660, 26751, 113, 8, 0, 13, 2003, 25128, 22777, 5114, 20183, 25108, 16804, 11130, 13136, 16804, 13261, 22795, 17179, 117, 7826, 25206, 27533, 117, 131, 117, 22834, 13644, 1503, 8394, 23984, 8227, 24770, 23967, 8421, 0, 12847, 128, 25099, 26457, 26148, 13, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3] unknown: <class 'list'>. Should be one of a python, numpy, pytorch or tensorflow object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-5f6e962f0c74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m# Defaut objective is the sum of all metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# when metrics are provided, so we have to maximize it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         batch = self.tokenizer.pad(\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2528\u001b[0m                 \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"np\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2531\u001b[0m                     \u001b[0;34mf\"type of {first_element} unknown: {type(first_element)}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m                     \u001b[0;34mf\"Should be one of a python, numpy, pytorch or tensorflow object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: type of [1, 0, 2358, 15730, 22307, 11, 6, 22693, 10537, 30, 23225, 29566, 11, 0, 15266, 11315, 13903, 12005, 8472, 13, 6, 3570, 11552, 13903, 11315, 15266, 22976, 10806, 25, 13041, 20722, 10537, 21953, 13644, 14742, 11, 0, 11281, 18883, 13, 7529, 11037, 13903, 22339, 10532, 5108, 13644, 11130, 17898, 17721, 28515, 21843, 14545, 16348, 22867, 11788, 13, 7778, 22795, 26709, 22384, 17795, 20722, 11130, 20722, 16760, 22331, 15963, 18312, 27102, 11, 13261, 8048, 11, 16804, 22300, 9645, 19732, 13, 3631, 13261, 16058, 20722, 3137, 17572, 11, 10516, 13903, 13261, 22339, 20722, 2511, 20407, 24660, 26751, 113, 8, 0, 13, 2003, 25128, 22777, 5114, 20183, 25108, 16804, 11130, 13136, 16804, 13261, 22795, 17179, 117, 7826, 25206, 27533, 117, 131, 117, 22834, 13644, 1503, 8394, 23984, 8227, 24770, 23967, 8421, 0, 12847, 128, 25099, 26457, 26148, 13, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3] unknown: <class 'list'>. Should be one of a python, numpy, pytorch or tensorflow object."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def model_init():\n",
    "    return bert_model\n",
    "\n",
    "def compute_metrics_old(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Evaluate during training and a bit more often\n",
    "# than the default to be able to prune bad trials early.\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    model_init=model_init,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# Defaut objective is the sum of all metrics\n",
    "# when metrics are provided, so we have to maximize it.\n",
    "#trainer.hyperparameter_search(\n",
    "#    direction=\"maximize\", \n",
    "#    backend=\"ray\", \n",
    "#   n_trials=100, # deafult 100\n",
    "#    # n_jobs=2  # number of parallel jobs, if multiple GPUs\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-subsection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-accreditation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histaware-NidRwJ64-py3.8",
   "language": "python",
   "name": "histaware-nidrwj64-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
