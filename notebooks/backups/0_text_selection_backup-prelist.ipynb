{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links to Project Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Trello board](https://trello.com/invite/b/BWnRAtKJ/3e7ce03017000289323e762d0ed2e304/histaware)\n",
    "- [Notion Wiki](https://www.notion.so/HistAware-529aba41f84946b19d493394ef6a2748)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text",
     "selection",
     "xml",
     "transformers"
    ]
   },
   "source": [
    "# Part I: Text selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first phase of the project, we approach the first problem of selecting texts similar texts. Intially the scope of the research is focused on texts that deal with `energy`. However, this scope might change and/or might be expanded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phases of Part I:**\n",
    "- **Validate the approach to the project**:\n",
    "    1. Decide whether to use title and paragraphs or only one of the two\n",
    "    2. Find the most efficient way to read all the xml files\n",
    "    3. Begin to label a golden set of texts that are within the scope of the research AND select the most important keywords that will be used to search for similar texts\n",
    "    4. Run the text similarity ML algorithm\n",
    "    5. Have the teaching assistant go throught the selection and identify mistakes\n",
    "- **To think about**: how to keep the relevant information about the text fragment (i.e. newspaper origin and date)?\n",
    "- **Decide the tools to use for text selection**. Current choices are:\n",
    "    - Use `sentence-transformers` from UKPLab (https://github.com/UKPLab/sentence-transformers)\n",
    "        - Generate embeddings on sentences (max 512 words)\n",
    "        - Find similar texts\n",
    "    - Use `faiss` from Facebook AI (https://github.com/facebookresearch/faiss)\n",
    "        - Less documentation but seemingly more scalable\n",
    "    - Use ASReview from Utrecht University ()\n",
    "        - A meeting with Jonathan or Raul is necessary to understand the feasibility of this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as et \n",
    "import collections\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import xmltodict\n",
    "import pathlib\n",
    "from itertools import chain\n",
    "import xml.etree.ElementTree as ET\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "np.set_printoptions(threshold=100)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "#### /print debug information to stdout\n",
    "\n",
    "# Find path of data folder\n",
    "main_path = sys.path\n",
    "# To go back to main folder\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delpher Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a catalogue of the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the location of each article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the file path and the file name into a dictionary. Then we transform the dictionary into a DataFrame so that we can later keep track of the index at which the parsing got stopped/interrupted (Dictionaries in Python do not have an order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_directory(path_dir, file_type):\n",
    "    \"\"\"Iterate over the `path_dir` and its children and\n",
    "    create a dictionary of\n",
    "        - name\n",
    "        - path\n",
    "        - dir\n",
    "    names of files found\n",
    "    \"\"\"\n",
    "    rootdir = main_path[0]+path_dir\n",
    "    file_names = {}\n",
    "    list_names = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(rootdir, topdown=True):\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "            if filepath.endswith(str(file_type)):\n",
    "                file_names[\"article_name\"] = file\n",
    "                file_names[\"article_path\"] = filepath\n",
    "                file_names[\"article_dir\"] = subdir\n",
    "                list_names.append(file_names)\n",
    "                file_names = {}\n",
    "\n",
    "    return(list_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_article_names = iterate_directory(\"/data/1950/\",\".xml\")\n",
    "article_names = pd.DataFrame.from_dict(xml_article_names)\n",
    "article_names.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find location of each metadata and \"ungizp\" them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_directory_gz(path_dir,file_type):\n",
    "    \"\"\"Iterate over the `path_dir` and its children and\n",
    "    create a dictionary of\n",
    "        - name\n",
    "        - path\n",
    "        - dir\n",
    "        - content\n",
    "    of .gz files found.\n",
    "    \"\"\"\n",
    "    rootdir = main_path[0]+path_dir\n",
    "    gz_content = {}\n",
    "    list_gzs = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(rootdir, topdown=True):\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "            if filepath.endswith(str(file_type)):\n",
    "                # Create list of dict\n",
    "                with gzip.open(filepath, 'rb') as f:\n",
    "                #, \\\n",
    "                #open(filepath+\".xml\", \"wb\") as r:\n",
    "                    gz_content[\"metadata_name\"] = file+\".xml\"\n",
    "                    gz_content[\"metadata_dir\"] = subdir\n",
    "                    gz_content[\"metadata_path\"] = filepath+\".xml\"\n",
    "                    # Ungzipping and writing to .xml\n",
    "                    #shutil.copyfileobj(f, r, 65536)\n",
    "                    \n",
    "                    list_gzs.append(gz_content)\n",
    "                    gz_content = {}\n",
    "    \n",
    "    return(list_gzs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gz_metadata_files = iterate_directory_gz(\"/data/1950/\",\".gz\")\n",
    "metadata_files = pd.DataFrame.from_dict(gz_metadata_files)\n",
    "metadata_files.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_XML_article(path, art_dir, title, index):\n",
    "    \"\"\"Parse the input XML file and store the result in a pandas \n",
    "    DataFrame with the given columns. \n",
    "    \n",
    "    Takes the filepath, file title and index integer of the df\n",
    "    \"\"\"\n",
    "    \n",
    "    xtree = et.parse(path)\n",
    "    xroot = xtree.getroot()\n",
    "    article = {}\n",
    "    dict_articles = {}\n",
    "    \n",
    "    # Parse the date with regex\n",
    "    match = re.search(r'\\d{4}[/]\\d{2}[-]\\d{2}', path)\n",
    "    date = datetime.strptime(match.group(), '%Y/%m-%d').date()\n",
    "    \n",
    "    for i, node in enumerate(xroot):\n",
    "        article[\"article_name\"] = str(title)\n",
    "        article[\"date\"] = str(date)\n",
    "        article[\"index\"] = index\n",
    "        article[\"filepath\"] = path\n",
    "        article[\"dir\"] = art_dir\n",
    "        if node.tag != \"p\":\n",
    "            article[node.tag] = node.text\n",
    "        else:\n",
    "            article[node.tag+\"_\"+str(i)] = node.text\n",
    "        dict_articles[index] = article\n",
    "\n",
    "    # Returns dict of dicts to speed up the parsing\n",
    "    return dict_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_XML_article_list(path, art_dir, title, index):\n",
    "    \"\"\"Parse the input XML file and store the result in a pandas \n",
    "    DataFrame with the given columns. \n",
    "    \n",
    "    Takes the filepath, file title and index integer of the df\n",
    "    \"\"\"\n",
    "    \n",
    "    xtree = et.parse(path)\n",
    "    xroot = xtree.getroot()\n",
    "    list_articles = []\n",
    "    \n",
    "    # Parse the date with regex\n",
    "    match = re.search(r'\\d{4}[/]\\d{2}[-]\\d{2}', path)\n",
    "    date = datetime.strptime(match.group(), '%Y/%m-%d').date()\n",
    "    \n",
    "    for i, node in enumerate(xroot):\n",
    "        if node.tag == \"title\":\n",
    "            article = {}\n",
    "            article[\"type\"] = \"title\"\n",
    "            article[\"text\"] = node.text\n",
    "            article[\"article_name\"] = str(title)\n",
    "            article[\"date\"] = str(date)\n",
    "            article[\"index\"] = index\n",
    "            article[\"filepath\"] = path\n",
    "            article[\"dir\"] = art_dir\n",
    "            list_articles.append(article)\n",
    "        else:\n",
    "            article = {}\n",
    "            article[\"type\"] = \"p\"\n",
    "            article[\"text\"] = node.text\n",
    "            article[\"article_name\"] = str(title)\n",
    "            article[\"date\"] = str(date)\n",
    "            article[\"index\"] = index\n",
    "            article[\"filepath\"] = path\n",
    "            article[\"dir\"] = art_dir\n",
    "            list_articles.append(article)\n",
    "\n",
    "    # Returns list of dict of articles and titles\n",
    "    return list_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "#temp_data = doc[\"didl:DIDL\"][\"didl:Item\"][\"didl:Component\"][0][\"didl:Resource\"][\"srw_dc:dcx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_XML_metadata(path, met_dir, title, index):\n",
    "    \"\"\"Parse the input XML file and store the result in a pandas \n",
    "    DataFrame with the given columns. \n",
    "    \n",
    "    Takes the filepath, file title and index integer of the df\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    dict_metadata = {}\n",
    "    \n",
    "    # Parse the date with regex\n",
    "    match = re.search(r'\\d{4}[/]\\d{2}[-]\\d{2}', path)\n",
    "    date = datetime.strptime(match.group(), '%Y/%m-%d').date()\n",
    "    \n",
    "    # Parse DIDL XML\n",
    "    with open(pathlib.Path(path), 'r') as f:\n",
    "        doc = xmltodict.parse(f.read())\n",
    "    temp_data = doc[\"didl:DIDL\"][\"didl:Item\"][\"didl:Component\"][0][\"didl:Resource\"][\"srw_dc:dcx\"]\n",
    "\n",
    "    metadata[\"metadata_title\"] = title\n",
    "    metadata[\"date\"] = date\n",
    "    metadata[\"index\"] = index\n",
    "    metadata[\"filepath\"] = path\n",
    "    metadata[\"dir\"] = met_dir\n",
    "    \n",
    "    # Retrieve informations about the newspaper\n",
    "    metadata[\"newspaper_title\"] = temp_data[\"dc:title\"]\n",
    "    metadata[\"newspaper_date\"] = temp_data[\"dc:date\"]\n",
    "    metadata[\"newspaper_city\"] = temp_data[\"dcterms:spatial\"][1][\"#text\"]\n",
    "    metadata[\"newspaper_publisher\"] = temp_data[\"dc:publisher\"]\n",
    "    metadata[\"newspaper_source\"] = temp_data[\"dc:source\"]\n",
    "    metadata[\"newspaper_volume\"] = temp_data[\"dcx:volume\"]\n",
    "    metadata[\"newspaper_issuenumber\"] = temp_data[\"dcx:issuenumber\"]\n",
    "    metadata[\"newspaper_language\"] = temp_data[\"dc:language\"][\"#text\"]\n",
    "    \n",
    "    dict_metadata[index] = metadata\n",
    "\n",
    "    return(dict_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utils Addendum**\n",
    "\n",
    "To search for an `article_path` or `article_name` given the other, use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = df_file_names.loc[df_file_names['article_name'] == \"DDD_110637387_0004_articletext.xml\"]\n",
    "#a = df_file_names.iloc[0]\n",
    "c = df_file_names.iloc[500000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the files given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, this loop takes ~0.012s for each parsing. This is extremely slow and it's not due to the `parse_XML` function (which is efficient), but instead it's because of the `concat` between series. \n",
    "\n",
    "In this way 100.000 documents take around 20 minutes to be parsed.\n",
    "- If possible, substitute the concat statement with something more efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def iterate_files_list(files, restart=False, index_restart=0):\n",
    "    \"\"\"Iterate through files `files`, parse them and concatenate\n",
    "    the result to be saved as a DataFrame in a feather object (.ftr)\n",
    "    \"\"\"\n",
    "    list_articles = []\n",
    "    # Every 10000 articles create a ftr file\n",
    "    save_each = 10000\n",
    "    \n",
    "    if restart == False:\n",
    "        main = None\n",
    "        previous_i = 0\n",
    "        current_i = 0\n",
    "        i = 0\n",
    "        n = 0\n",
    "        cnt = 0\n",
    "        for index, row in files.iterrows():\n",
    "            try:\n",
    "                list_articles.append(parse_XML_article_list(\n",
    "                    path = row[\"article_path\"],\n",
    "                    art_dir = row[\"article_dir\"], \n",
    "                    title = row[\"article_name\"],\n",
    "                    index = row[\"index\"]))\n",
    "            except Exception as e:\n",
    "                print(f\"Index: {index}\", e.args)\n",
    "                continue\n",
    "            # Each X, save the file in a .ftr\n",
    "            if (i == save_each):\n",
    "                current_i = current_i + i\n",
    "                file_path = main_path[0]+\"/data/processed/processed_articles/processed_data_list_\"+str(previous_i)+\"_\"+str(current_i)+\".ftr\"\n",
    "                main = pd.DataFrame(list(chain.from_iterable(list_articles)))\n",
    "                main.to_feather(file_path)\n",
    "                main = None\n",
    "                list_articles = []\n",
    "                previous_i = current_i\n",
    "                i = 0\n",
    "            # Each 1000 files, print the progress\n",
    "            if (i % 2000 == 0):\n",
    "                clear_output(wait=True)\n",
    "                display(\"Files parsed: \"+str(2000*cnt))\n",
    "                display(\"Current file: \"+row[\"article_name\"]+\" (Index: \"+str(row[\"index\"])+\")\")\n",
    "                cnt += 1\n",
    "            i += 1\n",
    "    if restart == True:\n",
    "        main = None\n",
    "        previous_i = index_restart\n",
    "        current_i = index_restart\n",
    "        i = 0\n",
    "        n = 0\n",
    "        cnt = index_restart/2000\n",
    "        for index, row in files.iloc[index_restart:].iterrows():\n",
    "            try:\n",
    "                list_articles.append(parse_XML_article_list(\n",
    "                    path = row[\"article_path\"],\n",
    "                    art_dir = row[\"article_dir\"], \n",
    "                    title = row[\"article_name\"],\n",
    "                    index = row[\"index\"]))\n",
    "            except Exception as e:\n",
    "                print(f\"Index: {index}\", e.args)\n",
    "                continue\n",
    "            # Each X, save the file in a .ftr\n",
    "            if (i == save_each):\n",
    "                current_i = current_i + i\n",
    "                file_path = main_path[0]+\"/data/processed/processed_articles/processed_data_list_\"+str(previous_i)+\"_\"+str(current_i)+\".ftr\"\n",
    "                main = pd.DataFrame(list(chain.from_iterable(list_articles)))\n",
    "                main.to_feather(file_path)\n",
    "                main = None\n",
    "                list_articles = []\n",
    "                previous_i = current_i\n",
    "                i = 0\n",
    "            # Each 1000 files, print the progress\n",
    "            if (i % 2000 == 0):\n",
    "                clear_output(wait=True)\n",
    "                display(\"Files parsed: \"+str(2000*cnt))\n",
    "                display(\"Current file: \"+row[\"article_name\"]+\"(Index: \"+str(row[\"index\"])+\")\")\n",
    "                cnt += 1\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_metadata(files):\n",
    "    \"\"\"Iterate through files `files`, parse them and concatenate\n",
    "    the result to be saved as a DataFrame in a feather object (.ftr)\n",
    "    \"\"\"\n",
    "    main = None\n",
    "    previous_i = 0\n",
    "    current_i = 0\n",
    "    i = 0\n",
    "    n = 0\n",
    "    cnt = 0\n",
    "    dict_metadata = {}\n",
    "    \n",
    "    for index, row in files.iterrows():\n",
    "        try:\n",
    "            dict_metadata.update(\n",
    "                parse_XML_metadata(\n",
    "                    path = row[\"metadata_path\"],\n",
    "                    met_dir = row[\"metadata_dir\"], \n",
    "                    title = row[\"metadata_name\"],\n",
    "                    index = row[\"index\"]))\n",
    "        except Exception:\n",
    "            print(f\"Index: {index}\", e.args)\n",
    "            continue\n",
    "        # Each X, save the file in a .ftr\n",
    "        if (i == 1000):\n",
    "            current_i = current_i + i\n",
    "            file_path = main_path[0]+\"/data/processed/processed_metadata/processed_metadata_\"+str(previous_i)+\"_\"+str(current_i)+\".ftr\"\n",
    "            main = pd.DataFrame.from_dict(dict_metadata).T.reset_index()\n",
    "            main.to_feather(file_path)\n",
    "            main = None\n",
    "            dict_metadata = {}\n",
    "            previous_i = current_i\n",
    "            i = 0\n",
    "        # Each 100 files, print the progress\n",
    "        if (i % 100 == 0):\n",
    "            clear_output(wait=True)\n",
    "            display(\"Files parsed: \"+str(2000*cnt))\n",
    "            display(\"Current file: \"+row[\"metadata_name\"]+\" (Index: \"+str(row[\"index\"])+\")\")\n",
    "            cnt += 1\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Files parsed: 64000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Current file: DDD_011155446_0029_articletext.xml (Index: 64002)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-f4bd4e7a44ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miterate_files_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-b93d9b06f782>\u001b[0m in \u001b[0;36miterate_files_list\u001b[0;34m(files, restart, index_restart)\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mart_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"article_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"article_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     index = row[\"index\"]))\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Index: {index}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b034712029db>\u001b[0m in \u001b[0;36mparse_XML_article_list\u001b[0;34m(path, art_dir, title, index)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mxtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0met\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mxroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlist_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \"\"\"\n\u001b[1;32m   1196\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;31m# It can be used to parse the whole source without feeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;31m# it with chunks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_whole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterate_files_list(article_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files parsed: 0\n",
      "Current file: DDD:ddd:110637387:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 50\n",
      "Current file: DDD:ddd:010865705:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 100\n",
      "Current file: DDD:ddd:010891636:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 150\n",
      "Current file: DDD:ddd:010480615:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 200\n",
      "Current file: DDD:ddd:010480574:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 250\n",
      "Current file: DDD:ddd:011155446:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 300\n",
      "Current file: DDD:ddd:010537413:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 350\n",
      "Current file: DDD:ddd:110637339:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 400\n",
      "Current file: DDD:ddd:010950304:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 450\n",
      "Current file: DDD:ddd:011202192:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 500\n",
      "Current file: DDD:ddd:010852499:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 550\n",
      "Current file: DDD:ddd:010862555:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 600\n",
      "Current file: DDD:ddd:010865671:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 650\n",
      "Current file: DDD:ddd:010950611:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 700\n",
      "Current file: DDD:ddd:010950604:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 750\n",
      "Current file: DDD:ddd:010733900:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 800\n",
      "Current file: DDD:ddd:010886513:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 850\n",
      "Current file: DDD:ddd:010475617:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 900\n",
      "Current file: DDD:ddd:010417752:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 950\n",
      "Current file: DDD:ddd:010475618:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1000\n",
      "Current file: DDD:ddd:010862664:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1050\n",
      "Current file: DDD:ddd:010850961:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1100\n",
      "Current file: DDD:ddd:011155321:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1150\n",
      "Current file: DDD:ddd:010852448:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1200\n",
      "Current file: DDD:ddd:010537412:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1250\n",
      "Current file: DDD:ddd:010476410:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1300\n",
      "Current file: DDD:ddd:010475604:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1350\n",
      "Current file: DDD:ddd:010987679:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1400\n",
      "Current file: DDD:ddd:010852523:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1450\n",
      "Current file: DDD:ddd:011155471:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1500\n",
      "Current file: DDD:ddd:010862791:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1550\n",
      "Current file: DDD:ddd:110584895:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1600\n",
      "Current file: DDD:ddd:010862798:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1650\n",
      "Current file: DDD:ddd:110637545:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1700\n",
      "Current file: DDD:ddd:010480431:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1750\n",
      "Current file: DDD:ddd:010850943:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1800\n",
      "Current file: DDD:ddd:010852559:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1850\n",
      "Current file: DDD:ddd:010417410:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1900\n",
      "Current file: DDD:ddd:010476457:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 1950\n",
      "Current file: DDD:ddd:010862620:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 2000\n",
      "Current file: DDD:ddd:010852585:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 2050\n",
      "Current file: DDD:ddd:010476428:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 2100\n",
      "Current file: DDD:ddd:110637581:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 2150\n",
      "Current file: DDD:ddd:010852453:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 2200\n",
      "Current file: DDD:ddd:010852385:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 2250\n",
      "Current file: DDD:ddd:010950489:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 2300\n",
      "Current file: DDD:ddd:011210764:mpeg21.didl.xml.gz.xml\n",
      "\n",
      "Files parsed: 2350\n",
      "Current file: DDD:ddd:010862753:mpeg21.didl.xml.gz.xml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iterate_metadata(metadata_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text selection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest parsed files previously saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we parse all the files present in the example `data-1950` folder, we produce 65 files containing the parsed original data into a format which is more easily readable by a machine. The total weight of the files is 65*10=650MB which is a 5x reduction from the original size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-16 09:24:58 - PyTorch version 1.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "# https://www.sbert.net/docs/\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util\n",
    "\n",
    "# These are the pure transformers from huggingface\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For saving\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "# Set searborn settings\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "# Set fixed random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Find GPU on device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read saved files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve all the names of the ftr files saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_articles = iterate_directory(\"/data/processed/processed_articles\",\".ftr\")\n",
    "ftr_articles = pd.DataFrame(ftr_articles)\n",
    "ftr_articles.rename({'article_name': 'ftr_name', 'article_path': 'ftr_path', 'article_dir': 'ftr_dir'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_metadata = iterate_directory(\"/data/processed/processed_metadata\",\".ftr\")\n",
    "ftr_metadata = pd.DataFrame(ftr_metadata)\n",
    "ftr_metadata.rename({'article_name': 'ftr_name', 'article_path': 'ftr_path', 'article_dir': 'ftr_dir'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve all the content of the files into a list format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read one ftr file as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_ftr(df):\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        ftr = pd.read_feather(row[\"ftr_path\"])\n",
    "        result = result.append(ftr)\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = iterate_ftr(ftr_articles)\n",
    "df_articles.sort_values(by=[\"index\"], ascending=True)\n",
    "df_articles.rename({\"filepath\": \"article_filepath\", \"index\": \"index_article\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = iterate_ftr(ftr_metadata)\n",
    "df_metadata.drop([\"level_0\", \"date\"], axis=1, inplace=True)\n",
    "df_metadata.rename({\"filepath\": \"metadata_filepath\", \"index\": \"index_metadata\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge articles and metadata in one single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_articles.merge(df_metadata, how='left', on='dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have (one)a merged file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be needed to be done recursively, for all files present in the database.\n",
    "\n",
    "So efficiency is key here!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find synonym(s) for the key search word(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nl_core_news_lg\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = nl_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a list with only the text (paragraphs) and not the other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all the paragraphs into one single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def list_paragraphs(df):\n",
    "    list_p = []\n",
    "\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        for i in range(1,df.shape[1]):\n",
    "            p = \"p_\"+str(i)\n",
    "            try:\n",
    "                if row[p] and row[p] is not None:\n",
    "                    list_p.append(row[p])\n",
    "            except KeyError as e:\n",
    "                continue\n",
    "\n",
    "    return(list_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def list_title(df):\n",
    "    list_titles = []\n",
    "\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        try:\n",
    "            if row[\"title\"] and row[\"title\"] is not None:\n",
    "                list_titles.append(row[\"title\"])\n",
    "        except KeyError as e:\n",
    "            continue\n",
    "\n",
    "    return(list_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30001it [00:04, 6845.35it/s]\n"
     ]
    }
   ],
   "source": [
    "pars = list_paragraphs(df_test_joined)\n",
    "titles = list_title(df_test_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_pars = pd.DataFrame(pars).reset_index()\n",
    "df_pars.rename({0: 'text'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_titles = pd.DataFrame(titles).reset_index()\n",
    "df_titles.rename({0: 'text'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_synonyms(word, df, n):\n",
    "    \"\"\"Find all texts in which a synonym of the word appears.\n",
    "    \n",
    "    Takes:\n",
    "        - string (word)\n",
    "        - dataframe in which to search\n",
    "        - The total number of synonym to retrieve\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    ms = nlp.vocab.vectors.most_similar(np.asarray([nlp.vocab.vectors[nlp.vocab.strings[word]]]), n=n)\n",
    "    synonyms = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "    print(f\"Searching using the following synonyms of {word}:\")\n",
    "    print(synonyms)\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    \n",
    "    for syn in tqdm(synonyms):\n",
    "        result = result.append(df[df[\"text\"].str.contains(syn, \n",
    "                                                          case=False,\n",
    "                                                          regex=False)\n",
    "                                 ]\n",
    "                              )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching using the following synonyms of energie:\n",
      "['energie', 'oerenergie', 'Cenergie', 'energieeen', 'energieen', 'energi', 'aarde-energie', 'lichtenergie', 'levensenergie', 'energiestroom', 'energie-boost', 'energiedip', 'hulpenergie', 'warmte-energie', 'Bio-energie', 'zonneenergie', 'Reiki-energie', 'hartenergie', 'remenergie', 'aardenergie', 'energievorm', 'energiegolf', 'energiestoot', 'energievol', 'energiegolven', 'bewegingsenergie', 'bio-energie', 'energiecellen', 'basisenergie', 'energievolle', 'energieën', 'energiebron', 'zonenergie', 'stralingsenergie', 'énergie', 'energiebalans', 'energiestromen', 'lichaamsenergie', 'energiebewustzijn', 'energietoevoer', 'energiemix', 'groepsenergie', 'energieboost', 'energievreter', 'Levensenergie', 'waterenergie', 'energie-opwekking', 'energierijk', 'energieverbuik', 'energiebronnen']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:57<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "a = search_synonyms(\"energie\", df_joined, 50)\n",
    "a.drop_duplicates(ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the row that were found, select the entire record from the merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>article_name</th>\n",
       "      <th>date</th>\n",
       "      <th>index_article</th>\n",
       "      <th>article_filepath</th>\n",
       "      <th>dir</th>\n",
       "      <th>metadata_title</th>\n",
       "      <th>index_metadata</th>\n",
       "      <th>metadata_filepath</th>\n",
       "      <th>newspaper_title</th>\n",
       "      <th>newspaper_date</th>\n",
       "      <th>newspaper_city</th>\n",
       "      <th>newspaper_publisher</th>\n",
       "      <th>newspaper_source</th>\n",
       "      <th>newspaper_volume</th>\n",
       "      <th>newspaper_issuenumber</th>\n",
       "      <th>newspaper_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>ét Stoelrompenfabrlek te Bergschenhoek vraagt ...</td>\n",
       "      <td>DDD_010950576_0040_articletext.xml</td>\n",
       "      <td>1950-10-26</td>\n",
       "      <td>30033</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576/DDD_010950576...</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576</td>\n",
       "      <td>DDD:ddd:010950576:mpeg21.didl.xml.gz.xml</td>\n",
       "      <td>275.0</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576/DDD:ddd:01095...</td>\n",
       "      <td>Het vrĳe volk : democratisch-socialistisch dag...</td>\n",
       "      <td>1950-10-26</td>\n",
       "      <td>Rotterdam</td>\n",
       "      <td>De Arbeiderspers</td>\n",
       "      <td>Gemeentearchief Rotterdam</td>\n",
       "      <td>6</td>\n",
       "      <td>1661</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p</td>\n",
       "      <td>De „Sibajak\" lag vaarklaar aan de trossen, We,...</td>\n",
       "      <td>DDD_010950576_0162_articletext.xml</td>\n",
       "      <td>1950-10-26</td>\n",
       "      <td>30041</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576/DDD_010950576...</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576</td>\n",
       "      <td>DDD:ddd:010950576:mpeg21.didl.xml.gz.xml</td>\n",
       "      <td>275.0</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576/DDD:ddd:01095...</td>\n",
       "      <td>Het vrĳe volk : democratisch-socialistisch dag...</td>\n",
       "      <td>1950-10-26</td>\n",
       "      <td>Rotterdam</td>\n",
       "      <td>De Arbeiderspers</td>\n",
       "      <td>Gemeentearchief Rotterdam</td>\n",
       "      <td>6</td>\n",
       "      <td>1661</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p</td>\n",
       "      <td>(:: --■:;- v.rr: ■■■--•■■■.■ : &gt;-. Bekwame Aut...</td>\n",
       "      <td>DDD_010950576_0057_articletext.xml</td>\n",
       "      <td>1950-10-26</td>\n",
       "      <td>30042</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576/DDD_010950576...</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576</td>\n",
       "      <td>DDD:ddd:010950576:mpeg21.didl.xml.gz.xml</td>\n",
       "      <td>275.0</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576/DDD:ddd:01095...</td>\n",
       "      <td>Het vrĳe volk : democratisch-socialistisch dag...</td>\n",
       "      <td>1950-10-26</td>\n",
       "      <td>Rotterdam</td>\n",
       "      <td>De Arbeiderspers</td>\n",
       "      <td>Gemeentearchief Rotterdam</td>\n",
       "      <td>6</td>\n",
       "      <td>1661</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>ét Stoelrompenfabrlek te Bergschenhoek vraagt ...</td>\n",
       "      <td>DDD_010950576_0058_articletext.xml</td>\n",
       "      <td>1950-10-26</td>\n",
       "      <td>30050</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576/DDD_010950576...</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576</td>\n",
       "      <td>DDD:ddd:010950576:mpeg21.didl.xml.gz.xml</td>\n",
       "      <td>275.0</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576/DDD:ddd:01095...</td>\n",
       "      <td>Het vrĳe volk : democratisch-socialistisch dag...</td>\n",
       "      <td>1950-10-26</td>\n",
       "      <td>Rotterdam</td>\n",
       "      <td>De Arbeiderspers</td>\n",
       "      <td>Gemeentearchief Rotterdam</td>\n",
       "      <td>6</td>\n",
       "      <td>1661</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p</td>\n",
       "      <td>Kaapland v Z Afrika Khyber v Japan 5 Nov - - ....</td>\n",
       "      <td>DDD_010950576_0181_articletext.xml</td>\n",
       "      <td>1950-10-26</td>\n",
       "      <td>30088</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576/DDD_010950576...</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576</td>\n",
       "      <td>DDD:ddd:010950576:mpeg21.didl.xml.gz.xml</td>\n",
       "      <td>275.0</td>\n",
       "      <td>../data/1950/10-26/DDD_010950576/DDD:ddd:01095...</td>\n",
       "      <td>Het vrĳe volk : democratisch-socialistisch dag...</td>\n",
       "      <td>1950-10-26</td>\n",
       "      <td>Rotterdam</td>\n",
       "      <td>De Arbeiderspers</td>\n",
       "      <td>Gemeentearchief Rotterdam</td>\n",
       "      <td>6</td>\n",
       "      <td>1661</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>p</td>\n",
       "      <td>Het ~Museum voor het Onderwijs in Den Haag\" is...</td>\n",
       "      <td>DDD_011199721_0056_articletext.xml</td>\n",
       "      <td>1950-03-07</td>\n",
       "      <td>43224</td>\n",
       "      <td>../data/1950/03-07/DDD_011199721/DDD_011199721...</td>\n",
       "      <td>../data/1950/03-07/DDD_011199721</td>\n",
       "      <td>DDD:ddd:011199721:mpeg21.didl.xml.gz.xml</td>\n",
       "      <td>402.0</td>\n",
       "      <td>../data/1950/03-07/DDD_011199721/DDD:ddd:01119...</td>\n",
       "      <td>De Tĳd : godsdienstig-staatkundig dagblad</td>\n",
       "      <td>1950-03-07</td>\n",
       "      <td>'s-Hertogenbosch</td>\n",
       "      <td>Gebr. Verhoeven</td>\n",
       "      <td>Koninklijke Bibliotheek C 236</td>\n",
       "      <td>105</td>\n",
       "      <td>34371</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>p</td>\n",
       "      <td>Na de pauze speelde Willem Hielkema Tschaikows...</td>\n",
       "      <td>DDD_010537470_0093_articletext.xml</td>\n",
       "      <td>1950-10-21</td>\n",
       "      <td>23397</td>\n",
       "      <td>../data/1950/10-21/DDD_010537470/DDD_010537470...</td>\n",
       "      <td>../data/1950/10-21/DDD_010537470</td>\n",
       "      <td>DDD:ddd:010537470:mpeg21.didl.xml.gz.xml</td>\n",
       "      <td>217.0</td>\n",
       "      <td>../data/1950/10-21/DDD_010537470/DDD:ddd:01053...</td>\n",
       "      <td>De Heerenveensche koerier : onafhankelĳk dagbl...</td>\n",
       "      <td>1950-10-21</td>\n",
       "      <td>Leeuwarden [etc.]</td>\n",
       "      <td>Stichting Je Maintiendrai Friesland</td>\n",
       "      <td>KB C 199</td>\n",
       "      <td>6</td>\n",
       "      <td>248</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>p</td>\n",
       "      <td>DERDE KLASSE A: Terrasvogels—Schagen 2—o; HMS—...</td>\n",
       "      <td>DDD_010852520_0115_articletext.xml</td>\n",
       "      <td>1950-04-11</td>\n",
       "      <td>18092</td>\n",
       "      <td>../data/1950/04-11/DDD_010852520/DDD_010852520...</td>\n",
       "      <td>../data/1950/04-11/DDD_010852520</td>\n",
       "      <td>DDD:ddd:010852520:mpeg21.didl.xml.gz.xml</td>\n",
       "      <td>167.0</td>\n",
       "      <td>../data/1950/04-11/DDD_010852520/DDD:ddd:01085...</td>\n",
       "      <td>De waarheid</td>\n",
       "      <td>1950-04-11</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>s.n.</td>\n",
       "      <td>Internationaal Instituut voor Sociale Geschied...</td>\n",
       "      <td>9</td>\n",
       "      <td>287</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>p</td>\n",
       "      <td>KAMPIOENSCH VAN NEDERLAND Maurits—Blauw Wit. A...</td>\n",
       "      <td>DDD_010850977_0129_articletext.xml</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>57221</td>\n",
       "      <td>../data/1950/03-31/DDD_010850977/DDD_010850977...</td>\n",
       "      <td>../data/1950/03-31/DDD_010850977</td>\n",
       "      <td>DDD:ddd:010850977:mpeg21.didl.xml.gz.xml</td>\n",
       "      <td>530.0</td>\n",
       "      <td>../data/1950/03-31/DDD_010850977/DDD:ddd:01085...</td>\n",
       "      <td>De waarheid</td>\n",
       "      <td>1950-03-31</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>s.n.</td>\n",
       "      <td>Internationaal Instituut voor Sociale Geschied...</td>\n",
       "      <td>9</td>\n",
       "      <td>279</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>p</td>\n",
       "      <td>Ce territoire, non pas par son augmentation ph...</td>\n",
       "      <td>DDD_010873955_0001_articletext.xml</td>\n",
       "      <td>1950-04-21</td>\n",
       "      <td>51704</td>\n",
       "      <td>../data/1950/04-21/DDD_010873955/DDD_010873955...</td>\n",
       "      <td>../data/1950/04-21/DDD_010873955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>656 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    type                                               text  \\\n",
       "0      p  ét Stoelrompenfabrlek te Bergschenhoek vraagt ...   \n",
       "1      p  De „Sibajak\" lag vaarklaar aan de trossen, We,...   \n",
       "2      p  (:: --■:;- v.rr: ■■■--•■■■.■ : >-. Bekwame Aut...   \n",
       "3      p  ét Stoelrompenfabrlek te Bergschenhoek vraagt ...   \n",
       "4      p  Kaapland v Z Afrika Khyber v Japan 5 Nov - - ....   \n",
       "..   ...                                                ...   \n",
       "651    p  Het ~Museum voor het Onderwijs in Den Haag\" is...   \n",
       "652    p  Na de pauze speelde Willem Hielkema Tschaikows...   \n",
       "653    p  DERDE KLASSE A: Terrasvogels—Schagen 2—o; HMS—...   \n",
       "654    p  KAMPIOENSCH VAN NEDERLAND Maurits—Blauw Wit. A...   \n",
       "655    p  Ce territoire, non pas par son augmentation ph...   \n",
       "\n",
       "                           article_name        date  index_article  \\\n",
       "0    DDD_010950576_0040_articletext.xml  1950-10-26          30033   \n",
       "1    DDD_010950576_0162_articletext.xml  1950-10-26          30041   \n",
       "2    DDD_010950576_0057_articletext.xml  1950-10-26          30042   \n",
       "3    DDD_010950576_0058_articletext.xml  1950-10-26          30050   \n",
       "4    DDD_010950576_0181_articletext.xml  1950-10-26          30088   \n",
       "..                                  ...         ...            ...   \n",
       "651  DDD_011199721_0056_articletext.xml  1950-03-07          43224   \n",
       "652  DDD_010537470_0093_articletext.xml  1950-10-21          23397   \n",
       "653  DDD_010852520_0115_articletext.xml  1950-04-11          18092   \n",
       "654  DDD_010850977_0129_articletext.xml  1950-03-31          57221   \n",
       "655  DDD_010873955_0001_articletext.xml  1950-04-21          51704   \n",
       "\n",
       "                                      article_filepath  \\\n",
       "0    ../data/1950/10-26/DDD_010950576/DDD_010950576...   \n",
       "1    ../data/1950/10-26/DDD_010950576/DDD_010950576...   \n",
       "2    ../data/1950/10-26/DDD_010950576/DDD_010950576...   \n",
       "3    ../data/1950/10-26/DDD_010950576/DDD_010950576...   \n",
       "4    ../data/1950/10-26/DDD_010950576/DDD_010950576...   \n",
       "..                                                 ...   \n",
       "651  ../data/1950/03-07/DDD_011199721/DDD_011199721...   \n",
       "652  ../data/1950/10-21/DDD_010537470/DDD_010537470...   \n",
       "653  ../data/1950/04-11/DDD_010852520/DDD_010852520...   \n",
       "654  ../data/1950/03-31/DDD_010850977/DDD_010850977...   \n",
       "655  ../data/1950/04-21/DDD_010873955/DDD_010873955...   \n",
       "\n",
       "                                  dir  \\\n",
       "0    ../data/1950/10-26/DDD_010950576   \n",
       "1    ../data/1950/10-26/DDD_010950576   \n",
       "2    ../data/1950/10-26/DDD_010950576   \n",
       "3    ../data/1950/10-26/DDD_010950576   \n",
       "4    ../data/1950/10-26/DDD_010950576   \n",
       "..                                ...   \n",
       "651  ../data/1950/03-07/DDD_011199721   \n",
       "652  ../data/1950/10-21/DDD_010537470   \n",
       "653  ../data/1950/04-11/DDD_010852520   \n",
       "654  ../data/1950/03-31/DDD_010850977   \n",
       "655  ../data/1950/04-21/DDD_010873955   \n",
       "\n",
       "                               metadata_title  index_metadata  \\\n",
       "0    DDD:ddd:010950576:mpeg21.didl.xml.gz.xml           275.0   \n",
       "1    DDD:ddd:010950576:mpeg21.didl.xml.gz.xml           275.0   \n",
       "2    DDD:ddd:010950576:mpeg21.didl.xml.gz.xml           275.0   \n",
       "3    DDD:ddd:010950576:mpeg21.didl.xml.gz.xml           275.0   \n",
       "4    DDD:ddd:010950576:mpeg21.didl.xml.gz.xml           275.0   \n",
       "..                                        ...             ...   \n",
       "651  DDD:ddd:011199721:mpeg21.didl.xml.gz.xml           402.0   \n",
       "652  DDD:ddd:010537470:mpeg21.didl.xml.gz.xml           217.0   \n",
       "653  DDD:ddd:010852520:mpeg21.didl.xml.gz.xml           167.0   \n",
       "654  DDD:ddd:010850977:mpeg21.didl.xml.gz.xml           530.0   \n",
       "655                                       NaN             NaN   \n",
       "\n",
       "                                     metadata_filepath  \\\n",
       "0    ../data/1950/10-26/DDD_010950576/DDD:ddd:01095...   \n",
       "1    ../data/1950/10-26/DDD_010950576/DDD:ddd:01095...   \n",
       "2    ../data/1950/10-26/DDD_010950576/DDD:ddd:01095...   \n",
       "3    ../data/1950/10-26/DDD_010950576/DDD:ddd:01095...   \n",
       "4    ../data/1950/10-26/DDD_010950576/DDD:ddd:01095...   \n",
       "..                                                 ...   \n",
       "651  ../data/1950/03-07/DDD_011199721/DDD:ddd:01119...   \n",
       "652  ../data/1950/10-21/DDD_010537470/DDD:ddd:01053...   \n",
       "653  ../data/1950/04-11/DDD_010852520/DDD:ddd:01085...   \n",
       "654  ../data/1950/03-31/DDD_010850977/DDD:ddd:01085...   \n",
       "655                                                NaN   \n",
       "\n",
       "                                       newspaper_title newspaper_date  \\\n",
       "0    Het vrĳe volk : democratisch-socialistisch dag...     1950-10-26   \n",
       "1    Het vrĳe volk : democratisch-socialistisch dag...     1950-10-26   \n",
       "2    Het vrĳe volk : democratisch-socialistisch dag...     1950-10-26   \n",
       "3    Het vrĳe volk : democratisch-socialistisch dag...     1950-10-26   \n",
       "4    Het vrĳe volk : democratisch-socialistisch dag...     1950-10-26   \n",
       "..                                                 ...            ...   \n",
       "651          De Tĳd : godsdienstig-staatkundig dagblad     1950-03-07   \n",
       "652  De Heerenveensche koerier : onafhankelĳk dagbl...     1950-10-21   \n",
       "653                                        De waarheid     1950-04-11   \n",
       "654                                        De waarheid     1950-03-31   \n",
       "655                                                NaN            NaN   \n",
       "\n",
       "        newspaper_city                  newspaper_publisher  \\\n",
       "0            Rotterdam                     De Arbeiderspers   \n",
       "1            Rotterdam                     De Arbeiderspers   \n",
       "2            Rotterdam                     De Arbeiderspers   \n",
       "3            Rotterdam                     De Arbeiderspers   \n",
       "4            Rotterdam                     De Arbeiderspers   \n",
       "..                 ...                                  ...   \n",
       "651   's-Hertogenbosch                      Gebr. Verhoeven   \n",
       "652  Leeuwarden [etc.]  Stichting Je Maintiendrai Friesland   \n",
       "653          Amsterdam                                 s.n.   \n",
       "654          Amsterdam                                 s.n.   \n",
       "655                NaN                                  NaN   \n",
       "\n",
       "                                      newspaper_source newspaper_volume  \\\n",
       "0                            Gemeentearchief Rotterdam                6   \n",
       "1                            Gemeentearchief Rotterdam                6   \n",
       "2                            Gemeentearchief Rotterdam                6   \n",
       "3                            Gemeentearchief Rotterdam                6   \n",
       "4                            Gemeentearchief Rotterdam                6   \n",
       "..                                                 ...              ...   \n",
       "651                      Koninklijke Bibliotheek C 236              105   \n",
       "652                                           KB C 199                6   \n",
       "653  Internationaal Instituut voor Sociale Geschied...                9   \n",
       "654  Internationaal Instituut voor Sociale Geschied...                9   \n",
       "655                                                NaN              NaN   \n",
       "\n",
       "    newspaper_issuenumber newspaper_language  \n",
       "0                    1661                 nl  \n",
       "1                    1661                 nl  \n",
       "2                    1661                 nl  \n",
       "3                    1661                 nl  \n",
       "4                    1661                 nl  \n",
       "..                    ...                ...  \n",
       "651                 34371                 nl  \n",
       "652                   248                 nl  \n",
       "653                   287                 nl  \n",
       "654                   279                 nl  \n",
       "655                   NaN                NaN  \n",
       "\n",
       "[656 rows x 18 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "b.to_csv(main_path[0]+\"/data\"+\"/\"+\"energie_\"+\"search_2020.csv\",\n",
    "            sep=\",\",\n",
    "            quotechar='\"',\n",
    "            index=False)\n",
    "\n",
    "#with open(path[0]+\"/data\"+\"/list_sentences.csv\", 'w') as myfile:\n",
    "#    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#    wr.writerow(list_sentences)\n",
    "\n",
    "#with open('list_sentences.pkl', \"wb\") as fOut:\n",
    "#    pickle.dump(list_sentences, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the multilingual model pre-trained on 10+ languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play around with `SBERT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is the `distiluse-base-multilingual-cased` model. From [sbert]( https://www.sbert.net/docs/pretrained_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-16 11:24:09 - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased\n",
      "2020-09-16 11:24:09 - Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "2020-09-16 11:24:09 - Load SentenceTransformer from folder: /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip\n",
      "2020-09-16 11:24:09 - loading configuration file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/config.json\n",
      "2020-09-16 11:24:09 - Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "2020-09-16 11:24:09 - loading weights file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/pytorch_model.bin\n",
      "2020-09-16 11:24:15 - All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "2020-09-16 11:24:15 - All the weights of DistilBertModel were initialized from the model checkpoint at /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "2020-09-16 11:24:15 - Model name '/Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT' not found in model shortcut name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). Assuming '/Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-09-16 11:24:15 - Didn't find file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/tokenizer.json. We won't load it.\n",
      "2020-09-16 11:24:15 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/vocab.txt\n",
      "2020-09-16 11:24:15 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/added_tokens.json\n",
      "2020-09-16 11:24:15 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/special_tokens_map.json\n",
      "2020-09-16 11:24:15 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/tokenizer_config.json\n",
      "2020-09-16 11:24:15 - loading file None\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2888\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2889\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 6",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-bc94c62de6c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Sentences are encoded by calling model.encode()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Print the embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, is_pretokenized, device, num_workers)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mlength_sorted_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0msentences_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlength_sorted_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0minp_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncodeDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_tokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_pretokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0minp_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_batching_collate_text_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mlength_sorted_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0msentences_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlength_sorted_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0minp_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncodeDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_tokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_pretokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0minp_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_batching_collate_text_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 6"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased', device=device)\n",
    "\n",
    "# Load paragraphs\n",
    "sentences = b\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maar het waren juist deze geheime gegevens, die Louwers in handen wilde krijgen. Want de Tsjechoslowaakse industrie heeft op dit gebied een reusachtige voorsprong. Of nemen wij Philips! Toen de Duitsers ons land binnenvielen, vertrokken uit Eindhoven 30 auto's, volgeladen met geheime papieren: de productiegeheimen van Philips! Die moesten in veiligheid. De rest was niet belangrijk. Geen mens buiten de vesting van Philips komt over deze geheimen iets te weten. En binnen de vesting is een wetenschappelijk systeem uitgewerkt, dat er voor zorgt, dat de lagere ingenieurs en technici slechts gedeelten van een bepaald productieproces leren kennen, waarmee zij niets kunnen beginnen. Verteld wordt, dat eens een delegatie van Japanse belangstellenden Philips kwam bezoeken. De heren werden „overal\" rondgeleid, ze kregen een pracht van een diner aangeboden. Maar de Philipsdirectie vermoedde dat sommigen van hen micro-fototoestellen by zich hadden. Fouilleren kon men de „geëerde gasten\" niet. Dus werden de heren vriendelijk uitgenodigd ook de laatste snufjes van de Röntgen-afdeling te bezichtigen. Met een „Japanse\" glimlach nodigde men de Japanners uit, van de gelegenheid gebruik te maken. Ze lieten dus Röntgenfoto's van zich zelf maken en meteen waren de microfilms van hun verborgen toestellen onbruikbaar gemaakt. Het beschermen van fabrieksgeheimen is dus de eerste zorg der ondernemers. Wetenschapelijke werkers, die de dienst van Philips verlaten, mogen een bepaald aantal jaren geen publicaties op hun gebied doen — zolang nl. totdat hun kennis van Philips-geheimen reeds weer verouderd is. Arbeiders in de textielververyen mogen gedurende een bepaald aantal jaren, nadat zij ontslag hebben genomen, niet bij concurrenten gaan werken. \t\t Alle denkbare voorzorgsmaatregelen worden dus genomen om de fabrieksgeheimen te beschermen. Anderzyds wordt ook alles in het werk gesteld om zulke geheimen te bemachtigen. Toen de Amerikanen West-Duitsland bezet hadden was hun eerste daad om de „specialisten\" naar Amerika over te brengen en hun tweede om zoveel mogeiyk fabrieksgeheimen te bemachtigen. Volgens officiële schattingen werden toen fabrieksgeheimen ter waarde van 10 milliard dollar buitgemaakt. Dat was dus „officieel\" spionnage in het groot. Maar in vredestyd geven de grote ondernemers, vooral in Amerika, jaariyks vele millioenen uit, om fabrieksgeheimen van anderen te bemachtigen, illegaal, door omkoperij, gewone diefstal en inbraak. Speciale politiecorpsen in particuliere dienst (zoals by Philips) moeten daarentegen weer er voor zorgen, dat dit soort roofridderconcurrentie verhinderd wordt. (Natuuriyk heeft die fabrieks-politie ook nog andere taken, zoals iedere Philipsarbeider weet.) \t\t Score: 0.5156\n",
      "Een groot aantal arbeiders werkzaam by de stofzuigerfabriek Efa Produka heeft jl. Maandag gedurende een half uur gestaakt tegen een dreigende tariefsverlaging. Toen een vertegenwoordiger van het zo gehate tariefbureau Beerenschot in de draaierij verscheen om de „tarieven aan een weteiischappehjk onderzoek te onderwerpen\", zetten de arbeiders van deze afdeling de machines stop. De arbeiders van de afdelingen slijperij, gietery, stuiterij en plaatwerkerij volgden hun voorbeeld op en gaven eensgezind te kennen niet op de „wetenschappelijke methoden\" van Beerenschot gesteld te zijn. \t\t De arbeiders hebben door deze proteststaking getoond, dat zij het smoesje van de directie, als zou dit tariefbureau tijdopmetingen doen in het belang van het personeel, op de juiste waarde weten te schatten. Het blijkt echter, dat sommige arbeiders uit de stempelmakerij nog niet volkomen begrijpen, dat de dreigende tariefsverlaging ook hun lonen zal verlagen. Het is verkeerd te denken, dat de stempelmakerij buiten schot zou blijven. Gezamenlijk zullen de arbeiders in staat zijn elke verslechtering af te wgzen. De stakers yan Maandag hebben getoond hun taak te hebben begrepen. (Arcor). \t\t Score: 0.5004\n"
     ]
    }
   ],
   "source": [
    "# Sentence mining from sentence-transformers\n",
    "sentences = sen[1:40]\n",
    "paraphrases = util.paraphrase_mining(\n",
    "    model,\n",
    "    sentences,\n",
    "    corpus_chunk_size=20, #len(sentences)\n",
    "    query_chunk_size=20,\n",
    "    top_k=20,\n",
    "    max_pairs=5)\n",
    "\n",
    "for paraphrase in paraphrases[0:10]:\n",
    "    score, i, j = paraphrase\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('paraphrase_test.pkl', \"wb\") as fOut:\n",
    "#    pickle.dump(paraphrase, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "import pickle\n",
    "\n",
    "with open('embeddings.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': sentences, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    \n",
    "#Load sentences & embeddings from disc\n",
    "with open('embeddings.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_sentences = stored_data['sentences']\n",
    "    stored_embeddings = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maar het waren juist deze geheime gegevens, die Louwers in handen wilde krijgen. Want de Tsjechoslowaakse industrie heeft op dit gebied een reusachtige voorsprong. Of nemen wij Philips! Toen de Duitsers ons land binnenvielen, vertrokken uit Eindhoven 30 auto's, volgeladen met geheime papieren: de productiegeheimen van Philips! Die moesten in veiligheid. De rest was niet belangrijk. Geen mens buiten de vesting van Philips komt over deze geheimen iets te weten. En binnen de vesting is een wetenschappelijk systeem uitgewerkt, dat er voor zorgt, dat de lagere ingenieurs en technici slechts gedeelten van een bepaald productieproces leren kennen, waarmee zij niets kunnen beginnen. Verteld wordt, dat eens een delegatie van Japanse belangstellenden Philips kwam bezoeken. De heren werden „overal\" rondgeleid, ze kregen een pracht van een diner aangeboden. Maar de Philipsdirectie vermoedde dat sommigen van hen micro-fototoestellen by zich hadden. Fouilleren kon men de „geëerde gasten\" niet. Dus werden de heren vriendelijk uitgenodigd ook de laatste snufjes van de Röntgen-afdeling te bezichtigen. Met een „Japanse\" glimlach nodigde men de Japanners uit, van de gelegenheid gebruik te maken. Ze lieten dus Röntgenfoto's van zich zelf maken en meteen waren de microfilms van hun verborgen toestellen onbruikbaar gemaakt. Het beschermen van fabrieksgeheimen is dus de eerste zorg der ondernemers. Wetenschapelijke werkers, die de dienst van Philips verlaten, mogen een bepaald aantal jaren geen publicaties op hun gebied doen — zolang nl. totdat hun kennis van Philips-geheimen reeds weer verouderd is. Arbeiders in de textielververyen mogen gedurende een bepaald aantal jaren, nadat zij ontslag hebben genomen, niet bij concurrenten gaan werken. \t\t Alle denkbare voorzorgsmaatregelen worden dus genomen om de fabrieksgeheimen te beschermen. Anderzyds wordt ook alles in het werk gesteld om zulke geheimen te bemachtigen. Toen de Amerikanen West-Duitsland bezet hadden was hun eerste daad om de „specialisten\" naar Amerika over te brengen en hun tweede om zoveel mogeiyk fabrieksgeheimen te bemachtigen. Volgens officiële schattingen werden toen fabrieksgeheimen ter waarde van 10 milliard dollar buitgemaakt. Dat was dus „officieel\" spionnage in het groot. Maar in vredestyd geven de grote ondernemers, vooral in Amerika, jaariyks vele millioenen uit, om fabrieksgeheimen van anderen te bemachtigen, illegaal, door omkoperij, gewone diefstal en inbraak. Speciale politiecorpsen in particuliere dienst (zoals by Philips) moeten daarentegen weer er voor zorgen, dat dit soort roofridderconcurrentie verhinderd wordt. (Natuuriyk heeft die fabrieks-politie ook nog andere taken, zoals iedere Philipsarbeider weet.) \t\t Score: 0.5156\n",
      "Een groot aantal arbeiders werkzaam by de stofzuigerfabriek Efa Produka heeft jl. Maandag gedurende een half uur gestaakt tegen een dreigende tariefsverlaging. Toen een vertegenwoordiger van het zo gehate tariefbureau Beerenschot in de draaierij verscheen om de „tarieven aan een weteiischappehjk onderzoek te onderwerpen\", zetten de arbeiders van deze afdeling de machines stop. De arbeiders van de afdelingen slijperij, gietery, stuiterij en plaatwerkerij volgden hun voorbeeld op en gaven eensgezind te kennen niet op de „wetenschappelijke methoden\" van Beerenschot gesteld te zijn. \t\t De arbeiders hebben door deze proteststaking getoond, dat zij het smoesje van de directie, als zou dit tariefbureau tijdopmetingen doen in het belang van het personeel, op de juiste waarde weten te schatten. Het blijkt echter, dat sommige arbeiders uit de stempelmakerij nog niet volkomen begrijpen, dat de dreigende tariefsverlaging ook hun lonen zal verlagen. Het is verkeerd te denken, dat de stempelmakerij buiten schot zou blijven. Gezamenlijk zullen de arbeiders in staat zijn elke verslechtering af te wgzen. De stakers yan Maandag hebben getoond hun taak te hebben begrepen. (Arcor). \t\t Score: 0.5004\n",
      "Alle denkbare voorzorgsmaatregelen worden dus genomen om de fabrieksgeheimen te beschermen. Anderzyds wordt ook alles in het werk gesteld om zulke geheimen te bemachtigen. Toen de Amerikanen West-Duitsland bezet hadden was hun eerste daad om de „specialisten\" naar Amerika over te brengen en hun tweede om zoveel mogeiyk fabrieksgeheimen te bemachtigen. Volgens officiële schattingen werden toen fabrieksgeheimen ter waarde van 10 milliard dollar buitgemaakt. Dat was dus „officieel\" spionnage in het groot. Maar in vredestyd geven de grote ondernemers, vooral in Amerika, jaariyks vele millioenen uit, om fabrieksgeheimen van anderen te bemachtigen, illegaal, door omkoperij, gewone diefstal en inbraak. Speciale politiecorpsen in particuliere dienst (zoals by Philips) moeten daarentegen weer er voor zorgen, dat dit soort roofridderconcurrentie verhinderd wordt. (Natuuriyk heeft die fabrieks-politie ook nog andere taken, zoals iedere Philipsarbeider weet.) \t\t WATEN STEDT-S ALZGITTER, 0 Maart (Reuter). — Meer dan 1000 Duitse arbeiders hebben een complete veldslag geleverd tegen een ontniautelingspatrouiKe, die cpflracht''had de~'lnstallaties v:vn de vroegere Hennann Göring-Staalfabrieken in Wateastedt op te blazen. Deze arbeiders drongen de fabrieken binnen en doofden de lonten van de explosieve ladingen, nadat zij de Duitse ontmante-I'ngspatrouille en de bewakers van deze patrouille uiteen hadden geslagen. De Britse ambtenaar, die cS leiding had van de ontnnr.telingsoperaties, riep de hulp van de politie in, die de orde wist te herstellen: Terwijl hc* gevecht gaande was tussen de ontmantelingspatrouille en de demonstranten, drongen ongeveer 100 woedende arbeiders het kantoor van de ontmantelLngs-commissie binnen en sloegen daar alles kort en k!cin. AHe papieren werden op straat geworpen en daar in brand gestoken. \t\t Score: 0.4718\n",
      "staan. In deze loodsen is een aantal daklozen ondergebracht. In verband met de verkoop van i de grond, waar deze loodscn op i s aan, moeten zij weg en elders : weer opgebouwd wouen. Dit j kost de gemeente een behoorlijk ; bedrag.' Ik ben er nu in geslaagd van de N.V. Proctor en- Gamble gedaan te krijgen, dat de fabriek oen deel van je kosten iop zich zal riemen. ' 'Terugkomende op de door •hem onwettig geachte berioe-1 rning van de heer Dul Arnowo j lo'v burgemeester van Soerabaja, vertelde de heer Radjamin, dat-j hij Maandag op :verzoek van en j vergezeld door twee leden van ( de Parindra ,die hem met schorsing als lid van do partij dreigden als hij nie: aan hun verzoek ; voldeed, een bezoek aan kolo-j ! nel Sungkono hooft gebracht. | Doel van dit bezoek was onder ! de aandacht van de Militaire i ; Gouverneur te brengen, dat het j j nu dringend \"noodzakelijk werd; : een burgemeester te benoemen, i mede gezien de in de gemeente-! raad aangenomen motie, waarin j ;de Militaire Gouverneur . ge-! I vraagd wordt een wali kota aan tc. stellen- •* ; Kolonel Sungkono anwoordde, , dat hij over deze kwestie ten .spoedigs :e overleg zou plegen mét do horen Samadikun c n Dul Arnowo. Daarna zcu door hem een besluit geslagen worden. Volkomen onverwacht hoorde | ik gisteren, da: de heer Dul: Arnowo door de heer Samadikun is benoemd. Met de krant in de hand ben ik daarop - gisteravond weer ' naar kolonel Sungkono loege-; gaai. Ik heb zijn aandach. , gevestigd op twee berichten in ae krant ii.l. de mot'ie van de ; gein ten'era ad, waarin de Mili-; t&ire Gouverneur. verzocht'. wordt een burgemeester aan te s.ellen en op het bericht, waarin ; gemeld wordt, dat de heer Dul Arnowo tot waarnemend bur- j gemcester is benoemd door de Gouverneur. De hcer_Radjamin doelde kolonel Sungkono mede van plan te zijn hedenmorgen op de stoel van de burgemees- j i ter te gaan zitten en zo spoc*: j dig-mogelijk het bestuur over j ' de stad op zich te nemeni ~Zoals reeds gezegd, ach t-ik de handelwijze. van de heer Samadikun volkomen onbegrijpelijk. Immers jl. Zaterdag op de bijeenkomst met do socreta-: ris-generaal van Binnenlandse j Zaken van de R.I.S. mr. Wong- j ! sonegoro, zei dc heer Samadi- I kun nog, dat zoveel mogelijk ; oudé \"Republikeinse ambtenaren : ; het bestuur van federalisten moesten overnemen. In deze bijeenkomst vestigde ik er de aandacht op, dat reeds . verschillende functies door fe- • deralisten waren overgedragen en ik vroeg wanneer de burge; meestersfuncties in Making en . Soerabaja aan de beurt zouden zijn- De lieer Samadikun antwoordde mij: „Heb nog even geduld, ook dat zal spoedig gebeuren\". . Ik acht het benoomingsbc- i j siui: van dc hoer Dul Arnowo ! ook daarom onwettig, omdat do, ; laatste tijd alle benoemingen ; zijn uitgegaan van de Militaire Gouverneur. Zo b.v. die van! resident en regent van Soer:i--' baja en die van directeur van ' ■do haven. Waarom moet\" daar Zoals gezegd, troffen wij do heer .Kadjamin Nasoetion hedenmorgen Tn do kamer'van de.. • burgemeester van Soerabaja- j 'Op zijn. tafel lagen verscheide-i - re besluiten en andere papie-[ ren, waarop hij do wettigheid j van zijn handelen baseert Allereerst toonde hij. oJls qcii waarop als functie* vermeld slaat : burgc- j meester ■» vttu Soerabaja e Deze ; identiteitskaart, ucrd op 7 Juli , 19.'/7 tc Malttng afgegeven eti is ondertekend, door de heer Dul Arnowo, in diens functie vim waarnemend ' gouverneur van Oost-\\iva. Vertier liet dr heer Radjamin ons een brief zien . van de h? er Soeroso, - hoofdbestuurslid van tic Parindra ,waarin deze aan de heer , IZiSljumin verslag uitbrmgi i *van hel onderhoud, dat hij c;> 9 Fbbruari jl met de minister van Binnenlandse Zaken van de Rcpitblik Indoncsia, mr■ £<.v- j santo had- In deze brief rcr-1 telt de heer Soeroso, dat de 1 minister dun hem heeft bevestigd. dat dc heer Radjamii nog steeds als burgemeester va:t j Soerabaja wordt beschouwd,; nimmer ontslagen iccrd en op het moment, dat de stad.Soc'rabttjV weer zal worden gevoegd bij de Rcpitblik Indoncsia als enige aanspraak kan maken op de burgemeestersplaats- Op 26 Febr. heeft de heer Soe: reso de woorden van de minister nog eens bevestigd in j de lodenvergadering van de ! Parindia, die hier ter stede In : de Ge.dong Nasional Indonesia werd gehouden- ' ■ Oe regerirg van de liopublik Indonesia. beschouwt mij dur nog\" steeds, zo vertelde de heer' \"Radjamin verder, als burgèmeester van Soerabnjn.- I!: wordt ook als zodanig, door Djokja uitbetaald- Ziet U maar hier- Uit de stapel papieren kwam een uitbot alingsbewijs over Januari jl- Dit bewijs is j afgestempeld dcor het kantoer i vai de Gouverneur van Oosti Java\". Hieruit blijkt dus boven''ditn. dat de heren Samadiliur. ! c-n Dul Arnowc volledig var.-, i de werkelijke situatie op de j hoogte zijn.\" aldus do heer Ra-! i djamin- Het is aldus, zq vervolgt d : heer Radjamin, voor mij dun ook een i aaJJ:I waarom ~ij zo 'gehandeld hebben- Eertijk gr-\" ' zegd ben ik eigenlijk r en bcct\\je verlegen om V dit alle maal tc verfeilt ii. Ons orderhoud werd evenj onderbroken door een telefoon- j . tje- Hot was oen gelukwens ! voor de lieer Radjamin, die hij ! echter afwimpelde mot de mededeling: „Ik heb het bestuur 1 nog niet overgenomen-\" Aansluitend op uit telefoon- i gesprek, vertelde de lieer Ra-djamin, dat hij al maandenlang ■ dcor zeer vele Soorabajanen als j burgemeester vail Soerabaja wordt beschouwd- Honderden stadgenoten zijn in de afgelopen maanden bij mij thuis gc, komen om raad tc vragen. Ik voerde in mijn hoedanig- j ■ heid van burgemeester onlange ! \\ ook besprekingen met de diree-1 ! tio val de N.V. Proctor en I Gamble. Het ging over het i verplaatsen van een aantal j loodsen, die nchtei—de-fabriek\"' van de N-V. Proctor en Gamble i, lui plotseling voor burgemees\" ' tor van Soerabaja van afgeweken worden ? • lenslottc toonde de heer Padjamin ens een telegram, dat hij gisteren heeft gezonden aan het ministerie van IJinnenlandsc Zaken Djokja. In, dit t-legram \\Taegt hij de goedkeuring van de regering van zijn besluit de hem toekomende plaats van burgemeester van Soerabaja'weer 'te hebben in- • genomen. ' - i To. zove'r ons onderhoud met! d? luer Radjamin- - j Verder vernamen wij nog,, dat het feit, dat de heer Radja- ■ mi hedenmorgen zijn cr.le! Vhnts achter.de burgemeester 'af el weer herft. ingenomen. i:i j doorgegeven aan hot kantoor ' >'an de heer Samadikun. Van ! enige reactie was bij het afslui- [ tcri van iit bericht nog niets bekend. Zoals wij gisteren hebben gemeld, bevinden zowel de lieer Samadikun als de heer Dul Arnowo zich vandaag on Ma- i doora. - • 1 Het ligt ?n de bedoeling, dat j de heer Dul Arnowo morgen- | ochtend om negen uur cp bi t • kantoer- voor Gemeentezaken , z*»l worden geïnstalleerd als [ waarnemend burgemeester van i \"oerabaja. J \t\t Heeft Soerabaja, «n vooral de benedenstad in de regentijd last van een overvloed van regenwater, dat niet snel genoeg kan worden afgevoerd, aan de andere kant heeft de stad gebrek aan leidingwater. Het. is weer de benedenstad, die daar het meest van te lijden heeft. Maar bij de aanvoer van leidingwater ligt de zaak wel iets anders, dan bij de afvoer van groiul- en regenwater. De hoeveelheid regenwater is niet groter geworden, het gebruik van leidingwater daarentegen wel. Thans produceert de waterzuiveringsinstallatie op Ngagel per maand meer, dan voor de oorlog in een heel jaar. Dat ondanks deze opvoering van de capaciteit van Ngagel er ren tekort aan leidingwater Is. komt ten dele door de aanwas van de bevolking, ten «lele door de vele lekkages in het oude materiaal en tenslotte wordt. Uit grotere waterverbruik ook veroorzaakt door een grotere ~waterbeschaving\" van de Indonesische. bevolking. Thans v,il men in de kampong zijn waterleiding hebben, terwijl dat vroeger in veel mindere mate het geval was \t\t Score: 0.4493\n",
      "..__, , — 1 ._..... , , .... Hedenavond BELLEVUE (ingang Leidsekade). Viering INTERNATIONALE VHOEJVENJDAG Spreekster: ANNIE GELOK. Medewerking: Lion Contran, piano; zangkoor „Morgenrood\"; Lizzy May en Jan Wronk, pantomime. — Massazang. Toegangsprijs 30 cent. Nog enkele kaarten aan de zaal. Aanvang 8 uur precies. \t\t Hilversum 1, SOl m. 20.00-20.05 iJWS- JmK Sweelinck-kwartet. 21.50 Commen__r **}?$ competiüe. 22.00 Marinierskapel. 22.J» ejt< Gr. 23.— Nws. 23.15 Utrechts Stedelijk OrJ 23.45 Jascha Heifetz (viool). 24.00 Sluiti\"8 ,» 20' _ Hilversum 11. 415 m. 20.00-20.05 Nws. \",M The Ramblers. 20.55 Christopher \"$& (hrsp.). 22.15 Kwintet. 22.45 Causerie. *» Nws. 23.15 Swing and sweet. \"3.40 Piaß\"\" 24.00 Sluiting. Programma voor Donderdaê Hilversum 1, SOl m. 8.00 Nws. 8.15 Gt-Jfi Schoolradio. 10.00-10.15 Residentie °I pK 10.45 Christelijke liederen. 11.00 Voor °%cc% ken. 11.45-12.00 Schoolradio. 12.03 eert. 13.00 Nws. 13.20 Pianovoordracht. ,j.*» Gr. 14.45 Voor de vrouw. 15.30-16.00 GT.J„o>' Gr. 17.00 Voor de Jeugd. 17.30-18.00 Met/°M Orkest. 18.30 Reg. Uitz. 19.00 Nws. 19.10 de Jeugd. 19.40 Radiokrant 20.00 Nwf- $& Gevar. Programma. 22.05 Gr. 22.15 22.35-22.45 Gr. 23.00 Nws. 23.16 Gr. Sluiting. .. te\" Hilversum 11, 4.15 m. 8.00 Nws. 8.15 OcW fi_ blad. 8.40 Gr. 8.55 Voor de vrouw. 9.30 Waterstanden. 9.35-10.10 Gr. 10.1$ Kx\\o*< orkest. 10.50 Voor de kinderen. 11.00 C»r atJf; orgel en bas-bariton. 11.45 TuinbouwPrjeiiJj; 12.00 Semi-klassieke muziek. 12.30 MeOeuj3J: gen. 12.38 Zigeunerensemble. 13.00 Nw»; <K Musette orkest, 13.50 Gr. 14.00 Voo'jj.iJ' Vrouw. 14.30 Sopraan, fluit en piano'r|teS,J' Voor de zieken. 16.00 Promenade ,°««B 16.30 Gr. 17.00 Voor de Jeugd. 17.6° iB.ffl Uitz. 18.00 Nws. 18.16 Sportpraatje- j 9£ Strijkorkest. 19.00 Voor de kinderen, zi# Accordeonorkest. 19.30 Radio Voll.sinun,irfs school. 22.00 Nws. 20.05 Gevar. Progr* 21.45 Radio Philharmonisch orkest, .„of* Causerie. 22.45 Gr. 23.00 Nws. 23.15 or actualiteiten. 23.30 Gr. 24.0 U Sluiting. \t\t Score: 0.4406\n",
      "Gistermorgen heeft een onderhoud plaatsgevonden tussen een delegatie van D.U.W.-arbeiders en een aantal ambtenaren van het gewestelijk arbeidsbureau te Amsterdam. De delegatie, die bestond uit een vijftal arbeiders in de D.U.W.-objecten Halfweg, Twiske, Someren by Eindhoven, V. .eringermeer en Bosplan, werd verge: _d door de E.V.C.-bestuurder J. v. d. Linden. Krachtig protesteerde de delegatie namens al hun collega's werkzaam op bovengenoemde obj 2ten, tegen het feit dat het G.A.B. D.U.W.-arbeiders oproept om onderkruiperswerk te aanvaarden bij de firma Wiersma, waar zoals wij eerder schreven, reeds ongeveer 3 weken een staking plaats vindt. Verder wees de delegatie er op dat het niet aangaat niet vakbekwame grondwerkers in het z.g. „diepriool\" aan het werk te zetten in verband met de grote kans op ongelukken. De ambtenaren hadden evenwel voor deze deskundige argumenten geen oor en zeiden te zullen voortgaan D.U.W.-art riders op te roepen ondanks dat de aannemer op wel zeer laatdunkende wyze heeft te kennen gegeven; „dat het merenr'eel der D.U.W.-arbeiders geen knip voor de neus waard is.\" De D.U.W.-arbeiders en de stakers van Wiersma zullen zowel deze man als de ambtenaren van het G.A.B, te verstaan moeten geven dat zij samen met alle bouwvakarbeiders, aan deze ronselprak» tyk een einde zullen maken. \t\t Een groot aantal arbeiders werkzaam by de stofzuigerfabriek Efa Produka heeft jl. Maandag gedurende een half uur gestaakt tegen een dreigende tariefsverlaging. Toen een vertegenwoordiger van het zo gehate tariefbureau Beerenschot in de draaierij verscheen om de „tarieven aan een weteiischappehjk onderzoek te onderwerpen\", zetten de arbeiders van deze afdeling de machines stop. De arbeiders van de afdelingen slijperij, gietery, stuiterij en plaatwerkerij volgden hun voorbeeld op en gaven eensgezind te kennen niet op de „wetenschappelijke methoden\" van Beerenschot gesteld te zijn. \t\t Score: 0.4356\n",
      "..__, , — 1 ._..... , , .... Hedenavond BELLEVUE (ingang Leidsekade). Viering INTERNATIONALE VHOEJVENJDAG Spreekster: ANNIE GELOK. Medewerking: Lion Contran, piano; zangkoor „Morgenrood\"; Lizzy May en Jan Wronk, pantomime. — Massazang. Toegangsprijs 30 cent. Nog enkele kaarten aan de zaal. Aanvang 8 uur precies. \t\t Speeltijden ||\"^T>77l J.70 JswA Zondag Matinee 18 u. v. m. r Plaatsbespreken 9-12 u. v.m. l Vrijkaarten lnw. A/h loket. I ' HEDEN voor hei laatst: v. ® „CAN'T HELP SINGING\" met DEANNA DURBIN-ROBFRT PAIGE-AKIM TAMiROFF. VANAF WORGEN: * 1 . . \t\t Score: 0.4229\n",
      "Woensdag as. zal hier ter stede een bijeenkomst plaats vin.den van ambtenaren van de voorlichtingsdienst in . Ooai> Java- Deze bijeenkomst zal vei\" moedelijk worden bijgewoond door de heer Sutomo i?an het ministerie rail Voorlichtingvan de R.I- \t\t Hedenmorgen te omstreeks acht uur is de heer Iladjamin Nasoetion, Republikeins burgemeester van Soerabaja in evacuatie, in ambtscostuum naar het kantoor van gei nieentezaken hier ter stede gekomen om in de kamer van Ide burgemeester plaats te nemen op de stoel van de VVali ' Kota. Zoals wij hebben gemeld is gisteren de heer I)ul Arnowo bij besluit .van de gouverneur van Oost-Java, de heer Samadikun; benoemd tot waarnemend burgemeester van Soerabaja..',: .f': V' i' •. In een.onderhoud, dat wij'- hedenmorgen met de heer Kadjamin in de burgemeesterskamer hadden, deelde hij ons mede het benoemingsbesluit van de heer l)ul Arnowo aflet te erkennen. Niet Gouverneur Samadikun moet een burgemeester, beroemen, doch de .Militair Gouverneur van Oost-Java. Bovendien ben ik aldus de heer Kadjamin dcor de regering te Djokja; die mij *n 1!)45 tot burgemeester van Soerabaja benoemd heeft, , nooit- ontslagen. Nog zeer onlangs is mij door Djokja de verzekering gegeven, dat ik nog steeds burgemeester in evacuatie ben.\" Nog vandaag, zo voegde de heer Kadjamin hieraan toe, zal 'ik de bevoegdheden van de loco-burgemeester, de heer Pranoto overnomen. \t\t Score: 0.4142\n",
      "Een groot aantal arbeiders werkzaam by de stofzuigerfabriek Efa Produka heeft jl. Maandag gedurende een half uur gestaakt tegen een dreigende tariefsverlaging. Toen een vertegenwoordiger van het zo gehate tariefbureau Beerenschot in de draaierij verscheen om de „tarieven aan een weteiischappehjk onderzoek te onderwerpen\", zetten de arbeiders van deze afdeling de machines stop. De arbeiders van de afdelingen slijperij, gietery, stuiterij en plaatwerkerij volgden hun voorbeeld op en gaven eensgezind te kennen niet op de „wetenschappelijke methoden\" van Beerenschot gesteld te zijn. \t\t WATEN STEDT-S ALZGITTER, 0 Maart (Reuter). — Meer dan 1000 Duitse arbeiders hebben een complete veldslag geleverd tegen een ontniautelingspatrouiKe, die cpflracht''had de~'lnstallaties v:vn de vroegere Hennann Göring-Staalfabrieken in Wateastedt op te blazen. Deze arbeiders drongen de fabrieken binnen en doofden de lonten van de explosieve ladingen, nadat zij de Duitse ontmante-I'ngspatrouille en de bewakers van deze patrouille uiteen hadden geslagen. De Britse ambtenaar, die cS leiding had van de ontnnr.telingsoperaties, riep de hulp van de politie in, die de orde wist te herstellen: Terwijl hc* gevecht gaande was tussen de ontmantelingspatrouille en de demonstranten, drongen ongeveer 100 woedende arbeiders het kantoor van de ontmantelLngs-commissie binnen en sloegen daar alles kort en k!cin. AHe papieren werden op straat geworpen en daar in brand gestoken. \t\t Score: 0.4063\n",
      "„In ons land is de vrede levenswet' \t\t Lid Vredescomité: „Het gaat goed in de haven \" \t\t Score: 0.4054\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around with BERTje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 11:23:32 - Lock 5397104528 acquired on /Users/leonardovida/.cache/torch/transformers/75d9be4cc7910048b3bdd477c435ffc46330193705f74eaf9a4f375cd3be28b2.1e00a56207196ed1759c49bdd1fa93c2fb20273d59fabb0c4c8092f7beb773c2.lock\n",
      "2020-09-02 11:23:32 - https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/vocab.txt not found in cache or force_download set to True, downloading to /Users/leonardovida/.cache/torch/transformers/tmppdbo_bik\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c05e6f8d9f4c20ab402b55cece1e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=241440.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 11:23:33 - storing https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/vocab.txt in cache at /Users/leonardovida/.cache/torch/transformers/75d9be4cc7910048b3bdd477c435ffc46330193705f74eaf9a4f375cd3be28b2.1e00a56207196ed1759c49bdd1fa93c2fb20273d59fabb0c4c8092f7beb773c2\n",
      "2020-09-02 11:23:33 - creating metadata file for /Users/leonardovida/.cache/torch/transformers/75d9be4cc7910048b3bdd477c435ffc46330193705f74eaf9a4f375cd3be28b2.1e00a56207196ed1759c49bdd1fa93c2fb20273d59fabb0c4c8092f7beb773c2\n",
      "2020-09-02 11:23:33 - Lock 5397104528 released on /Users/leonardovida/.cache/torch/transformers/75d9be4cc7910048b3bdd477c435ffc46330193705f74eaf9a4f375cd3be28b2.1e00a56207196ed1759c49bdd1fa93c2fb20273d59fabb0c4c8092f7beb773c2.lock\n",
      "2020-09-02 11:23:33 - loading file https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/vocab.txt from cache at /Users/leonardovida/.cache/torch/transformers/75d9be4cc7910048b3bdd477c435ffc46330193705f74eaf9a4f375cd3be28b2.1e00a56207196ed1759c49bdd1fa93c2fb20273d59fabb0c4c8092f7beb773c2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 11:23:34 - Lock 5357248464 acquired on /Users/leonardovida/.cache/torch/transformers/6702c5c53edb76b65d71f73ff2d9811ba62f16257ea58e36dedceffd71290a6a.1a78bd120fe46d78b55efa59f4ffa1dafcc9242743ab9fd6629d1b56672c9119.lock\n",
      "2020-09-02 11:23:34 - https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/config.json not found in cache or force_download set to True, downloading to /Users/leonardovida/.cache/torch/transformers/tmpqyn8q3s_\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147b29228fb34fab8f65f9318722d9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 11:23:34 - storing https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/config.json in cache at /Users/leonardovida/.cache/torch/transformers/6702c5c53edb76b65d71f73ff2d9811ba62f16257ea58e36dedceffd71290a6a.1a78bd120fe46d78b55efa59f4ffa1dafcc9242743ab9fd6629d1b56672c9119\n",
      "2020-09-02 11:23:34 - creating metadata file for /Users/leonardovida/.cache/torch/transformers/6702c5c53edb76b65d71f73ff2d9811ba62f16257ea58e36dedceffd71290a6a.1a78bd120fe46d78b55efa59f4ffa1dafcc9242743ab9fd6629d1b56672c9119\n",
      "2020-09-02 11:23:34 - Lock 5357248464 released on /Users/leonardovida/.cache/torch/transformers/6702c5c53edb76b65d71f73ff2d9811ba62f16257ea58e36dedceffd71290a6a.1a78bd120fe46d78b55efa59f4ffa1dafcc9242743ab9fd6629d1b56672c9119.lock\n",
      "2020-09-02 11:23:34 - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/config.json from cache at /Users/leonardovida/.cache/torch/transformers/6702c5c53edb76b65d71f73ff2d9811ba62f16257ea58e36dedceffd71290a6a.1a78bd120fe46d78b55efa59f4ffa1dafcc9242743ab9fd6629d1b56672c9119\n",
      "2020-09-02 11:23:34 - Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 11:23:35 - Lock 5390180048 acquired on /Users/leonardovida/.cache/torch/transformers/e5754f612ca0f16edba5b775fdddba806751f5e4b87c5e7f16cc0c8d8d17df4d.b7c03627733fd0712f078a4d3a31ad964550f50a6113efdf874ecbcf5ddf6b53.lock\n",
      "2020-09-02 11:23:35 - https://cdn.huggingface.co/wietsedv/bert-base-dutch-cased/pytorch_model.bin not found in cache or force_download set to True, downloading to /Users/leonardovida/.cache/torch/transformers/tmp1wjgs76_\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96474afab91444b2b20b8e15275941ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=438869143.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 11:24:36 - storing https://cdn.huggingface.co/wietsedv/bert-base-dutch-cased/pytorch_model.bin in cache at /Users/leonardovida/.cache/torch/transformers/e5754f612ca0f16edba5b775fdddba806751f5e4b87c5e7f16cc0c8d8d17df4d.b7c03627733fd0712f078a4d3a31ad964550f50a6113efdf874ecbcf5ddf6b53\n",
      "2020-09-02 11:24:36 - creating metadata file for /Users/leonardovida/.cache/torch/transformers/e5754f612ca0f16edba5b775fdddba806751f5e4b87c5e7f16cc0c8d8d17df4d.b7c03627733fd0712f078a4d3a31ad964550f50a6113efdf874ecbcf5ddf6b53\n",
      "2020-09-02 11:24:36 - Lock 5390180048 released on /Users/leonardovida/.cache/torch/transformers/e5754f612ca0f16edba5b775fdddba806751f5e4b87c5e7f16cc0c8d8d17df4d.b7c03627733fd0712f078a4d3a31ad964550f50a6113efdf874ecbcf5ddf6b53.lock\n",
      "2020-09-02 11:24:36 - loading weights file https://cdn.huggingface.co/wietsedv/bert-base-dutch-cased/pytorch_model.bin from cache at /Users/leonardovida/.cache/torch/transformers/e5754f612ca0f16edba5b775fdddba806751f5e4b87c5e7f16cc0c8d8d17df4d.b7c03627733fd0712f078a4d3a31ad964550f50a6113efdf874ecbcf5ddf6b53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-02 11:24:40 - All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "2020-09-02 11:24:40 - All the weights of BertModel were initialized from the model checkpoint at wietsedv/bert-base-dutch-cased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"wietsedv/bert-base-dutch-cased\")\n",
    "model = BertModel.from_pretrained(\"wietsedv/bert-base-dutch-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HistAware",
   "language": "python",
   "name": "histaware"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
