{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "diverse-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import wandb\n",
    "\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import alpino\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-american",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-oakland",
   "metadata": {},
   "source": [
    "Libraries settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "supposed-comparative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-woman",
   "metadata": {},
   "source": [
    "Model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "nasty-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'wietsedv/bert-base-dutch-cased'\n",
    "LEN_SENTS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-thomas",
   "metadata": {},
   "source": [
    "Wandb settings to track experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "revised-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(project=\"histaware\")\n",
    "wandb.config.dataset = \"edo_1990s\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-parade",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "silent-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas = pd.read_csv(\"sentiment/edo_1990s_gas_labeled.csv\")\n",
    "gas = gas[gas.energy == \"Y\"]\n",
    "gas = gas[gas.sentiment != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "capable-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "olie = pd.read_csv(\"sentiment/edo_1990s_olie_labeled.csv\")\n",
    "olie = olie[olie.energy == \"Y\"]\n",
    "olie = olie[olie.sentiment != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "informal-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "kool = pd.read_csv(\"sentiment/edo_1990s_kool_labeled.csv\")\n",
    "kool = kool[kool.energy == \"Y\"]\n",
    "kool = kool[kool.sentiment_coal != None]\n",
    "kool.drop([\"sentiment\", \"sentiment_gas\", \"sentiment_oil\"], axis=1, inplace=True)\n",
    "kool.rename(columns = {\"sentiment_coal\": \"sentiment\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "owned-reputation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1066, 24)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([gas, olie, kool], ignore_index=True)\n",
    "df.text.replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "df.sentiment.replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['sentiment'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-norman",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "municipal-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_sentiment = {\"sentiment\": {\"VN\": 1, \"NG\": 2, \"NE\": 3, \"PO\": 4, \"VP\": 5}}\n",
    "df = df.replace(cleanup_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-melbourne",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "featured-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.countplot(df.sentiment)\n",
    "#plt.xlabel('review sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-shopping",
   "metadata": {},
   "source": [
    "Reduce from 5 labels to 3 because of lack of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "mexican-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentiment(rating):\n",
    "    rating = int(rating)\n",
    "    if rating <= 2:\n",
    "        return 0\n",
    "    elif rating == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df['sentiment'] = df.sentiment.apply(to_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-moderator",
   "metadata": {},
   "source": [
    "Plot result reduction of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "radical-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.countplot(df.sentiment)\n",
    "#plt.xlabel('review sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-reading",
   "metadata": {},
   "source": [
    "### Split text and explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "multiple-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unite(l, n):\n",
    "    \"\"\"Unite sentences previously split using nltk.tokenize.\"\"\"\n",
    "    count = []\n",
    "    chunks = []\n",
    "    sents = []\n",
    "    for s in l:\n",
    "        count.append(len(s.split()))\n",
    "    value = 0\n",
    "    prev_idx = 0\n",
    "    for i in range(0, len(count)):\n",
    "        if value == 0:\n",
    "            value = value + count[i]\n",
    "        elif (i+1 == len(count)):\n",
    "            chunks.append(l[prev_idx:i])\n",
    "            value = 0\n",
    "        elif value >= n:\n",
    "            chunks.append(l[prev_idx:i])\n",
    "            prev_idx = i\n",
    "            value = 0\n",
    "        else:\n",
    "             value = value + count[i]\n",
    "    for c in chunks:\n",
    "        sents.append(' '.join(c))\n",
    "    return(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "moderate-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(s, n):\n",
    "    \"\"\"Split sentences only using the number of words.\"\"\"\n",
    "    pieces = s.split()\n",
    "    return [\" \".join(pieces[i:i+n]) for i in range(0, len(pieces), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "current-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_split\"] = df[\"text\"].apply(sent_tokenize)\n",
    "df[\"text_split\"] = df[\"text_split\"].apply(unite, n = LEN_SENTS)\n",
    "df.text_split.replace([], np.nan, inplace=True)\n",
    "df.dropna(subset=['text_split'], inplace=True)\n",
    "# Cancel all text_split == 0\n",
    "df.drop(df[df.text_split.map(len) == 0].index, inplace=True)\n",
    "# Currently not splitting the cleaned sentences\n",
    "#df[\"text_clean_split\"] = df[\"text_clean\"].apply(splitter, n = LEN_SENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-softball",
   "metadata": {},
   "source": [
    "### Load Pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "likely-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-synthetic",
   "metadata": {},
   "source": [
    "### Verify the max tokens length in text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "vietnamese-combat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 0\n",
    "def max_len(x):\n",
    "    lengths = []\n",
    "    for i in x:\n",
    "        lengths.append(len(tokenizer.encode(i.split())))\n",
    "    res = max(lengths)\n",
    "    return(res)\n",
    "    \n",
    "temp = df[\"text_split\"].apply(max_len)\n",
    "MAX_LEN = max(temp)\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "integral-smith",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2342, 25)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.explode('text_split')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "marine-transition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "token_lens = []\n",
    "for txt in df.text_split:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "satisfied-image",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 489,
       "width": 731
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 512]);\n",
    "plt.xlabel('Token count');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-metadata",
   "metadata": {},
   "source": [
    "### Train test and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "equipped-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "  df,\n",
    "  test_size=0.25,\n",
    "  random_state=RANDOM_SEED\n",
    ")\n",
    "df_val, df_test = train_test_split(\n",
    "  df_test,\n",
    "  test_size=0.5,\n",
    "  random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-weather",
   "metadata": {},
   "source": [
    "### Dataset and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "caroline-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HADataset(Dataset):\n",
    "    # Characterizes a dataset for Pytorch\n",
    "    def __init__(self, articles, labels, tokenizer, max_len):\n",
    "        # Initialization\n",
    "        self.articles = articles\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Total number of articles\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Generates one sample of the data/article\n",
    "        article = str(self.articles[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            article,\n",
    "            add_special_tokens=True,\n",
    "            #padding='max_length',\n",
    "            truncation=True, # there should be no need of this as all p should be smaller than this\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "          'article_text': article,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dominant-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, batch_size, num_workers):\n",
    "    ds = HADataset(\n",
    "        articles=df.text_split.to_numpy(),\n",
    "        labels=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "      )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size,\n",
    "        num_workers\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "senior-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dated-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "#train_data_loader = create_data_loader(df_train, tokenizer, batch_size, num_workers)\n",
    "#val_data_loader = create_data_loader(df_val, tokenizer, batch_size, num_workers)\n",
    "#test_data_loader = create_data_loader(df_test, tokenizer, batch_size, num_workers)\n",
    "\n",
    "MAX_LEN = 512 #rewriting value computed above\n",
    "\n",
    "# Create Dataset (?)\n",
    "dataset_train = HADataset(\n",
    "        articles=df_train.text_split.to_numpy(),\n",
    "        labels=df_train.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN)\n",
    "\n",
    "dataset_val = HADataset(\n",
    "        articles=df_val.text_split.to_numpy(),\n",
    "        labels=df_val.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-rhythm",
   "metadata": {},
   "source": [
    "### Modified huggingface hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "determined-florida",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='wietsedv/bert-base-dutch-cased', vocab_size=30000, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "from transformers import (AutoModel, AutoTokenizer, AutoConfig,\n",
    "                          Trainer, TrainingArguments)\n",
    "\n",
    "config = AutoConfig.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#print(config)\n",
    "bert_model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#print(bert_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "annual-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"hist-aware/notebooks/models\",\n",
    "    overwrite_output_dir = False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8, # default is 8\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"hist-aware/notebooks/logging\",\n",
    "    eval_steps=500,\n",
    "    seed=RANDOM_SEED,\n",
    "    label_names=\"labels\", # check this\n",
    "    disable_tqdm=False # check this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-steam",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 18:16:33,865\tWARNING worker.py:1034 -- Warning: The actor ImplicitFunc has size 439034169 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n",
      "2021-02-10 18:16:34,500\tWARNING util.py:141 -- The `start_trial` operation took 8.913 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 12.8/39.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/5 CPUs, 1/1 GPUs, 0.0/20.95 GiB heap, 0.0/7.23 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19\n",
      "Number of trials: 1/100 (1 RUNNING)\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+---------+\n",
      "| Trial name             | status   | loc   |   learning_rate |   num_train_epochs |   per_device_train_batch_size |    seed |\n",
      "|------------------------+----------+-------+-----------------+--------------------+-------------------------------+---------|\n",
      "| _objective_1420e_00000 | RUNNING  |       |     5.61152e-06 |                  4 |                             4 | 38.0779 |\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+---------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 18:16:43,827\tWARNING worker.py:1034 -- The actor or task with ID ffffffffffffffff205ffc5701000000 cannot be scheduled right now. It requires {GPU: 1.000000}, {CPU: 1.000000} for placement, but this node only has remaining {accelerator_type:RTX: 1.000000}, {node:192.168.1.142: 1.000000}, {CPU: 4.000000}, {object_store_memory: 7.226562 GiB}, {memory: 20.947266 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m 2021-02-10 18:16:46.689090: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m 2021-02-10 18:16:46.689146: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(pid=19154)\u001b[0m /home/leonardovida/.pyenv/versions/3.8.6/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(pid=19154)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: Currently logged in as: leonardovida (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: wandb version 0.10.18 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: Tracking run with wandb version 0.10.12\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: Syncing run hist-aware/notebooks/models\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb:  View project at https://wandb.ai/leonardovida/huggingface\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb:  View run at https://wandb.ai/leonardovida/huggingface/runs/7bvcikji\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: Run data is saved locally in /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/wandb/run-20210210_181654-7bvcikji\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m 2021-02-10 18:16:55,792\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     tensor = as_tensor(value)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m ValueError: too many dimensions 'str'\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     trainer.train(model_path=model_path, trial=trial)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     for step, inputs in enumerate(epoch_iterator):\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     data = self._next_data()\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     return self.collate_fn(data)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     batch = self.tokenizer.pad(\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     tensor = as_tensor(value)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m ValueError: too many dimensions 'str'\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.pyenv/versions/3.8.6/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m output = fn()\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     trainer.train(model_path=model_path, trial=trial)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     for step, inputs in enumerate(epoch_iterator):\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     data = self._next_data()\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     return self.collate_fn(data)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     batch = self.tokenizer.pad(\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "2021-02-10 18:16:55,850\tERROR trial_runner.py:607 -- Trial _objective_1420e_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 519, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 497, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/worker.py\", line 1379, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19429, ip=192.168.1.142)\n",
      "  File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 415, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/trainable.py\", line 183, in train\n",
      "    result = self.step()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 512, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19429, ip=192.168.1.142)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "ValueError: too many dimensions 'str'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19429, ip=192.168.1.142)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "    trainer.train(model_path=model_path, trial=trial)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "    for step, inputs in enumerate(epoch_iterator):\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "    batch = self.tokenizer.pad(\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m \n",
      "Result for _objective_1420e_00000:\n",
      "  {}\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 12.7/39.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/5 CPUs, 0/1 GPUs, 0.0/20.95 GiB heap, 0.0/7.23 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19\n",
      "Number of trials: 2/100 (1 ERROR, 1 PENDING)\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+---------+\n",
      "| Trial name             | status   | loc   |   learning_rate |   num_train_epochs |   per_device_train_batch_size |    seed |\n",
      "|------------------------+----------+-------+-----------------+--------------------+-------------------------------+---------|\n",
      "| _objective_1420e_00001 | PENDING  |       |     2.91064e-05 |                  2 |                             8 | 24.3477 |\n",
      "| _objective_1420e_00000 | ERROR    |       |     5.61152e-06 |                  4 |                             4 | 38.0779 |\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+---------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                                                                 |\n",
      "|------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| _objective_1420e_00000 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/error.txt |\n",
      "+------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: Waiting for W&B process to finish, PID 19458\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)\n",
      "wandb:                                                                                \n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: Find user logs for this run at: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/wandb/run-20210210_181654-7bvcikji/logs/debug.log\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: Find internal logs for this run at: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/wandb/run-20210210_181654-7bvcikji/logs/debug-internal.log\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m wandb: Synced hist-aware/notebooks/models: https://wandb.ai/leonardovida/huggingface/runs/7bvcikji\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19429)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 18:17:05,218\tWARNING util.py:141 -- The `start_trial` operation took 9.351 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/39.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/5 CPUs, 1/1 GPUs, 0.0/20.95 GiB heap, 0.0/7.23 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19\n",
      "Number of trials: 2/100 (1 ERROR, 1 RUNNING)\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+---------+\n",
      "| Trial name             | status   | loc   |   learning_rate |   num_train_epochs |   per_device_train_batch_size |    seed |\n",
      "|------------------------+----------+-------+-----------------+--------------------+-------------------------------+---------|\n",
      "| _objective_1420e_00001 | RUNNING  |       |     2.91064e-05 |                  2 |                             8 | 24.3477 |\n",
      "| _objective_1420e_00000 | ERROR    |       |     5.61152e-06 |                  4 |                             4 | 38.0779 |\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+---------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                                                                 |\n",
      "|------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| _objective_1420e_00000 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/error.txt |\n",
      "+------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m 2021-02-10 18:17:07.760606: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m 2021-02-10 18:17:07.760659: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: Currently logged in as: leonardovida (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: wandb version 0.10.18 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: Tracking run with wandb version 0.10.12\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: Syncing run hist-aware/notebooks/models\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb:  View project at https://wandb.ai/leonardovida/huggingface\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb:  View run at https://wandb.ai/leonardovida/huggingface/runs/15uqkcaz\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: Run data is saved locally in /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00001_1_learning_rate=2.9106e-05,num_train_epochs=2,per_device_train_batch_size=8,seed=24.348_2021-02-10_18-16-59/wandb/run-20210210_181715-15uqkcaz\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m 2021-02-10 18:17:16,668\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     tensor = as_tensor(value)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m ValueError: too many dimensions 'str'\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     trainer.train(model_path=model_path, trial=trial)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     for step, inputs in enumerate(epoch_iterator):\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     data = self._next_data()\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     return self.collate_fn(data)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     batch = self.tokenizer.pad(\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     tensor = as_tensor(value)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m ValueError: too many dimensions 'str'\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.pyenv/versions/3.8.6/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     trainer.train(model_path=model_path, trial=trial)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     for step, inputs in enumerate(epoch_iterator):\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     data = self._next_data()\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     return self.collate_fn(data)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     batch = self.tokenizer.pad(\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "2021-02-10 18:17:16,875\tERROR trial_runner.py:607 -- Trial _objective_1420e_00001: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 519, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 497, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/worker.py\", line 1379, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19491, ip=192.168.1.142)\n",
      "  File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 415, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/trainable.py\", line 183, in train\n",
      "    result = self.step()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 512, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19491, ip=192.168.1.142)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "ValueError: too many dimensions 'str'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19491, ip=192.168.1.142)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "    trainer.train(model_path=model_path, trial=trial)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "    for step, inputs in enumerate(epoch_iterator):\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "    batch = self.tokenizer.pad(\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m \n",
      "Result for _objective_1420e_00001:\n",
      "  {}\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 13.1/39.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/5 CPUs, 0/1 GPUs, 0.0/20.95 GiB heap, 0.0/7.23 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19\n",
      "Number of trials: 3/100 (2 ERROR, 1 PENDING)\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "| Trial name             | status   | loc   |   learning_rate |   num_train_epochs |   per_device_train_batch_size |     seed |\n",
      "|------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------|\n",
      "| _objective_1420e_00002 | PENDING  |       |     2.05134e-06 |                  4 |                            32 |  7.08379 |\n",
      "| _objective_1420e_00000 | ERROR    |       |     5.61152e-06 |                  4 |                             4 | 38.0779  |\n",
      "| _objective_1420e_00001 | ERROR    |       |     2.91064e-05 |                  2 |                             8 | 24.3477  |\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "Number of errored trials: 2\n",
      "+------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                                                                 |\n",
      "|------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| _objective_1420e_00000 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/error.txt |\n",
      "| _objective_1420e_00001 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00001_1_learning_rate=2.9106e-05,num_train_epochs=2,per_device_train_batch_size=8,seed=24.348_2021-02-10_18-16-59/error.txt |\n",
      "+------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: Waiting for W&B process to finish, PID 19516\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)\n",
      "wandb:                                                                                \n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: Find user logs for this run at: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00001_1_learning_rate=2.9106e-05,num_train_epochs=2,per_device_train_batch_size=8,seed=24.348_2021-02-10_18-16-59/wandb/run-20210210_181715-15uqkcaz/logs/debug.log\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: Find internal logs for this run at: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00001_1_learning_rate=2.9106e-05,num_train_epochs=2,per_device_train_batch_size=8,seed=24.348_2021-02-10_18-16-59/wandb/run-20210210_181715-15uqkcaz/logs/debug-internal.log\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m wandb: Synced hist-aware/notebooks/models: https://wandb.ai/leonardovida/huggingface/runs/15uqkcaz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19491)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 18:17:26,025\tWARNING util.py:141 -- The `start_trial` operation took 9.132 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/39.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/5 CPUs, 1/1 GPUs, 0.0/20.95 GiB heap, 0.0/7.23 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19\n",
      "Number of trials: 3/100 (2 ERROR, 1 RUNNING)\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "| Trial name             | status   | loc   |   learning_rate |   num_train_epochs |   per_device_train_batch_size |     seed |\n",
      "|------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------|\n",
      "| _objective_1420e_00002 | RUNNING  |       |     2.05134e-06 |                  4 |                            32 |  7.08379 |\n",
      "| _objective_1420e_00000 | ERROR    |       |     5.61152e-06 |                  4 |                             4 | 38.0779  |\n",
      "| _objective_1420e_00001 | ERROR    |       |     2.91064e-05 |                  2 |                             8 | 24.3477  |\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "Number of errored trials: 2\n",
      "+------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                                                                 |\n",
      "|------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| _objective_1420e_00000 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/error.txt |\n",
      "| _objective_1420e_00001 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00001_1_learning_rate=2.9106e-05,num_train_epochs=2,per_device_train_batch_size=8,seed=24.348_2021-02-10_18-16-59/error.txt |\n",
      "+------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m 2021-02-10 18:17:28.588348: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m 2021-02-10 18:17:28.588397: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: Currently logged in as: leonardovida (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: wandb version 0.10.18 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: Tracking run with wandb version 0.10.12\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: Syncing run hist-aware/notebooks/models\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb:  View project at https://wandb.ai/leonardovida/huggingface\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb:  View run at https://wandb.ai/leonardovida/huggingface/runs/21wlibu2\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: Run data is saved locally in /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00002_2_learning_rate=2.0513e-06,num_train_epochs=4,per_device_train_batch_size=32,seed=7.0838_2021-02-10_18-17-19/wandb/run-20210210_181736-21wlibu2\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m 2021-02-10 18:17:37,543\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     tensor = as_tensor(value)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m ValueError: too many dimensions 'str'\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     trainer.train(model_path=model_path, trial=trial)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     for step, inputs in enumerate(epoch_iterator):\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     data = self._next_data()\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     return self.collate_fn(data)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     batch = self.tokenizer.pad(\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     tensor = as_tensor(value)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m ValueError: too many dimensions 'str'\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.pyenv/versions/3.8.6/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     trainer.train(model_path=model_path, trial=trial)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     for step, inputs in enumerate(epoch_iterator):\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     data = self._next_data()\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     return self.collate_fn(data)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     batch = self.tokenizer.pad(\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "2021-02-10 18:17:37,668\tERROR trial_runner.py:607 -- Trial _objective_1420e_00002: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 519, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 497, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/worker.py\", line 1379, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19546, ip=192.168.1.142)\n",
      "  File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 415, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/trainable.py\", line 183, in train\n",
      "    result = self.step()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 512, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19546, ip=192.168.1.142)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "ValueError: too many dimensions 'str'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19546, ip=192.168.1.142)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "    trainer.train(model_path=model_path, trial=trial)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "    for step, inputs in enumerate(epoch_iterator):\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "    batch = self.tokenizer.pad(\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m \n",
      "Result for _objective_1420e_00002:\n",
      "  {}\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 12.7/39.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/5 CPUs, 0/1 GPUs, 0.0/20.95 GiB heap, 0.0/7.23 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19\n",
      "Number of trials: 4/100 (3 ERROR, 1 PENDING)\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "| Trial name             | status   | loc   |   learning_rate |   num_train_epochs |   per_device_train_batch_size |     seed |\n",
      "|------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------|\n",
      "| _objective_1420e_00003 | PENDING  |       |     1.30667e-06 |                  5 |                             4 | 34.7809  |\n",
      "| _objective_1420e_00000 | ERROR    |       |     5.61152e-06 |                  4 |                             4 | 38.0779  |\n",
      "| _objective_1420e_00001 | ERROR    |       |     2.91064e-05 |                  2 |                             8 | 24.3477  |\n",
      "| _objective_1420e_00002 | ERROR    |       |     2.05134e-06 |                  4 |                            32 |  7.08379 |\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "Number of errored trials: 3\n",
      "+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                                                                  |\n",
      "|------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| _objective_1420e_00000 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/error.txt  |\n",
      "| _objective_1420e_00001 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00001_1_learning_rate=2.9106e-05,num_train_epochs=2,per_device_train_batch_size=8,seed=24.348_2021-02-10_18-16-59/error.txt  |\n",
      "| _objective_1420e_00002 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00002_2_learning_rate=2.0513e-06,num_train_epochs=4,per_device_train_batch_size=32,seed=7.0838_2021-02-10_18-17-19/error.txt |\n",
      "+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: Waiting for W&B process to finish, PID 19571\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)\n",
      "wandb:                                                                                \n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: Find user logs for this run at: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00002_2_learning_rate=2.0513e-06,num_train_epochs=4,per_device_train_batch_size=32,seed=7.0838_2021-02-10_18-17-19/wandb/run-20210210_181736-21wlibu2/logs/debug.log\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: Find internal logs for this run at: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00002_2_learning_rate=2.0513e-06,num_train_epochs=4,per_device_train_batch_size=32,seed=7.0838_2021-02-10_18-17-19/wandb/run-20210210_181736-21wlibu2/logs/debug-internal.log\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m wandb: Synced hist-aware/notebooks/models: https://wandb.ai/leonardovida/huggingface/runs/21wlibu2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19546)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 18:17:46,973\tWARNING util.py:141 -- The `start_trial` operation took 9.284 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/39.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/5 CPUs, 1/1 GPUs, 0.0/20.95 GiB heap, 0.0/7.23 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19\n",
      "Number of trials: 4/100 (3 ERROR, 1 RUNNING)\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "| Trial name             | status   | loc   |   learning_rate |   num_train_epochs |   per_device_train_batch_size |     seed |\n",
      "|------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------|\n",
      "| _objective_1420e_00003 | RUNNING  |       |     1.30667e-06 |                  5 |                             4 | 34.7809  |\n",
      "| _objective_1420e_00000 | ERROR    |       |     5.61152e-06 |                  4 |                             4 | 38.0779  |\n",
      "| _objective_1420e_00001 | ERROR    |       |     2.91064e-05 |                  2 |                             8 | 24.3477  |\n",
      "| _objective_1420e_00002 | ERROR    |       |     2.05134e-06 |                  4 |                            32 |  7.08379 |\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "Number of errored trials: 3\n",
      "+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                                                                  |\n",
      "|------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| _objective_1420e_00000 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/error.txt  |\n",
      "| _objective_1420e_00001 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00001_1_learning_rate=2.9106e-05,num_train_epochs=2,per_device_train_batch_size=8,seed=24.348_2021-02-10_18-16-59/error.txt  |\n",
      "| _objective_1420e_00002 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00002_2_learning_rate=2.0513e-06,num_train_epochs=4,per_device_train_batch_size=32,seed=7.0838_2021-02-10_18-17-19/error.txt |\n",
      "+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m 2021-02-10 18:17:49.540350: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m 2021-02-10 18:17:49.540399: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: Currently logged in as: leonardovida (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: wandb version 0.10.18 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: Tracking run with wandb version 0.10.12\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: Syncing run hist-aware/notebooks/models\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb:  View project at https://wandb.ai/leonardovida/huggingface\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb:  View run at https://wandb.ai/leonardovida/huggingface/runs/157dh8ll\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: Run data is saved locally in /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00003_3_learning_rate=1.3067e-06,num_train_epochs=5,per_device_train_batch_size=4,seed=34.781_2021-02-10_18-17-40/wandb/run-20210210_181757-157dh8ll\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m 2021-02-10 18:17:58,490\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     tensor = as_tensor(value)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m ValueError: too many dimensions 'str'\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     trainer.train(model_path=model_path, trial=trial)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     for step, inputs in enumerate(epoch_iterator):\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     data = self._next_data()\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     return self.collate_fn(data)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     batch = self.tokenizer.pad(\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     tensor = as_tensor(value)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m ValueError: too many dimensions 'str'\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m \n",
      "2021-02-10 18:17:58,510\tERROR trial_runner.py:607 -- Trial _objective_1420e_00003: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 519, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 497, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/worker.py\", line 1379, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19623, ip=192.168.1.142)\n",
      "  File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 415, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/trainable.py\", line 183, in train\n",
      "    result = self.step()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 512, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19623, ip=192.168.1.142)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "ValueError: too many dimensions 'str'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19623, ip=192.168.1.142)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "    trainer.train(model_path=model_path, trial=trial)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "    for step, inputs in enumerate(epoch_iterator):\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "    batch = self.tokenizer.pad(\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.pyenv/versions/3.8.6/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     trainer.train(model_path=model_path, trial=trial)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     for step, inputs in enumerate(epoch_iterator):\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     data = self._next_data()\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     return self.collate_fn(data)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     batch = self.tokenizer.pad(\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m \n",
      "Result for _objective_1420e_00003:\n",
      "  {}\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 12.7/39.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/5 CPUs, 0/1 GPUs, 0.0/20.95 GiB heap, 0.0/7.23 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19\n",
      "Number of trials: 5/100 (4 ERROR, 1 PENDING)\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "| Trial name             | status   | loc   |   learning_rate |   num_train_epochs |   per_device_train_batch_size |     seed |\n",
      "|------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------|\n",
      "| _objective_1420e_00004 | PENDING  |       |     1.59305e-05 |                  3 |                             4 | 28.6148  |\n",
      "| _objective_1420e_00000 | ERROR    |       |     5.61152e-06 |                  4 |                             4 | 38.0779  |\n",
      "| _objective_1420e_00001 | ERROR    |       |     2.91064e-05 |                  2 |                             8 | 24.3477  |\n",
      "| _objective_1420e_00002 | ERROR    |       |     2.05134e-06 |                  4 |                            32 |  7.08379 |\n",
      "| _objective_1420e_00003 | ERROR    |       |     1.30667e-06 |                  5 |                             4 | 34.7809  |\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "Number of errored trials: 4\n",
      "+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                                                                  |\n",
      "|------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| _objective_1420e_00000 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/error.txt  |\n",
      "| _objective_1420e_00001 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00001_1_learning_rate=2.9106e-05,num_train_epochs=2,per_device_train_batch_size=8,seed=24.348_2021-02-10_18-16-59/error.txt  |\n",
      "| _objective_1420e_00002 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00002_2_learning_rate=2.0513e-06,num_train_epochs=4,per_device_train_batch_size=32,seed=7.0838_2021-02-10_18-17-19/error.txt |\n",
      "| _objective_1420e_00003 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00003_3_learning_rate=1.3067e-06,num_train_epochs=5,per_device_train_batch_size=4,seed=34.781_2021-02-10_18-17-40/error.txt  |\n",
      "+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: Waiting for W&B process to finish, PID 19966\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)\n",
      "wandb:                                                                                \n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: Find user logs for this run at: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00003_3_learning_rate=1.3067e-06,num_train_epochs=5,per_device_train_batch_size=4,seed=34.781_2021-02-10_18-17-40/wandb/run-20210210_181757-157dh8ll/logs/debug.log\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: Find internal logs for this run at: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00003_3_learning_rate=1.3067e-06,num_train_epochs=5,per_device_train_batch_size=4,seed=34.781_2021-02-10_18-17-40/wandb/run-20210210_181757-157dh8ll/logs/debug-internal.log\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m wandb: Synced hist-aware/notebooks/models: https://wandb.ai/leonardovida/huggingface/runs/157dh8ll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19623)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-10 18:18:07,864\tWARNING util.py:141 -- The `start_trial` operation took 9.345 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/39.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/5 CPUs, 1/1 GPUs, 0.0/20.95 GiB heap, 0.0/7.23 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19\n",
      "Number of trials: 5/100 (4 ERROR, 1 RUNNING)\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "| Trial name             | status   | loc   |   learning_rate |   num_train_epochs |   per_device_train_batch_size |     seed |\n",
      "|------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------|\n",
      "| _objective_1420e_00004 | RUNNING  |       |     1.59305e-05 |                  3 |                             4 | 28.6148  |\n",
      "| _objective_1420e_00000 | ERROR    |       |     5.61152e-06 |                  4 |                             4 | 38.0779  |\n",
      "| _objective_1420e_00001 | ERROR    |       |     2.91064e-05 |                  2 |                             8 | 24.3477  |\n",
      "| _objective_1420e_00002 | ERROR    |       |     2.05134e-06 |                  4 |                            32 |  7.08379 |\n",
      "| _objective_1420e_00003 | ERROR    |       |     1.30667e-06 |                  5 |                             4 | 34.7809  |\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "Number of errored trials: 4\n",
      "+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                                                                  |\n",
      "|------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| _objective_1420e_00000 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/error.txt  |\n",
      "| _objective_1420e_00001 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00001_1_learning_rate=2.9106e-05,num_train_epochs=2,per_device_train_batch_size=8,seed=24.348_2021-02-10_18-16-59/error.txt  |\n",
      "| _objective_1420e_00002 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00002_2_learning_rate=2.0513e-06,num_train_epochs=4,per_device_train_batch_size=32,seed=7.0838_2021-02-10_18-17-19/error.txt |\n",
      "| _objective_1420e_00003 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00003_3_learning_rate=1.3067e-06,num_train_epochs=5,per_device_train_batch_size=4,seed=34.781_2021-02-10_18-17-40/error.txt  |\n",
      "+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m 2021-02-10 18:18:10.396351: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m 2021-02-10 18:18:10.396404: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb: Currently logged in as: leonardovida (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb: wandb version 0.10.18 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb: Tracking run with wandb version 0.10.12\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb: Syncing run hist-aware/notebooks/models\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb:  View project at https://wandb.ai/leonardovida/huggingface\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb:  View run at https://wandb.ai/leonardovida/huggingface/runs/2n3614iy\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb: Run data is saved locally in /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00004_4_learning_rate=1.5931e-05,num_train_epochs=3,per_device_train_batch_size=4,seed=28.615_2021-02-10_18-18-01/wandb/run-20210210_181818-2n3614iy\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m 2021-02-10 18:18:19,315\tERROR function_runner.py:254 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     tensor = as_tensor(value)\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m ValueError: too many dimensions 'str'\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     trainer.train(model_path=model_path, trial=trial)\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     for step, inputs in enumerate(epoch_iterator):\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     data = self._next_data()\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     return self.collate_fn(data)\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     batch = self.tokenizer.pad(\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     tensor = as_tensor(value)\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m ValueError: too many dimensions 'str'\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.pyenv/versions/3.8.6/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 267, in run\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     trainer.train(model_path=model_path, trial=trial)\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     for step, inputs in enumerate(epoch_iterator):\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     data = self._next_data()\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     return self.collate_fn(data)\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     batch = self.tokenizer.pad(\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m   File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "2021-02-10 18:18:19,481\tERROR trial_runner.py:607 -- Trial _objective_1420e_00004: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 519, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 497, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/worker.py\", line 1379, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19998, ip=192.168.1.142)\n",
      "  File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 415, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/trainable.py\", line 183, in train\n",
      "    result = self.step()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 366, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 512, in _report_thread_runner_error\n",
      "    raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19998, ip=192.168.1.142)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 594, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "ValueError: too many dimensions 'str'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=19998, ip=192.168.1.142)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 248, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 315, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 575, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/integrations.py\", line 126, in _objective\n",
      "    trainer.train(model_path=model_path, trial=trial)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/trainer.py\", line 738, in train\n",
      "    for step, inputs in enumerate(epoch_iterator):\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 403, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 101, in __call__\n",
      "    batch = self.tokenizer.pad(\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2514, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 216, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "  File \"/home/leonardovida/.cache/pypoetry/virtualenvs/histaware-JJpORNNs-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 610, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m \n",
      "Result for _objective_1420e_00004:\n",
      "  {}\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 12.7/39.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/5 CPUs, 0/1 GPUs, 0.0/20.95 GiB heap, 0.0/7.23 GiB objects (0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19\n",
      "Number of trials: 6/100 (5 ERROR, 1 PENDING)\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "| Trial name             | status   | loc   |   learning_rate |   num_train_epochs |   per_device_train_batch_size |     seed |\n",
      "|------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------|\n",
      "| _objective_1420e_00005 | PENDING  |       |     1.09943e-06 |                  2 |                            16 | 38.8265  |\n",
      "| _objective_1420e_00000 | ERROR    |       |     5.61152e-06 |                  4 |                             4 | 38.0779  |\n",
      "| _objective_1420e_00001 | ERROR    |       |     2.91064e-05 |                  2 |                             8 | 24.3477  |\n",
      "| _objective_1420e_00002 | ERROR    |       |     2.05134e-06 |                  4 |                            32 |  7.08379 |\n",
      "| _objective_1420e_00003 | ERROR    |       |     1.30667e-06 |                  5 |                             4 | 34.7809  |\n",
      "| _objective_1420e_00004 | ERROR    |       |     1.59305e-05 |                  3 |                             4 | 28.6148  |\n",
      "+------------------------+----------+-------+-----------------+--------------------+-------------------------------+----------+\n",
      "Number of errored trials: 5\n",
      "+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                                                                                  |\n",
      "|------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| _objective_1420e_00000 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00000_0_learning_rate=5.6115e-06,num_train_epochs=4,per_device_train_batch_size=4,seed=38.078_2021-02-10_18-16-28/error.txt  |\n",
      "| _objective_1420e_00001 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00001_1_learning_rate=2.9106e-05,num_train_epochs=2,per_device_train_batch_size=8,seed=24.348_2021-02-10_18-16-59/error.txt  |\n",
      "| _objective_1420e_00002 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00002_2_learning_rate=2.0513e-06,num_train_epochs=4,per_device_train_batch_size=32,seed=7.0838_2021-02-10_18-17-19/error.txt |\n",
      "| _objective_1420e_00003 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00003_3_learning_rate=1.3067e-06,num_train_epochs=5,per_device_train_batch_size=4,seed=34.781_2021-02-10_18-17-40/error.txt  |\n",
      "| _objective_1420e_00004 |            1 | /home/leonardovida/ray_results/_objective_2021-02-10_18-16-19/_objective_1420e_00004_4_learning_rate=1.5931e-05,num_train_epochs=3,per_device_train_batch_size=4,seed=28.615_2021-02-10_18-18-01/error.txt  |\n",
      "+------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb: Waiting for W&B process to finish, PID 20023\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=19998)\u001b[0m wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)\n"
     ]
    }
   ],
   "source": [
    "def model_init():\n",
    "    return AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Evaluate during training and a bit more often\n",
    "# than the default to be able to prune bad trials early.\n",
    "# Disabling tqdm is a matter of preference.\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    model_init=model_init,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Defaut objective is the sum of all metrics\n",
    "# when metrics are provided, so we have to maximize it.\n",
    "trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    backend=\"ray\", \n",
    "    n_trials=100, # deafult 100\n",
    "    # n_jobs=2  # number of parallel jobs, if multiple GPUs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"ray\",\n",
    "    # Choose among many libraries:\n",
    "    # https://docs.ray.io/en/latest/tune/api_docs/suggestion.html\n",
    "    search_alg=HyperOptSearch(),\n",
    "    # Choose among schedulers:\n",
    "    # https://docs.ray.io/en/latest/tune/api_docs/schedulers.html\n",
    "    scheduler=AsyncHyperBand())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histaware-JJpORNNs-py3.8",
   "language": "python",
   "name": "histaware-jjpornns-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
