{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links to Project Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Trello board](https://trello.com/invite/b/BWnRAtKJ/3e7ce03017000289323e762d0ed2e304/histaware)\n",
    "- [Notion Wiki](https://www.notion.so/HistAware-529aba41f84946b19d493394ef6a2748)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text",
     "selection",
     "xml",
     "transformers"
    ]
   },
   "source": [
    "# Part I: Text selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first phase of the project, we approach the first problem of selecting texts similar texts. Intially the scope of the research is focused on texts that deal with `energy`. However, this scope might change and/or might be expanded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phases of Part I:**\n",
    "- **Validate the approach to the project**:\n",
    "    1. Decide whether to use title and paragraphs or only one of the two\n",
    "    2. Find the most efficient way to read all the xml files\n",
    "    3. Begin to label a golden set of texts that are within the scope of the research AND select the most important keywords that will be used to search for similar texts\n",
    "    4. Run the text similarity ML algorithm\n",
    "    5. Have the teaching assistant go throught the selection and identify mistakes\n",
    "- **To think about**: how to keep the relevant information about the text fragment (i.e. newspaper origin and date)?\n",
    "- **Decide the tools to use for text selection**. Current choices are:\n",
    "    - Use `sentence-transformers` from UKPLab (https://github.com/UKPLab/sentence-transformers)\n",
    "        - Generate embeddings on sentences (max 512 words)\n",
    "        - Find similar texts\n",
    "    - Use `faiss` from Facebook AI (https://github.com/facebookresearch/faiss)\n",
    "        - Less documentation but seemingly more scalable\n",
    "    - Use ASReview from Utrecht University ()\n",
    "        - A meeting with Jonathan or Raul is necessary to understand the feasibility of this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as et \n",
    "import collections\n",
    "import sys\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "np.set_printoptions(threshold=100)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "# Find path of data folder\n",
    "path = sys.path\n",
    "# To go back to main folder\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a catalogue of the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the file path and the file name into a dictionary. Then we transform the dictionary into a DataFrame so that we can later keep track of the index at which the parsing got stopped/interrupted (Dictionaries in Python do not have an order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_directory(path_dir,file_type):\n",
    "    \"\"\"Iterate over the `path_dir` and its children and\n",
    "    create a dictionary of names of files found, given their\n",
    "    file type `file_type`, and their path.\n",
    "    \"\"\"\n",
    "    rootdir = path[0]+path_dir\n",
    "    file_names = {}\n",
    "\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            #print os.path.join(subdir, file)\n",
    "            filepath = subdir + os.sep + file\n",
    "\n",
    "            if filepath.endswith(str(file_type)):\n",
    "                file_names[file] = filepath\n",
    "\n",
    "    return(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file_names = iterate_directory(\"/data/\",\".xml\")\n",
    "df_file_names = pd.DataFrame.from_dict(xml_file_names.items())\n",
    "df_file_names.rename({0: 'article_name', 1: 'article_path'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_XML(xml_file, title, index):\n",
    "    \"\"\"Parse the input XML file and store the result in a pandas \n",
    "    DataFrame with the given columns. \n",
    "    \n",
    "    Takes the filepath, file title and index integer of the df\n",
    "    \"\"\"\n",
    "    \n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    data = {}\n",
    "    \n",
    "    # Parse the date with regex\n",
    "    match = re.search(r'\\d{4}[/]\\d{2}[-]\\d{2}', xml_file)\n",
    "    date = datetime.strptime(match.group(), '%Y/%m-%d').date()\n",
    "    \n",
    "    for i, node in enumerate(xroot):\n",
    "        data[\"article_name\"] = str(title)\n",
    "        data[\"date\"] = str(date)\n",
    "        data[\"index\"] = index\n",
    "        if node.tag != \"p\":\n",
    "            data[node.tag] = node.text\n",
    "        else:\n",
    "            data[node.tag+\"_\"+str(i)] = node.text\n",
    "            \n",
    "#    out_df = pd.DataFrame.from_dict(data.items(), columns=data.keys())\n",
    "    s = pd.Series(data)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utils Addendum**\n",
    "\n",
    "To search for an `article_path` or `article_name` given the other, use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = df_file_names.loc[df_file_names['article_name'] == \"DDD_110637387_0004_articletext.xml\"]\n",
    "#a = df_file_names.iloc[0]\n",
    "c = df_file_names.iloc[500000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the files given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, this loop takes ~0.012s for each parsing. This is extremely slow and it's not due to the `parse_XML` function (which is efficient), but instead it's because of the `concat` between series. \n",
    "\n",
    "In this way 100.000 documents take around 20 minutes to be parsed.\n",
    "- If possible, substitute the concat statement with something more efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_files(files):\n",
    "    \"\"\"Iterate through files `files`, parse them and concatenate\n",
    "    the result in a pandas DataFrame with the\n",
    "    \"\"\"\n",
    "    main = None\n",
    "    previous_i = 0\n",
    "    current_i = 0\n",
    "    i = 0\n",
    "    n = 0\n",
    "    cnt = 0\n",
    "    list_series = []\n",
    "    \n",
    "    for index, row in files.iterrows():\n",
    "        list_series.append(parse_XML(row[\"article_path\"], row[\"article_name\"], index))\n",
    "        if (i == 10000):\n",
    "            current_i = current_i + i\n",
    "            file_path = path[0]+\"/data/processed/processed_data_\"+str(previous_i)+\"_\"+str(current_i)+\".ftr\"\n",
    "            main = pd.DataFrame(pd.concat(list_series, axis = 1).T)\n",
    "            main.to_feather(file_path)\n",
    "            list_series = []\n",
    "            main = None\n",
    "            previous_i = current_i\n",
    "            i = 0\n",
    "        if (i % 1000 == 0):\n",
    "            print(\"Files parsed: \"+str(1000*cnt))\n",
    "            print(\"Current file: \"+row[\"article_name\"]+\"\\n\")\n",
    "            cnt += 1\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files parsed: 0\n",
      "Current file: DDD_110637387_0004_articletext.xml\n",
      "\n",
      "Files parsed: 1000\n",
      "Current file: DDD_010865749_0044_articletext.xml\n",
      "\n",
      "Files parsed: 2000\n",
      "Current file: DDD_010537363_0050_articletext.xml\n",
      "\n",
      "Files parsed: 3000\n",
      "Current file: DDD_011210678_0092_articletext.xml\n",
      "\n",
      "Files parsed: 4000\n",
      "Current file: DDD_010612636_0060_articletext.xml\n",
      "\n",
      "Files parsed: 5000\n",
      "Current file: DDD_110584865_0073_articletext.xml\n",
      "\n",
      "Files parsed: 6000\n",
      "Current file: DDD_010537272_0086_articletext.xml\n",
      "\n",
      "Files parsed: 7000\n",
      "Current file: DDD_010862531_0063_articletext.xml\n",
      "\n",
      "Files parsed: 8000\n",
      "Current file: DDD_010873914_0016_articletext.xml\n",
      "\n",
      "Files parsed: 9000\n",
      "Current file: DDD_011202061_0045_articletext.xml\n",
      "\n",
      "Files parsed: 10000\n",
      "Current file: DDD_010850957_0008_articletext.xml\n",
      "\n",
      "Files parsed: 11000\n",
      "Current file: DDD_010950415_0125_articletext.xml\n",
      "\n",
      "Files parsed: 12000\n",
      "Current file: DDD_010612597_0072_articletext.xml\n",
      "\n",
      "Files parsed: 13000\n",
      "Current file: DDD_010733818_0003_articletext.xml\n",
      "\n",
      "Files parsed: 14000\n",
      "Current file: DDD_010950501_0028_articletext.xml\n",
      "\n",
      "Files parsed: 15000\n",
      "Current file: DDD_110585108_0045_articletext.xml\n",
      "\n",
      "Files parsed: 16000\n",
      "Current file: DDD_010612646_0014_articletext.xml\n",
      "\n",
      "Files parsed: 17000\n",
      "Current file: DDD_010537372_0007_articletext.xml\n",
      "\n",
      "Files parsed: 18000\n",
      "Current file: DDD_010417454_0077_articletext.xml\n",
      "\n",
      "Files parsed: 19000\n",
      "Current file: DDD_010862653_0079_articletext.xml\n",
      "\n",
      "Files parsed: 20000\n",
      "Current file: DDD_011155304_0067_articletext.xml\n",
      "\n",
      "Files parsed: 21000\n",
      "Current file: DDD_010955592_0002_articletext.xml\n",
      "\n",
      "Files parsed: 22000\n",
      "Current file: DDD_010897377_0016_articletext.xml\n",
      "\n",
      "Files parsed: 23000\n",
      "Current file: DDD_010537470_0095_articletext.xml\n",
      "\n",
      "Files parsed: 24000\n",
      "Current file: DDD_010896589_0073_articletext.xml\n",
      "\n",
      "Files parsed: 25000\n",
      "Current file: DDD_010554069_0031_articletext.xml\n",
      "\n",
      "Files parsed: 26000\n",
      "Current file: DDD_011155497_0001_articletext.xml\n",
      "\n",
      "Files parsed: 27000\n",
      "Current file: DDD_011202149_0122_articletext.xml\n",
      "\n",
      "Files parsed: 28000\n",
      "Current file: DDD_010417657_0054_articletext.xml\n",
      "\n",
      "Files parsed: 29000\n",
      "Current file: DDD_010865399_0048_articletext.xml\n",
      "\n",
      "Files parsed: 30000\n",
      "Current file: DDD_010417695_0001_articletext.xml\n",
      "\n",
      "Files parsed: 31000\n",
      "Current file: DDD_110585183_0020_articletext.xml\n",
      "\n",
      "Files parsed: 32000\n",
      "Current file: DDD_010950522_0077_articletext.xml\n",
      "\n",
      "Files parsed: 33000\n",
      "Current file: DDD_010865408_0082_articletext.xml\n",
      "\n",
      "Files parsed: 34000\n",
      "Current file: DDD_011218953_0022_articletext.xml\n",
      "\n",
      "Files parsed: 35000\n",
      "Current file: DDD_011202141_0058_articletext.xml\n",
      "\n",
      "Files parsed: 36000\n",
      "Current file: DDD_010733894_0124_articletext.xml\n",
      "\n",
      "Files parsed: 37000\n",
      "Current file: DDD_010417665_0039_articletext.xml\n",
      "\n",
      "Files parsed: 38000\n",
      "Current file: DDD_010950578_0082_articletext.xml\n",
      "\n",
      "Files parsed: 39000\n",
      "Current file: DDD_110585185_0100_articletext.xml\n",
      "\n",
      "Files parsed: 40000\n",
      "Current file: DDD_110585175_0162_articletext.xml\n",
      "\n",
      "Files parsed: 41000\n",
      "Current file: DDD_011218956_0039_articletext.xml\n",
      "\n",
      "Files parsed: 42000\n",
      "Current file: DDD_011210659_0062_articletext.xml\n",
      "\n",
      "Files parsed: 43000\n",
      "Current file: DDD_010950414_0081_articletext.xml\n",
      "\n",
      "Files parsed: 44000\n",
      "Current file: DDD_010612702_0009_articletext.xml\n",
      "\n",
      "Files parsed: 45000\n",
      "Current file: DDD_010865370_0030_articletext.xml\n",
      "\n",
      "Files parsed: 46000\n",
      "Current file: DDD_010862658_0007_articletext.xml\n",
      "\n",
      "Files parsed: 47000\n",
      "Current file: DDD_010950446_0047_articletext.xml\n",
      "\n",
      "Files parsed: 48000\n",
      "Current file: DDD_010475615_0043_articletext.xml\n",
      "\n",
      "Files parsed: 49000\n",
      "Current file: DDD_010417626_0027_articletext.xml\n",
      "\n",
      "Files parsed: 50000\n",
      "Current file: DDD_010887110_0007_articletext.xml\n",
      "\n",
      "Files parsed: 51000\n",
      "Current file: DDD_010612639_0117_articletext.xml\n",
      "\n",
      "Files parsed: 52000\n",
      "Current file: DDD_010733840_0013_articletext.xml\n",
      "\n",
      "Files parsed: 53000\n",
      "Current file: DDD_110584832_0033_articletext.xml\n",
      "\n",
      "Files parsed: 54000\n",
      "Current file: DDD_010475574_0044_articletext.xml\n",
      "\n",
      "Files parsed: 55000\n",
      "Current file: DDD_010873952_0016_articletext.xml\n",
      "\n",
      "Files parsed: 56000\n",
      "Current file: DDD_011210679_0190_articletext.xml\n",
      "\n",
      "Files parsed: 57000\n",
      "Current file: DDD_011210699_0029_articletext.xml\n",
      "\n",
      "Files parsed: 58000\n",
      "Current file: DDD_010862536_0059_articletext.xml\n",
      "\n",
      "Files parsed: 59000\n",
      "Current file: DDD_110585083_0018_articletext.xml\n",
      "\n",
      "Files parsed: 60000\n",
      "Current file: DDD_010862677_0022_articletext.xml\n",
      "\n",
      "Files parsed: 61000\n",
      "Current file: DDD_010475607_0028_articletext.xml\n",
      "\n",
      "Files parsed: 62000\n",
      "Current file: DDD_010862660_0038_articletext.xml\n",
      "\n",
      "Files parsed: 63000\n",
      "Current file: DDD_011202099_0056_articletext.xml\n",
      "\n",
      "Files parsed: 64000\n",
      "Current file: DDD_010612740_0028_articletext.xml\n",
      "\n",
      "Files parsed: 65000\n",
      "Current file: DDD_010612787_0069_articletext.xml\n",
      "\n",
      "Files parsed: 66000\n",
      "Current file: DDD_010950567_0020_articletext.xml\n",
      "\n",
      "Files parsed: 67000\n",
      "Current file: DDD_010950523_0070_articletext.xml\n",
      "\n",
      "Files parsed: 68000\n",
      "Current file: DDD_010865409_0130_articletext.xml\n",
      "\n",
      "Files parsed: 69000\n",
      "Current file: DDD_010891627_0107_articletext.xml\n",
      "\n",
      "Files parsed: 70000\n",
      "Current file: DDD_010852369_0058_articletext.xml\n",
      "\n",
      "Files parsed: 71000\n",
      "Current file: DDD_010476413_0072_articletext.xml\n",
      "\n",
      "Files parsed: 72000\n",
      "Current file: DDD_010891633_0005_articletext.xml\n",
      "\n",
      "Files parsed: 73000\n",
      "Current file: DDD_011202148_0125_articletext.xml\n",
      "\n",
      "Files parsed: 74000\n",
      "Current file: DDD_010891641_0123_articletext.xml\n",
      "\n",
      "Files parsed: 75000\n",
      "Current file: DDD_010612797_0037_articletext.xml\n",
      "\n",
      "Files parsed: 76000\n",
      "Current file: DDD_110585120_0108_articletext.xml\n",
      "\n",
      "Files parsed: 77000\n",
      "Current file: DDD_010476507_0049_articletext.xml\n",
      "\n",
      "Files parsed: 78000\n",
      "Current file: DDD_010987734_0040_articletext.xml\n",
      "\n",
      "Files parsed: 79000\n",
      "Current file: DDD_110585178_0078_articletext.xml\n",
      "\n",
      "Files parsed: 80000\n",
      "Current file: DDD_010403250_0073_articletext.xml\n",
      "\n",
      "Files parsed: 81000\n",
      "Current file: DDD_010417579_0140_articletext.xml\n",
      "\n",
      "Files parsed: 82000\n",
      "Current file: DDD_010886543_0069_articletext.xml\n",
      "\n",
      "Files parsed: 83000\n",
      "Current file: DDD_110584791_0148_articletext.xml\n",
      "\n",
      "Files parsed: 84000\n",
      "Current file: DDD_010886528_0037_articletext.xml\n",
      "\n",
      "Files parsed: 85000\n",
      "Current file: DDD_110584906_0005_articletext.xml\n",
      "\n",
      "Files parsed: 86000\n",
      "Current file: DDD_010475647_0005_articletext.xml\n",
      "\n",
      "Files parsed: 87000\n",
      "Current file: DDD_011155272_0015_articletext.xml\n",
      "\n",
      "Files parsed: 88000\n",
      "Current file: DDD_010417393_0085_articletext.xml\n",
      "\n",
      "Files parsed: 89000\n",
      "Current file: DDD_110637434_0073_articletext.xml\n",
      "\n",
      "Files parsed: 90000\n",
      "Current file: DDD_110584925_0124_articletext.xml\n",
      "\n",
      "Files parsed: 91000\n",
      "Current file: DDD_010480533_0024_articletext.xml\n",
      "\n",
      "Files parsed: 92000\n",
      "Current file: DDD_010403256_0004_articletext.xml\n",
      "\n",
      "Files parsed: 93000\n",
      "Current file: DDD_010950373_0075_articletext.xml\n",
      "\n",
      "Files parsed: 94000\n",
      "Current file: DDD_010886526_0098_articletext.xml\n",
      "\n",
      "Files parsed: 95000\n",
      "Current file: DDD_010403151_0007_articletext.xml\n",
      "\n",
      "Files parsed: 96000\n",
      "Current file: DDD_011155258_0098_articletext.xml\n",
      "\n",
      "Files parsed: 97000\n",
      "Current file: DDD_010554011_0052_articletext.xml\n",
      "\n",
      "Files parsed: 98000\n",
      "Current file: DDD_010612674_0002_articletext.xml\n",
      "\n",
      "Files parsed: 99000\n",
      "Current file: DDD_010850934_0007_articletext.xml\n",
      "\n",
      "Files parsed: 100000\n",
      "Current file: DDD_010417404_0104_articletext.xml\n",
      "\n",
      "Files parsed: 101000\n",
      "Current file: DDD_010673073_0131_articletext.xml\n",
      "\n",
      "Files parsed: 102000\n",
      "Current file: DDD_010950450_0039_articletext.xml\n",
      "\n",
      "Files parsed: 103000\n",
      "Current file: DDD_010403267_0061_articletext.xml\n",
      "\n",
      "Files parsed: 104000\n",
      "Current file: DDD_110637334_0002_articletext.xml\n",
      "\n",
      "Files parsed: 105000\n",
      "Current file: DDD_010480432_0028_articletext.xml\n",
      "\n",
      "Files parsed: 106000\n",
      "Current file: DDD_010475631_0015_articletext.xml\n",
      "\n",
      "Files parsed: 107000\n",
      "Current file: DDD_011155366_0187_articletext.xml\n",
      "\n",
      "Files parsed: 108000\n",
      "Current file: DDD_010891669_0064_articletext.xml\n",
      "\n",
      "Files parsed: 109000\n",
      "Current file: DDD_010733936_0143_articletext.xml\n",
      "\n",
      "Files parsed: 110000\n",
      "Current file: DDD_010612767_0118_articletext.xml\n",
      "\n",
      "Files parsed: 111000\n",
      "Current file: DDD_010733907_0066_articletext.xml\n",
      "\n",
      "Files parsed: 112000\n",
      "Current file: DDD_011202172_0128_articletext.xml\n",
      "\n",
      "Files parsed: 113000\n",
      "Current file: DDD_010950594_0001_articletext.xml\n",
      "\n",
      "Files parsed: 114000\n",
      "Current file: DDD_011202192_0202_articletext.xml\n",
      "\n",
      "Files parsed: 115000\n",
      "Current file: DDD_010554091_0056_articletext.xml\n",
      "\n",
      "Files parsed: 116000\n",
      "Current file: DDD_011155476_0153_articletext.xml\n",
      "\n",
      "Files parsed: 117000\n",
      "Current file: DDD_010897407_0082_articletext.xml\n",
      "\n",
      "Files parsed: 118000\n",
      "Current file: DDD_010417708_0067_articletext.xml\n",
      "\n",
      "Files parsed: 119000\n",
      "Current file: DDD_110585197_0162_articletext.xml\n",
      "\n",
      "Files parsed: 120000\n",
      "Current file: DDD_010554079_0027_articletext.xml\n",
      "\n",
      "Files parsed: 121000\n",
      "Current file: DDD_010862733_0024_articletext.xml\n",
      "\n",
      "Files parsed: 122000\n",
      "Current file: DDD_110637549_0003_articletext.xml\n",
      "\n",
      "Files parsed: 123000\n",
      "Current file: DDD_010891660_0079_articletext.xml\n",
      "\n",
      "Files parsed: 124000\n",
      "Current file: DDD_010865477_0049_articletext.xml\n",
      "\n",
      "Files parsed: 125000\n",
      "Current file: DDD_011202181_0048_articletext.xml\n",
      "\n",
      "Files parsed: 126000\n",
      "Current file: DDD_010862793_0064_articletext.xml\n",
      "\n",
      "Files parsed: 127000\n",
      "Current file: DDD_010987703_0064_articletext.xml\n",
      "\n",
      "Files parsed: 128000\n",
      "Current file: DDD_010897401_0064_articletext.xml\n",
      "\n",
      "Files parsed: 129000\n",
      "Current file: DDD_010987759_0052_articletext.xml\n",
      "\n",
      "Files parsed: 130000\n",
      "Current file: DDD_010891661_0051_articletext.xml\n",
      "\n",
      "Files parsed: 131000\n",
      "Current file: DDD_110585219_0087_articletext.xml\n",
      "\n",
      "Files parsed: 132000\n",
      "Current file: DDD_010852418_0047_articletext.xml\n",
      "\n",
      "Files parsed: 133000\n",
      "Current file: DDD_010403156_0067_articletext.xml\n",
      "\n",
      "Files parsed: 134000\n",
      "Current file: DDD_011199679_0114_articletext.xml\n",
      "\n",
      "Files parsed: 135000\n",
      "Current file: DDD_010417593_0067_articletext.xml\n",
      "\n",
      "Files parsed: 136000\n",
      "Current file: DDD_110584897_0039_articletext.xml\n",
      "\n",
      "Files parsed: 137000\n",
      "Current file: DDD_010612570_0123_articletext.xml\n",
      "\n",
      "Files parsed: 138000\n",
      "Current file: DDD_010850924_0016_articletext.xml\n",
      "\n",
      "Files parsed: 139000\n",
      "Current file: DDD_011185169_0013_articletext.xml\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ec255c601097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miterate_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_file_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-74ba724a6428>\u001b[0m in \u001b[0;36miterate_files\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mcurrent_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/data/processed/processed_data_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".ftr\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mlist_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         ]\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;31m# Forces alignment. No need to copy data since we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;31m# are putting it into an ndarray later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, index, **kwargs)\u001b[0m\n\u001b[1;32m   4397\u001b[0m     )\n\u001b[1;32m   4398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4401\u001b[0m     def drop(\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4451\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4452\u001b[0m         return self._reindex_axes(\n\u001b[0;32m-> 4453\u001b[0;31m             \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4454\u001b[0m         ).__finalize__(self, method=\"reindex\")\n\u001b[1;32m   4455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   4474\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4475\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4476\u001b[0;31m                 \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4477\u001b[0m             )\n\u001b[1;32m   4478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   4527\u001b[0m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4531\u001b[0m     def filter(\n",
      "\u001b[0;32m~/dev/HistAware/.venv/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    210\u001b[0m         ):\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# GH#33357 called with just the SingleBlockManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterate_files(df_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Text selection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest parsed files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we parse all the files present in the example `data-1950` folder, we produce 65 files containing the parsed original data into a format which is more easily readable by a machine. The total weight of the files is 65*10=650MB which is a 5x reduction from the original size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sbert.net/docs/\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "\n",
    "# These are the pure transformers from huggingface\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set searborn settings\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "# Set fixed random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Find GPU on device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_file_names = iterate_directory(\"/data/processed/\",\".ftr\")\n",
    "ftr_file_names = pd.DataFrame.from_dict(ftr_file_names.items())\n",
    "ftr_file_names.rename({0: 'ftr_name', 1: 'ftr_path'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ftr = []\n",
    "\n",
    "for index, row in ftr_file_names.iterrows():\n",
    "    if index > 1:\n",
    "        break\n",
    "    else:\n",
    "        ftr = pd.read_feather(row[\"ftr_path\"])\n",
    "        list_ftr.append(ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences = []\n",
    "\n",
    "for index, row in ftr.iterrows():\n",
    "    for i in range(1,ftr.shape[1]-4):\n",
    "        p = \"p_\"+str(i)\n",
    "        if row[p] and row[p] is not None:\n",
    "            list_sentences.append(row[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(list_sentences, key=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the multilingual model pre-trained on 10+ languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is the `distiluse-base-multilingual-cased` model. From (sbert)[https://www.sbert.net/docs/pretrained_models.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-27 15:25:05 - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased\n",
      "2020-08-27 15:25:05 - Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "2020-08-27 15:25:05 - Downloading sentence transformer model from https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/distiluse-base-multilingual-cased.zip and saving it at /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504M/504M [01:04<00:00, 7.81MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-27 15:26:21 - Load SentenceTransformer from folder: /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip\n",
      "2020-08-27 15:26:22 - loading configuration file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/config.json\n",
      "2020-08-27 15:26:22 - Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "2020-08-27 15:26:22 - loading weights file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/pytorch_model.bin\n",
      "2020-08-27 15:26:32 - All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "2020-08-27 15:26:32 - All the weights of DistilBertModel were initialized from the model checkpoint at /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "2020-08-27 15:26:32 - Model name '/Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT' not found in model shortcut name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). Assuming '/Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-08-27 15:26:32 - Didn't find file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/tokenizer.json. We won't load it.\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/vocab.txt\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/added_tokens.json\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/special_tokens_map.json\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/tokenizer_config.json\n",
      "2020-08-27 15:26:32 - loading file None\n",
      "2020-08-27 15:26:33 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('distiluse-base-multilingual-cased', device=device)\n",
    "# Load paragraphs\n",
    "sentences = \n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HistAware",
   "language": "python",
   "name": "histaware"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
