{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links to Project Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Trello board](https://trello.com/invite/b/BWnRAtKJ/3e7ce03017000289323e762d0ed2e304/histaware)\n",
    "- [Notion Wiki](https://www.notion.so/HistAware-529aba41f84946b19d493394ef6a2748)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text",
     "selection",
     "xml",
     "transformers"
    ]
   },
   "source": [
    "# Part I: Text selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first phase of the project, we approach the first problem of selecting texts similar texts. Intially the scope of the research is focused on texts that deal with `energy`. However, this scope might change and/or might be expanded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phases of Part I:**\n",
    "- **Validate the approach to the project**:\n",
    "    1. Decide whether to use title and paragraphs or only one of the two\n",
    "    2. Find the most efficient way to read all the xml files\n",
    "    3. Begin to label a golden set of texts that are within the scope of the research AND select the most important keywords that will be used to search for similar texts\n",
    "    4. Run the text similarity ML algorithm\n",
    "    5. Have the teaching assistant go throught the selection and identify mistakes\n",
    "- **To think about**: how to keep the relevant information about the text fragment (i.e. newspaper origin and date)?\n",
    "- **Decide the tools to use for text selection**. Current choices are:\n",
    "    - Use `sentence-transformers` from UKPLab (https://github.com/UKPLab/sentence-transformers)\n",
    "        - Generate embeddings on sentences (max 512 words)\n",
    "        - Find similar texts\n",
    "    - Use `faiss` from Facebook AI (https://github.com/facebookresearch/faiss)\n",
    "        - Less documentation but seemingly more scalable\n",
    "    - Use ASReview from Utrecht University ()\n",
    "        - A meeting with Jonathan or Raul is necessary to understand the feasibility of this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "np.set_printoptions(threshold=100)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et \n",
    "import collections\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Find path of data folder\n",
    "path = sys.path\n",
    "# To go back to main folder\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a catalogue of the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the file path and the file name into a dictionary. Then we transform the dictionary into a DataFrame so that we can later keep track of the index at which the parsing got stopped/interrupted (Dictionaries in Python do not have an order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the main directory and its children and create a dictionary of names\n",
    "rootdir = path[0]+\"/data/\"\n",
    "xml_file_names = {}\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print os.path.join(subdir, file)\n",
    "        filepath = subdir + os.sep + file\n",
    "        \n",
    "        if filepath.endswith(\".xml\"):\n",
    "            xml_file_names[file] = filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_names = pd.DataFrame.from_dict(xml_file_names.items())\n",
    "df_file_names.rename({0: 'article_name', 1: 'article_path'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_XML(xml_file, title, index):\n",
    "    \"\"\"Parse the input XML file and store the result in a pandas \n",
    "    DataFrame with the given columns. \n",
    "    \n",
    "    Takes the filepath, file title and index integer of the df\n",
    "    \"\"\"\n",
    "    \n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    data = {}\n",
    "    \n",
    "    # Parse the date with regex\n",
    "    match = re.search(r'\\d{4}[/]\\d{2}[-]\\d{2}', xml_file)\n",
    "    date = datetime.strptime(match.group(), '%Y/%m-%d').date()\n",
    "    \n",
    "    for i, node in enumerate(xroot):\n",
    "        data[\"article_name\"] = str(title)\n",
    "        data[\"date\"] = str(date)\n",
    "        data[\"index\"] = index\n",
    "        if node.tag != \"p\":\n",
    "            data[node.tag] = node.text\n",
    "        else:\n",
    "            data[node.tag+\"_\"+str(i)] = node.text\n",
    "            \n",
    "#    out_df = pd.DataFrame.from_dict(data.items(), columns=data.keys())\n",
    "    s = pd.Series(data)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for an `article_path` or `article_name` given the other, use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = df_file_names.loc[df_file_names['article_name'] == \"DDD_110637387_0004_articletext.xml\"]\n",
    "#a = df_file_names.iloc[0]\n",
    "c = df_file_names.iloc[500000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the files given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, this loop takes ~0.012s for each parsing. This is extremely slow and it's not due to the `parse_XML` function (which is efficient), but instead it's because of the `concat` between series. \n",
    "\n",
    "In this way 100.000 documents take around 20 minutes to be parsed.\n",
    "- If possible, substitute the concat statement with something more efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_files(files):\n",
    "    \"\"\"Iterate through files `files`, parse them and concatenate\n",
    "    the result in a pandas DataFrame with the\n",
    "    \"\"\"\n",
    "    main = None\n",
    "    previous_i = 0\n",
    "    current_i = 0\n",
    "    i = 0\n",
    "    n = 0\n",
    "    cnt = 0\n",
    "    list_series = []\n",
    "    \n",
    "    for index, row in files.iterrows():\n",
    "        series = parse_XML(row[\"article_path\"], row[\"article_name\"], index)\n",
    "        list_series.append(series)\n",
    "        if (i == 10000):\n",
    "            current_i = current_i + i\n",
    "            file_path = path[0]+\"/data/processed/processed_data_\"+str(previous_i)+\"_\"+str(current_i)+\".ftr\"\n",
    "            main = pd.DataFrame(pd.concat(list_series, axis = 1).T)\n",
    "            main.to_feather(file_path)\n",
    "            main = None\n",
    "            previous_i = current_i\n",
    "            i = 0\n",
    "        if (i % 1000 == 0):\n",
    "            print(\"Files parsed: \"+str(1000*cnt))\n",
    "            print(\"Current file: \"+row[\"article_name\"]+\"\\n\")\n",
    "            cnt += 1\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files parsed: 0\n",
      "Current file: DDD_110637387_0004_articletext.xml\n",
      "\n",
      "Files parsed: 1000\n",
      "Current file: DDD_010865749_0044_articletext.xml\n",
      "\n",
      "Files parsed: 2000\n",
      "Current file: DDD_010537363_0050_articletext.xml\n",
      "\n",
      "Files parsed: 3000\n",
      "Current file: DDD_011210678_0092_articletext.xml\n",
      "\n",
      "Files parsed: 4000\n",
      "Current file: DDD_010612636_0060_articletext.xml\n",
      "\n",
      "Files parsed: 5000\n",
      "Current file: DDD_110584865_0073_articletext.xml\n",
      "\n",
      "Files parsed: 6000\n",
      "Current file: DDD_010537272_0086_articletext.xml\n",
      "\n",
      "Files parsed: 7000\n",
      "Current file: DDD_010862531_0063_articletext.xml\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-347-ec255c601097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miterate_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_file_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-346-0d8959756619>\u001b[0m in \u001b[0;36miterate_files\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_XML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"article_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"article_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlist_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-300-45d539073426>\u001b[0m in \u001b[0;36mparse_XML\u001b[0;34m(xml_file, title, index)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mxtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0met\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mxroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \"\"\"\n\u001b[1;32m   1196\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;31m# It can be used to parse the whole source without feeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;31m# it with chunks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_whole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterate_files(df_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text selection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the multilingual model pre-trained on 10+ languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is the `distiluse-base-multilingual-cased` model. From (sbert)[https://www.sbert.net/docs/pretrained_models.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-27 15:25:05 - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased\n",
      "2020-08-27 15:25:05 - Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "2020-08-27 15:25:05 - Downloading sentence transformer model from https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/distiluse-base-multilingual-cased.zip and saving it at /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504M/504M [01:04<00:00, 7.81MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-27 15:26:21 - Load SentenceTransformer from folder: /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip\n",
      "2020-08-27 15:26:22 - loading configuration file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/config.json\n",
      "2020-08-27 15:26:22 - Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "2020-08-27 15:26:22 - loading weights file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/pytorch_model.bin\n",
      "2020-08-27 15:26:32 - All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "2020-08-27 15:26:32 - All the weights of DistilBertModel were initialized from the model checkpoint at /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "2020-08-27 15:26:32 - Model name '/Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT' not found in model shortcut name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). Assuming '/Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-08-27 15:26:32 - Didn't find file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/tokenizer.json. We won't load it.\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/vocab.txt\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/added_tokens.json\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/special_tokens_map.json\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/tokenizer_config.json\n",
      "2020-08-27 15:26:32 - loading file None\n",
      "2020-08-27 15:26:33 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('distiluse-base-multilingual-cased')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HistAware",
   "language": "python",
   "name": "histaware"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
