{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "text",
     "selection",
     "xml",
     "transformers"
    ]
   },
   "source": [
    "# Part I: Text selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first phase of the project, we approach the first problem of selecting texts similar texts. Intially the scope of the research is focused on texts that deal with `energy`. However, this scope might change and/or might be expanded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phases of Part I:**\n",
    "- **Validate the approach to the project**:\n",
    "    1. Decide whether to use title and paragraphs or only one of the two\n",
    "    2. Find the most efficient way to read all the xml files\n",
    "    3. Begin to label a golden set of texts that are within the scope of the research AND select the most important keywords that will be used to search for similar texts\n",
    "    4. Run the text similarity ML algorithm\n",
    "    5. Have the teaching assistant go throught the selection and identify mistakes\n",
    "- **To think about**: how to keep the relevant information about the text fragment (i.e. newspaper origin and date)?\n",
    "- **Decide the tools to use for text selection**. Current choices are:\n",
    "    - Use `sentence-transformers` from UKPLab (https://github.com/UKPLab/sentence-transformers)\n",
    "        - Generate embeddings on sentences (max 512 words)\n",
    "        - Find similar texts\n",
    "    - Use `faiss` from Facebook AI (https://github.com/facebookresearch/faiss)\n",
    "        - Less documentation but seemingly more scalable\n",
    "    - Use ASReview from Utrecht University ()\n",
    "        - A meeting with Jonathan or Raul is necessary to understand the feasibility of this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach with `sentence-transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "np.set_printoptions(threshold=100)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the multilingual model pre-trained on 10+ languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is the `distiluse-base-multilingual-cased` model. From (sbert)[https://www.sbert.net/docs/pretrained_models.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-27 15:25:05 - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased\n",
      "2020-08-27 15:25:05 - Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "2020-08-27 15:25:05 - Downloading sentence transformer model from https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/distiluse-base-multilingual-cased.zip and saving it at /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504M/504M [01:04<00:00, 7.81MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-27 15:26:21 - Load SentenceTransformer from folder: /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip\n",
      "2020-08-27 15:26:22 - loading configuration file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/config.json\n",
      "2020-08-27 15:26:22 - Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "2020-08-27 15:26:22 - loading weights file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/pytorch_model.bin\n",
      "2020-08-27 15:26:32 - All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "2020-08-27 15:26:32 - All the weights of DistilBertModel were initialized from the model checkpoint at /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "2020-08-27 15:26:32 - Model name '/Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT' not found in model shortcut name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). Assuming '/Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-08-27 15:26:32 - Didn't find file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/tokenizer.json. We won't load it.\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/vocab.txt\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/added_tokens.json\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/special_tokens_map.json\n",
      "2020-08-27 15:26:32 - loading file /Users/leonardovida/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_distiluse-base-multilingual-cased.zip/0_DistilBERT/tokenizer_config.json\n",
      "2020-08-27 15:26:32 - loading file None\n",
      "2020-08-27 15:26:33 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('distiluse-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find a way to create inventory of the files\n",
    "    - Create a dictionary(?) to keep track of the files location\n",
    "- Each Series or Dataframe needs to have a title that describe the file location\n",
    "- Compare speed between Series and DF?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et \n",
    "import collections\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalogue files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the main directory and its children and create a dictionary of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = path[0]+\"/data/\"\n",
    "xml_file_names = {}\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print os.path.join(subdir, file)\n",
    "        filepath = subdir + os.sep + file\n",
    "\n",
    "        if filepath.endswith(\".xml\"):\n",
    "            xml_file_names[file] = filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_XML(xml_file):\n",
    "    \"\"\"Parse the input XML file and store the result in a pandas \n",
    "    DataFrame with the given columns. \n",
    "    \"\"\"\n",
    "    \n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    data = {}\n",
    "    \n",
    "    for i, node in enumerate(xroot):\n",
    "        if node.tag != \"p\":\n",
    "            data[node.tag] = node.text\n",
    "        else:\n",
    "            data[node.tag+\"_\"+str(i)] = node.text\n",
    "    s = pd.Series(data)\n",
    "    out_df = pd.DataFrame.from_records(data, index=[0])\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p_1</th>\n",
       "      <td>Xew Vork — Volgens de „New Vork Times\" zouden ...</td>\n",
       "      <td>Londen — Mao Tse Toeng, de Chinese communistis...</td>\n",
       "      <td>Xew Vork — Volgens de „New Vork Times\" zouden ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_2</th>\n",
       "      <td>eens geworden zijn, dat van een Xoordamerikaan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eens geworden zijn, dat van een Xoordamerikaan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_3</th>\n",
       "      <td>Volgens Reston is er een ernstig meningsversch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Volgens Reston is er een ernstig meningsversch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_4</th>\n",
       "      <td>Het Staatsdepartement was: van gevoelen, dat d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Het Staatsdepartement was: van gevoelen, dat d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>U.S.A. — CHINA.</td>\n",
       "      <td>Chinese communistenleider onderhandelt in Mosk...</td>\n",
       "      <td>U.S.A. — CHINA.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       a  \\\n",
       "p_1    Xew Vork — Volgens de „New Vork Times\" zouden ...   \n",
       "p_2    eens geworden zijn, dat van een Xoordamerikaan...   \n",
       "p_3    Volgens Reston is er een ernstig meningsversch...   \n",
       "p_4    Het Staatsdepartement was: van gevoelen, dat d...   \n",
       "title                                    U.S.A. — CHINA.   \n",
       "\n",
       "                                                       b  \\\n",
       "p_1    Londen — Mao Tse Toeng, de Chinese communistis...   \n",
       "p_2                                                  NaN   \n",
       "p_3                                                  NaN   \n",
       "p_4                                                  NaN   \n",
       "title  Chinese communistenleider onderhandelt in Mosk...   \n",
       "\n",
       "                                                       c  \n",
       "p_1    Xew Vork — Volgens de „New Vork Times\" zouden ...  \n",
       "p_2    eens geworden zijn, dat van een Xoordamerikaan...  \n",
       "p_3    Volgens Reston is er een ernstig meningsversch...  \n",
       "p_4    Het Staatsdepartement was: van gevoelen, dat d...  \n",
       "title                                    U.S.A. — CHINA.  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = parse_XML(path[0]+\"/data/1950/01-02/DDD_010403143/DDD_010403143_0002_articletext.xml\")\n",
    "b = parse_XML(path[0]+\"/data/1950/01-02/DDD_010403143/DDD_010403143_0001_articletext.xml\")\n",
    "c = parse_XML(path[0]+\"/data/1950/01-02/DDD_010403143/DDD_010403143_0002_articletext.xml\")\n",
    "data = pd.DataFrame({'a': a,'b':b, 'c':c})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_1</th>\n",
       "      <th>p_2</th>\n",
       "      <th>p_3</th>\n",
       "      <th>p_4</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xew Vork — Volgens de „New Vork Times\" zouden ...</td>\n",
       "      <td>eens geworden zijn, dat van een Xoordamerikaan...</td>\n",
       "      <td>Volgens Reston is er een ernstig meningsversch...</td>\n",
       "      <td>Het Staatsdepartement was: van gevoelen, dat d...</td>\n",
       "      <td>U.S.A. — CHINA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Londen — Mao Tse Toeng, de Chinese communistis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chinese communistenleider onderhandelt in Mosk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xew Vork — Volgens de „New Vork Times\" zouden ...</td>\n",
       "      <td>eens geworden zijn, dat van een Xoordamerikaan...</td>\n",
       "      <td>Volgens Reston is er een ernstig meningsversch...</td>\n",
       "      <td>Het Staatsdepartement was: van gevoelen, dat d...</td>\n",
       "      <td>U.S.A. — CHINA.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 p_1  \\\n",
       "0  Xew Vork — Volgens de „New Vork Times\" zouden ...   \n",
       "1  Londen — Mao Tse Toeng, de Chinese communistis...   \n",
       "2  Xew Vork — Volgens de „New Vork Times\" zouden ...   \n",
       "\n",
       "                                                 p_2  \\\n",
       "0  eens geworden zijn, dat van een Xoordamerikaan...   \n",
       "1                                                NaN   \n",
       "2  eens geworden zijn, dat van een Xoordamerikaan...   \n",
       "\n",
       "                                                 p_3  \\\n",
       "0  Volgens Reston is er een ernstig meningsversch...   \n",
       "1                                                NaN   \n",
       "2  Volgens Reston is er een ernstig meningsversch...   \n",
       "\n",
       "                                                 p_4  \\\n",
       "0  Het Staatsdepartement was: van gevoelen, dat d...   \n",
       "1                                                NaN   \n",
       "2  Het Staatsdepartement was: van gevoelen, dat d...   \n",
       "\n",
       "                                               title  \n",
       "0                                    U.S.A. — CHINA.  \n",
       "1  Chinese communistenleider onderhandelt in Mosk...  \n",
       "2                                    U.S.A. — CHINA.  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = parse_XML(path[0]+\"/data/1950/01-02/DDD_010403143/DDD_010403143_0002_articletext.xml\")\n",
    "b = parse_XML(path[0]+\"/data/1950/01-02/DDD_010403143/DDD_010403143_0001_articletext.xml\")\n",
    "c = parse_XML(path[0]+\"/data/1950/01-02/DDD_010403143/DDD_010403143_0002_articletext.xml\")\n",
    "pd.concat([a, b, c], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HistAware",
   "language": "python",
   "name": "histaware"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
