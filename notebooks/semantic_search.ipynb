{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('histaware-RplM6c3o-py3.8')",
   "display_name": "Python 3.8.5 64-bit ('histaware-RplM6c3o-py3.8')",
   "metadata": {
    "interpreter": {
     "hash": "86315069d9bbcc5e7b6a2d7416b15171a30b5af37cfb10a9c473db24ce95b203"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2020-10-12 15:52:38,125 — parsers — DEBUG — Test message\n",
      "2020-10-12 15:52:38,125 — parsers — DEBUG — Test message\n",
      "2020-10-12 15:52:38,127 — iterators — DEBUG — Test message\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries needed\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/Users/leonardovida/dev/HistAware\")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from src import iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "source": [
    "# Sentence Transformer "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the model - Bertje\n",
    "tokenizer = BertTokenizer.from_pretrained(\"wietsedv/bert-base-dutch-cased\")\n",
    "model = BertModel.from_pretrained(\"wietsedv/bert-base-dutch-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the file\n",
    "csv = iterators.iterate_directory(\"../data/processed/selected_articles/\", \".csv\")\n",
    "df = pd.concat([pd.read_csv(c[\"article_path\"]) for c in csv],ignore_index=True)\n",
    "df.sort_values(by=[\"count\"], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df[0:100]\n",
    "df_remaining = df[101:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\n",
    "        \"../data/models/distiluse/distiluse-base-multilingual-cased\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(df_remaining[\"text\"])\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = list(df_selected[\"text\"])\n",
    "queries_embeddings = embedder.encode(queries, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for query_emb in queries_embeddings:\n",
    "    test = util.semantic_search(\n",
    "        query_embeddings=query_emb,\n",
    "        corpus_embeddings=corpus_embeddings,\n",
    "        query_chunk_size=100,\n",
    "        corpus_chunk_size=100000,\n",
    "        top_k=10,\n",
    "    )\n",
    "    results.append(test)\n",
    "    test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'corpus_id': 56, 'score': 1.0000002},\n",
       "  {'corpus_id': 1191, 'score': 0.46423584},\n",
       "  {'corpus_id': 1001, 'score': 0.46423584},\n",
       "  {'corpus_id': 169, 'score': 0.46423584},\n",
       "  {'corpus_id': 4795, 'score': 0.4443941},\n",
       "  {'corpus_id': 1085, 'score': 0.4443941},\n",
       "  {'corpus_id': 2554, 'score': 0.4293515},\n",
       "  {'corpus_id': 887, 'score': 0.42333776},\n",
       "  {'corpus_id': 13, 'score': 0.42276603},\n",
       "  {'corpus_id': 3721, 'score': 0.41876274}]]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ning tijdens de ontgassing normaal kan doorgaan. In de Belgische mijn „Le Grand Trait\" te Frameries in Henegouwen „oogstte\" men op deze wijze in 2 maanden tijds 378.000 m 3 methaangas, in de mijn „Saint Albert\" te Ressaix in een iets langere periode 428.650 m 3 methaan. In Henegouwen wordt het gas reeds naar buiten geleverd via de lichtgasfabrieken te Tertre. Methaangas levert 8000 tot 9000 caloriën warmte, hetgeen tweemaal zoveel is als gewoon cokesovengas. In vele andere mijnen, waaronder de Kempische, neemt men proeven. Er bestaan plannen in Belgisch Limburg een leidingermet aan te leggen voor de distributie van het gas aan de bevolking. Een probleem vormt echter de vrij onregelmatige toevoer, waarmee men ongetwijfeld te kampen zal krijgen. In de mijn Hirschbach in het Saargebied heeft men een andere methode gevolgd. In deze mijn ontsnapte zoveel gas dat met luchtverversing niet voldoende te bereiken was. Een gedeelte van de mijn werd daarom met dammen van de rest afgesloten. Door hel verrichten van boringen en door het afbouwen van een dieper gelegen laag zorgde men er voor, dat het gas naar de afgesloten ruimte ontsnapte. Hieruit werd het weggezogen om bovengronds te worden gebruikt voor de verhitting van een stoomketel. De besparing bedroeg 500 ton kolen per maand. In ruim 11 jaar won men in Hirschbach 5 millioen in 3 mijngas. Voordelen Technisch gesproken heeft men al veel bereikt. Er blijven echter nog problemen, die om een oplossing vragen. Het lijdt echter geen twijfel, dat men op het goede spoor is. Aan de mijngas-winning zijn vele voordelen verbonden. In het ene mijnbekken zal een bepaald voordeel zwaarder wegen dan in het andere, maar in het algemeen zijn het de volgende. Door het gas te winnen voordat het in pijlers en mijngangen terecht komt, wordt de veiligheid in het ondergronds bedrijf verhoogd. De kans op vorming van ontplofbare mengsels wordt veel kleiner. Hiermee wordt het belang van de mijnwerkers gediend. In een buitenlandse mijn daalde het gasgehalte van de uittrekkende luchtstroom op deze manier van 2 of 3 percent tot beneden 1 percent. Daarnaast is er een economisch voordeel, dat niet alleen voor de mijnwerkers, maar ook voor een veel grotere groep mensen van belang is. Het gas kan worden gebruikt voor de voorziening van industrieën en woningen. Met het mijngas, dat nu dagelijks uit onze mijnen wordt geblazen, zouden zeker tienduizenden huismoeders hun potje kunnen koken. Een andere mogelijkheid is dat het als stookgas wordt gebruikt in ketels, waardoor kolen worden bespaard. En dan zullen er zeker chemische fabrieken zijn die er nuttige producten van kunnen maken. Voor de mijnen zelf zijn er ook speciale voordelen verbonden aan de ontgassing. Het komt vaak voor dat de koolwinning wordt gehandicapt door het ontsnappen van mijngas. Het kan zijn dat van een bepaalde pijler gemakkelijk 1000 ton kolen per dag kunnen worden getrokken, terwijl men er toch niet meer dan 500 ton uithaalt, omdat anders het mijngasgehalte in de luchtstroom boven het voorgeschreven peil stijgt. Wordt de betreffende kolenlaag voor of tijdens de exploitatie ontgast, dan kan men zonder bezwaar de productie opvoeren. Dat is een kwestie van meer mensen. Dit voordeel noemt men: een grotere concentratie van de ontginning. Deze doet de kostprijs per ton dalen. Ten slotte is daar f N De moderne gestroomlijnde schacht van Staatsmijn Emma, waardoor per aur 500 ton kool van 546 meter dieptt naar boven wordt gebracht -\"* -•-•■ \\'•■ _■ _,*<£_&_ ■■■■■-■ \\'fTÜlfc\\'in •■:-\\' ■\\' ...*......->...'"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "df_selected.iloc[1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'HST AANBOREN van gas bU Coevorden heeft tot deze boring b\\\\j Gramsbergen geleid. Waar gas zit, zit dikwijls ook olie en olie is tenslotte de voornaamste prooi, waarop wordt geloerd. Ook gas kan echter van grote betekenis zijn. De succesvolle boringen bU Coevorden, en later bij de Wijk (bü Meppel) hebben al een stroom van geruchten door Drente, Overijssel en Groningen doen gaan. Men sprak al van een stad (Assen), die haar gasbedrijf per 1 Januari j.L op het aardgas van de „Nam\" zou overschakelen. Men vertelde van Groningen en andere plaatsen, die de uitbreidingsplannen voor hun gasfabrieken reeds ter zijde zouden hebben gelegd, omdat de productie van aardgas grootse perspectieven zou hebben geopend. Natuurlijk is dat alles ontzaggelijk overdreven. Vooralsnog hebben de onderzoekers van de .Nam\\' het plan om eventueel te vinden gas te gebruiken voor het gemakkelijker naar boven brengen van de olie. Gelijk bekend, zijn er elders ter wereld verschillende spui-'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "df_remaining.iloc[1191][\"text\"]"
   ]
  },
  {
   "source": [
    "# For sentiment analysis system"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = \"max_length\"\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "MODEL = \"wietsedv/bert-base-dutch-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model - Bertje\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
    "model = BertModel.from_pretrained(MODEL)"
   ]
  },
  {
   "source": [
    "Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.type\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "FULL Dataset: (2565, 2)\nTRAIN Dataset: (2052, 2)\nTEST Dataset: (513, 2)\n"
    }
   ],
   "source": [
    "\n",
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "            'shuffle': True,\n",
    "            'num_workers': 0\n",
    "            }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "            'shuffle': True,\n",
    "            'num_workers': 0\n",
    "            }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name &#39;batch_sentences&#39; is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m&lt;ipython-input-31-21da5e5961b4&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[0;32m----&gt; 1\u001b[0;31m encoded_input = tokenizer(batch_sentences,\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     return_tensors=&quot;pt&quot;)\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m&quot;input_ids&quot;\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name &#39;batch_sentences&#39; is not defined"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch_sentences,\n",
    "    # Pad to the longest allowed length by the model \n",
    "    padding=\"max_length\",\n",
    "    # Truncate to maximum length\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\")\n",
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "source": [
    "# Correct OCR words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nl_core_news_lg\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import enchant\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from enchant.checker import SpellChecker\n",
    "from difflib import SequenceMatcher\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_original = df[\"text\"][1]\n",
    "text = df[\"text\"][1]"
   ]
  },
  {
   "source": [
    "### Split the data in smaller chunks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(txt):\n",
    "  len_tot = []\n",
    "  len_partial = []\n",
    "  if len(txt.split())//150 >0:\n",
    "    n = len(txt.split())//150\n",
    "  else: \n",
    "    n = 1\n",
    "  for w in range(n):\n",
    "    if w == 0:\n",
    "      len_partial = txt.split()[:200]\n",
    "      len_tot.append(\" \".join(len_partial))\n",
    "    else:\n",
    "      len_partial = txt.split()[w*150:w*150 + 200]\n",
    "      len_tot.append(\" \".join(len_partial))\n",
    "  return len_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_selected\n",
    "df[\"text_split\"] = df[\"text\"].apply(get_split)"
   ]
  },
  {
   "source": [
    "Select the divided text and retrieve also article_ids and article_name"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "divided_texts = []\n",
    "idx_texts = []\n",
    "name_texts = []\n",
    "dfids_texts = []\n",
    "for idx, row in df.iterrows():\n",
    "  for text in row['text_split']:\n",
    "    divided_texts.append(text)\n",
    "    idx_texts.append(idx)\n",
    "    name_texts.append(row[\"article_name\"])\n",
    "    dfids_texts.append(row[\"Unnamed: 0_x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts = pd.DataFrame({\"text\":divided_texts, \"article_id\":dfids_texts, \"article_name\":name_texts})"
   ]
  },
  {
   "source": [
    "Remove parts of speech that are not useful"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Find all incorrect words in the df"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f17c9a091bba46b1b7b06be985f35482"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "incorrectwords = []\n",
    "masked_texts = []\n",
    "suggestedwords = []\n",
    "d = SpellChecker(\"nl_NL\")\n",
    "\n",
    "rep = { '\\n': ' ', '\\\\': ' ', '\\\"': '\"', '-': ' ', '\"': ' \" ', \n",
    "        '\"': ' \" ', '\"': ' \" ', ',':' , ', '.':' . ', '!':' ! ', \n",
    "        '?':' ? ', \"n't\": \" not\" , \"'ll\": \" will\", '*':' * ', \n",
    "        '(': ' ( ', ')': ' ) ', \"s'\": \"s '\"}\n",
    "rep = dict((re.escape(k), v) for k, v in rep.items()) \n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "ignorewords = [\"!\", \",\", \".\", \"\\\"\", \"?\", '(', ')', '*', \"'\"]\n",
    "\n",
    "for ids, row in tqdm(df_texts.iterrows(), total=df_texts.shape[0]):\n",
    "    #using enchant.checker.SpellChecker, identify incorrect words\n",
    "    text = row[\"text\"]\n",
    "    text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n",
    "    words = text.split()\n",
    "    incorrectwords = [w for w in words if not d.check(w) and w not in ignorewords]\n",
    "    suggestedwords.append([d.suggest(w) for w in incorrectwords])\n",
    "    for w in incorrectwords:\n",
    "        text = text.replace(w, '[MASK]')\n",
    "    masked_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts[\"masked_texts\"] = masked_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  article_id  \\\n",
       "0  Aoiang er mijnen bestaan is het mijngas de gro...       36288   \n",
       "1  Het spreekt dus vanzelf dat in alle kolenmijne...       36288   \n",
       "2  lucht en methaan. dat tussen 6% en 14% methaan...       36288   \n",
       "3  10.000 m 3 lucht uit zijn ondergrondse werken....       36288   \n",
       "4  ruim 42 atmosfeer. | Het gas blijft waar het i...       36288   \n",
       "\n",
       "                         article_name  \\\n",
       "0  DDD_010417712_0100_articletext.xml   \n",
       "1  DDD_010417712_0100_articletext.xml   \n",
       "2  DDD_010417712_0100_articletext.xml   \n",
       "3  DDD_010417712_0100_articletext.xml   \n",
       "4  DDD_010417712_0100_articletext.xml   \n",
       "\n",
       "                                        masked_texts  \n",
       "0  [MASK] er mijnen bestaan is het mijngas de gro...  \n",
       "1  Het spreekt dus vanzelf dat in alle kolenmijne...  \n",
       "2  lucht en methaan .  dat tussen [MASK] en [MASK...  \n",
       "3  [MASK] . [MASK] m 3 lucht uit zijn ondergronds...  \n",
       "4  ruim [MASK] atmosfeer .  | Het gas blijft waar...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>article_id</th>\n      <th>article_name</th>\n      <th>masked_texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Aoiang er mijnen bestaan is het mijngas de gro...</td>\n      <td>36288</td>\n      <td>DDD_010417712_0100_articletext.xml</td>\n      <td>[MASK] er mijnen bestaan is het mijngas de gro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Het spreekt dus vanzelf dat in alle kolenmijne...</td>\n      <td>36288</td>\n      <td>DDD_010417712_0100_articletext.xml</td>\n      <td>Het spreekt dus vanzelf dat in alle kolenmijne...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>lucht en methaan. dat tussen 6% en 14% methaan...</td>\n      <td>36288</td>\n      <td>DDD_010417712_0100_articletext.xml</td>\n      <td>lucht en methaan .  dat tussen [MASK] en [MASK...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10.000 m 3 lucht uit zijn ondergrondse werken....</td>\n      <td>36288</td>\n      <td>DDD_010417712_0100_articletext.xml</td>\n      <td>[MASK] . [MASK] m 3 lucht uit zijn ondergronds...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ruim 42 atmosfeer. | Het gas blijft waar het i...</td>\n      <td>36288</td>\n      <td>DDD_010417712_0100_articletext.xml</td>\n      <td>ruim [MASK] atmosfeer .  | Het gas blijft waar...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 204
    }
   ],
   "source": [
    "df_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a4d696355bf409a8e702f63e3e92d26"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "MASKIDS = []\n",
    "# Load, train and predict using pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"wietsedv/bert-base-dutch-cased\")\n",
    "# Load pre-trained model\n",
    "model = AutoModel.from_pretrained(\"wietsedv/bert-base-dutch-cased\")\n",
    "\n",
    "for ids, row in tqdm(df_texts.iterrows(), total=df_texts.shape[0]):\n",
    "    text = row[\"masked_texts\"]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    MASKIDS.append([i for i, e in enumerate(tokenized_text) if e == '[MASK]'])\n",
    "\n",
    "    # Create the segments\n",
    "    segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        predictions.append(model(tokens_tensor, segments_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97f72231973148f88809afd1ef9e73dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at wietsedv/bert-base-dutch-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "PipelineException",
     "evalue": "More than one mask_token ([MASK]) is not supported",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-e3dd96f9fdff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wietsedv/bert-base-dutch-cased\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         topk=5)\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/histaware-RplM6c3o-py3.8/lib/python3.8/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, targets, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                 \u001b[0;31m# Fill mask pipeline supports only one ${mask_token} per sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_exactly_one_mask_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/histaware-RplM6c3o-py3.8/lib/python3.8/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36mensure_exactly_one_mask_token\u001b[0;34m(self, masked_index)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0mnumel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             raise PipelineException(\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;34m\"fill-mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPipelineException\u001b[0m: More than one mask_token ([MASK]) is not supported"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "for ids, row in tqdm(df_texts.iterrows(), total=df_texts.shape[0]):\n",
    "    text = row[\"masked_texts\"]\n",
    "    nlp_fill = pipeline(\n",
    "        'fill-mask',\n",
    "        model=\"wietsedv/bert-base-dutch-cased\",\n",
    "        tokenizer=\"wietsedv/bert-base-dutch-cased\",\n",
    "        topk=5)\n",
    "    pprint(nlp_fill(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.1571, -0.6540, -0.1885,  ...,  0.0497,  0.1652, -0.4206],\n        [-0.2000,  0.4086, -0.7478,  ..., -0.1650, -0.1866,  0.1263],\n        [ 0.8099, -1.8925,  0.0474,  ...,  0.4340,  0.3164,  0.6638],\n        ...,\n        [-0.3609, -0.5708,  0.0787,  ...,  0.1913, -0.3615,  0.0275],\n        [-0.9404, -0.7848, -0.5773,  ..., -0.1136, -0.0910,  0.1418],\n        [ 1.0243, -0.8926, -0.2411,  ..., -0.3097,  0.3824,  0.2190]])\n"
     ]
    }
   ],
   "source": [
    "a = predictions[0][0]\n",
    "print(a[MASKIDS[0][i]])\n",
    "#preds = torch.topk(a[MASKIDS[0][i]], k=25)\n",
    "#indices = preds.indices.tolist()\n",
    "#print(indices)\n",
    "#list1 = tokenizer.convert_ids_to_tokens(indices)\n",
    "#for i in range(len(MASKIDS[0])):\n",
    "    #print(i)\n",
    "    #torch.topk(predictions[0][0, MASKIDS[0][i]], k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict words for mask using BERT; \n",
    "def predict_word(text, predictions, maskids):\n",
    "    pred_words=[]\n",
    "    for item in range(len(predictions)):\n",
    "        tns = predictions[item][0]\n",
    "        for i in range(len(maskids)):\n",
    "            preds = torch.topk(tns[maskids[item][i]], k=10) \n",
    "            indices = preds.indices.tolist()\n",
    "            list1 = tokenizer.convert_ids_to_tokens(indices)\n",
    "            list2 = suggestedwords[item][i]\n",
    "            simmax=0\n",
    "            predicted_token=''\n",
    "            for word1 in list1:\n",
    "                for word2 in list2:\n",
    "                    s = SequenceMatcher(None, word1, word2).ratio()\n",
    "                    if s is not None and s > simmax:\n",
    "                        simmax = s\n",
    "                        predicted_token = word1\n",
    "            text = text.replace('[MASK]', predicted_token, 1)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea4882760d9d4059b6b03f1cd5e7f06c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-1907732b4295>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmasked_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"masked_texts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtext_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMASKIDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-176-eb5634245a62>\u001b[0m in \u001b[0;36mpredict_word\u001b[0;34m(text, predictions, maskids)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmaskids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mlist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mlist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuggestedwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0msimmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/histaware-RplM6c3o-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "text_cleaned = []\n",
    "for ids, row in tqdm(df_texts.iterrows(), total=df_texts.shape[0]):\n",
    "    masked_text = row[\"masked_texts\"]\n",
    "    text_cleaned.append(predict_word(masked_text, predictions, MASKIDS))"
   ]
  },
  {
   "source": [
    "# LDA model and TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = nl_core_news_lg.load()\n",
    "\n",
    "# My list of stop words.\n",
    "#stop_list = [\"Mrs.\",\"Ms.\",\"say\",\"WASHINGTON\",\"'s\",\"Mr.\",]\n",
    "\n",
    "# Updates spaCy's default stop words list with my additional words. \n",
    "#nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "# Iterates over the words in the stop words list and resets the \"is_stop\" flag.\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2565.0), HTML(value=&#39;&#39;)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53a6908f93d84394b0423d4624147210"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    }
   ],
   "source": [
    "doc_list = []\n",
    "# Iterates through each article in the corpus.\n",
    "for d in tqdm(doc):\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(d)\n",
    "    doc_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates, which is a mapping of word IDs to words.\n",
    "words = corpora.Dictionary(doc_list)\n",
    "\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [words.doc2bow(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(0,\n  &#39;0.007*&quot;3&quot; + 0.007*&quot;b&quot; + 0.006*&quot;u.&quot; + 0.006*&quot;ned&quot; + 0.005*&quot;100&quot; + 0.005*&quot;4&quot; + 0.005*&quot;n&quot; + 0.005*&quot;1&quot; + 0.005*&quot;10&quot; + 0.005*&quot;eva&quot;&#39;),\n (1,\n  &#39;0.008*&quot;komen&quot; + 0.007*&quot;twee&quot; + 0.006*&quot;rust&quot; + 0.006*&quot;goed&quot; + 0.006*&quot;minuut&quot; + 0.006*&quot;doelpunt&quot; + 0.005*&quot;gast&quot; + 0.005*&quot;brengen&quot; + 0.005*&quot;weten&quot; + 0.004*&quot;gaan&quot;&#39;),\n (2,\n  &#39;0.016*&quot;ƒ&quot; + 0.015*&quot;1&quot; + 0.012*&quot;2&quot; + 0.011*&quot;■&quot; + 0.010*&quot;tel&quot; + 0.010*&quot;f&quot; + 0.009*&quot;koop&quot; + 0.009*&quot;ét&quot; + 0.009*&quot;no.&quot; + 0.008*&quot;br&quot;&#39;),\n (3,\n  &#39;0.007*&quot;komen&quot; + 0.006*&quot;goed&quot; + 0.006*&quot;groot&quot; + 0.005*&quot;één&quot; + 0.005*&quot;gaan&quot; + 0.005*&quot;jaar&quot; + 0.004*&quot;zien&quot; + 0.004*&quot;zeggen&quot; + 0.004*&quot;maken&quot; + 0.004*&quot;staan&quot;&#39;),\n (4,\n  &#39;0.016*&quot;g&quot; + 0.012*&quot;j.&quot; + 0.011*&quot;1950&quot; + 0.010*&quot;mei&quot; + 0.009*&quot;h.&quot; + 0.006*&quot;a.&quot; + 0.005*&quot;uur&quot; + 0.005*&quot;overlijden&quot; + 0.004*&quot;b&quot; + 0.004*&quot;fan&quot;&#39;),\n (5,\n  &#39;0.018*&quot;mrs&quot; + 0.011*&quot;mr&quot; + 0.007*&quot;j&quot; + 0.004*&quot;afgel&quot; + 0.004*&quot;|&quot; + 0.003*&quot;john&quot; + 0.003*&quot;ballon&quot; + 0.003*&quot;bezoeken&quot; + 0.002*&quot;mr.&quot; + 0.002*&quot;kerk&quot;&#39;),\n (6,\n  &#39;0.044*&quot;v&quot; + 0.019*&quot;n&quot; + 0.009*&quot;■&quot; + 0.007*&quot;17&quot; + 0.007*&quot;28&quot; + 0.007*&quot;25&quot; + 0.006*&quot;18&quot; + 0.006*&quot;mei&quot; + 0.006*&quot;p&quot; + 0.006*&quot;26&quot;&#39;),\n (7,\n  &#39;0.006*&quot;krant&quot; + 0.006*&quot;gast&quot; + 0.005*&quot;heer&quot; + 0.004*&quot;nederlands&quot; + 0.004*&quot;minister&quot; + 0.004*&quot;regering&quot; + 0.004*&quot;amsterdam&quot; + 0.004*&quot;land&quot; + 0.003*&quot;wensen&quot; + 0.003*&quot;groot&quot;&#39;),\n (8,\n  &#39;0.032*&quot;■&quot; + 0.009*&quot;&gt;&quot; + 0.006*&quot;1&quot; + 0.006*&quot;j&quot; + 0.005*&quot;n&quot; + 0.005*&quot;v&quot; + 0.004*&quot;&lt;&quot; + 0.004*&quot;k&quot; + 0.004*&quot;f&quot; + 0.004*&quot;m&quot;&#39;),\n (9,\n  &#39;0.035*&quot;3&quot; + 0.018*&quot;100&quot; + 0.015*&quot;4&quot; + 0.015*&quot;31&quot; + 0.008*&quot;101&quot; + 0.006*&quot;102&quot; + 0.006*&quot;96&quot; + 0.005*&quot;99&quot; + 0.005*&quot;34&quot; + 0.005*&quot;97&quot;&#39;)]"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}