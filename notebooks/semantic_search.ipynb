{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('histaware-RplM6c3o-py3.8')",
   "display_name": "Python 3.8.5 64-bit ('histaware-RplM6c3o-py3.8')",
   "metadata": {
    "interpreter": {
     "hash": "86315069d9bbcc5e7b6a2d7416b15171a30b5af37cfb10a9c473db24ce95b203"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2020-10-12 15:52:38,125 — parsers — DEBUG — Test message\n",
      "2020-10-12 15:52:38,125 — parsers — DEBUG — Test message\n",
      "2020-10-12 15:52:38,127 — iterators — DEBUG — Test message\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries needed\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/Users/leonardovida/dev/HistAware\")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from src import iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "source": [
    "# Sentence Transformer "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the model - Bertje\n",
    "tokenizer = BertTokenizer.from_pretrained(\"wietsedv/bert-base-dutch-cased\")\n",
    "model = BertModel.from_pretrained(\"wietsedv/bert-base-dutch-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the file\n",
    "csv = iterators.iterate_directory(\"../data/processed/selected_articles/\", \".csv\")\n",
    "df = pd.concat([pd.read_csv(c[\"article_path\"]) for c in csv],ignore_index=True)\n",
    "df.sort_values(by=[\"count\"], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df[0:100]\n",
    "df_remaining = df[101:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\n",
    "        \"../data/models/distiluse/distiluse-base-multilingual-cased\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(df_remaining[\"text\"])\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = list(df_selected[\"text\"])\n",
    "queries_embeddings = embedder.encode(queries, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for query_emb in queries_embeddings:\n",
    "    test = util.semantic_search(\n",
    "        query_embeddings=query_emb,\n",
    "        corpus_embeddings=corpus_embeddings,\n",
    "        query_chunk_size=100,\n",
    "        corpus_chunk_size=100000,\n",
    "        top_k=10,\n",
    "    )\n",
    "    results.append(test)\n",
    "    test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'corpus_id': 56, 'score': 1.0000002},\n",
       "  {'corpus_id': 1191, 'score': 0.46423584},\n",
       "  {'corpus_id': 1001, 'score': 0.46423584},\n",
       "  {'corpus_id': 169, 'score': 0.46423584},\n",
       "  {'corpus_id': 4795, 'score': 0.4443941},\n",
       "  {'corpus_id': 1085, 'score': 0.4443941},\n",
       "  {'corpus_id': 2554, 'score': 0.4293515},\n",
       "  {'corpus_id': 887, 'score': 0.42333776},\n",
       "  {'corpus_id': 13, 'score': 0.42276603},\n",
       "  {'corpus_id': 3721, 'score': 0.41876274}]]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ning tijdens de ontgassing normaal kan doorgaan. In de Belgische mijn „Le Grand Trait\" te Frameries in Henegouwen „oogstte\" men op deze wijze in 2 maanden tijds 378.000 m 3 methaangas, in de mijn „Saint Albert\" te Ressaix in een iets langere periode 428.650 m 3 methaan. In Henegouwen wordt het gas reeds naar buiten geleverd via de lichtgasfabrieken te Tertre. Methaangas levert 8000 tot 9000 caloriën warmte, hetgeen tweemaal zoveel is als gewoon cokesovengas. In vele andere mijnen, waaronder de Kempische, neemt men proeven. Er bestaan plannen in Belgisch Limburg een leidingermet aan te leggen voor de distributie van het gas aan de bevolking. Een probleem vormt echter de vrij onregelmatige toevoer, waarmee men ongetwijfeld te kampen zal krijgen. In de mijn Hirschbach in het Saargebied heeft men een andere methode gevolgd. In deze mijn ontsnapte zoveel gas dat met luchtverversing niet voldoende te bereiken was. Een gedeelte van de mijn werd daarom met dammen van de rest afgesloten. Door hel verrichten van boringen en door het afbouwen van een dieper gelegen laag zorgde men er voor, dat het gas naar de afgesloten ruimte ontsnapte. Hieruit werd het weggezogen om bovengronds te worden gebruikt voor de verhitting van een stoomketel. De besparing bedroeg 500 ton kolen per maand. In ruim 11 jaar won men in Hirschbach 5 millioen in 3 mijngas. Voordelen Technisch gesproken heeft men al veel bereikt. Er blijven echter nog problemen, die om een oplossing vragen. Het lijdt echter geen twijfel, dat men op het goede spoor is. Aan de mijngas-winning zijn vele voordelen verbonden. In het ene mijnbekken zal een bepaald voordeel zwaarder wegen dan in het andere, maar in het algemeen zijn het de volgende. Door het gas te winnen voordat het in pijlers en mijngangen terecht komt, wordt de veiligheid in het ondergronds bedrijf verhoogd. De kans op vorming van ontplofbare mengsels wordt veel kleiner. Hiermee wordt het belang van de mijnwerkers gediend. In een buitenlandse mijn daalde het gasgehalte van de uittrekkende luchtstroom op deze manier van 2 of 3 percent tot beneden 1 percent. Daarnaast is er een economisch voordeel, dat niet alleen voor de mijnwerkers, maar ook voor een veel grotere groep mensen van belang is. Het gas kan worden gebruikt voor de voorziening van industrieën en woningen. Met het mijngas, dat nu dagelijks uit onze mijnen wordt geblazen, zouden zeker tienduizenden huismoeders hun potje kunnen koken. Een andere mogelijkheid is dat het als stookgas wordt gebruikt in ketels, waardoor kolen worden bespaard. En dan zullen er zeker chemische fabrieken zijn die er nuttige producten van kunnen maken. Voor de mijnen zelf zijn er ook speciale voordelen verbonden aan de ontgassing. Het komt vaak voor dat de koolwinning wordt gehandicapt door het ontsnappen van mijngas. Het kan zijn dat van een bepaalde pijler gemakkelijk 1000 ton kolen per dag kunnen worden getrokken, terwijl men er toch niet meer dan 500 ton uithaalt, omdat anders het mijngasgehalte in de luchtstroom boven het voorgeschreven peil stijgt. Wordt de betreffende kolenlaag voor of tijdens de exploitatie ontgast, dan kan men zonder bezwaar de productie opvoeren. Dat is een kwestie van meer mensen. Dit voordeel noemt men: een grotere concentratie van de ontginning. Deze doet de kostprijs per ton dalen. Ten slotte is daar f N De moderne gestroomlijnde schacht van Staatsmijn Emma, waardoor per aur 500 ton kool van 546 meter dieptt naar boven wordt gebracht -\"* -•-•■ \\'•■ _■ _,*<£_&_ ■■■■■-■ \\'fTÜlfc\\'in •■:-\\' ■\\' ...*......->...'"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "df_selected.iloc[1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ning tijdens de ontgassing normaal kan doorgaan. In de Belgische mijn „Le Grand Trait\" te Frameries in Henegouwen „oogstte\" men op deze wijze in 2 maanden tijds 378.000 m 3 methaangas, in de mijn „Saint Albert\" te Ressaix in een iets langere periode 428.650 m 3 methaan. In Henegouwen wordt het gas reeds naar buiten geleverd via de lichtgasfabrieken te Tertre. Methaangas levert 8000 tot 9000 caloriën warmte, hetgeen tweemaal zoveel is als gewoon cokesovengas. In vele andere mijnen, waaronder de Kempische, neemt men proeven. Er bestaan plannen in Belgisch Limburg een leidingermet aan te leggen voor de distributie van het gas aan de bevolking. Een probleem vormt echter de vrij onregelmatige toevoer, waarmee men ongetwijfeld te kampen zal krijgen. In de mijn Hirschbach in het Saargebied heeft men een andere methode gevolgd. In deze mijn ontsnapte zoveel gas dat met luchtverversing niet voldoende te bereiken was. Een gedeelte van de mijn werd daarom met dammen van de rest afgesloten. Door hel verrichten van boringen en door het afbouwen van een dieper gelegen laag zorgde men er voor, dat het gas naar de afgesloten ruimte ontsnapte. Hieruit werd het weggezogen om bovengronds te worden gebruikt voor de verhitting van een stoomketel. De besparing bedroeg 500 ton kolen per maand. In ruim 11 jaar won men in Hirschbach 5 millioen in 3 mijngas. Voordelen Technisch gesproken heeft men al veel bereikt. Er blijven echter nog problemen, die om een oplossing vragen. Het lijdt echter geen twijfel, dat men op het goede spoor is. Aan de mijngas-winning zijn vele voordelen verbonden. In het ene mijnbekken zal een bepaald voordeel zwaarder wegen dan in het andere, maar in het algemeen zijn het de volgende. Door het gas te winnen voordat het in pijlers en mijngangen terecht komt, wordt de veiligheid in het ondergronds bedrijf verhoogd. De kans op vorming van ontplofbare mengsels wordt veel kleiner. Hiermee wordt het belang van de mijnwerkers gediend. In een buitenlandse mijn daalde het gasgehalte van de uittrekkende luchtstroom op deze manier van 2 of 3 percent tot beneden 1 percent. Daarnaast is er een economisch voordeel, dat niet alleen voor de mijnwerkers, maar ook voor een veel grotere groep mensen van belang is. Het gas kan worden gebruikt voor de voorziening van industrieën en woningen. Met het mijngas, dat nu dagelijks uit onze mijnen wordt geblazen, zouden zeker tienduizenden huismoeders hun potje kunnen koken. Een andere mogelijkheid is dat het als stookgas wordt gebruikt in ketels, waardoor kolen worden bespaard. En dan zullen er zeker chemische fabrieken zijn die er nuttige producten van kunnen maken. Voor de mijnen zelf zijn er ook speciale voordelen verbonden aan de ontgassing. Het komt vaak voor dat de koolwinning wordt gehandicapt door het ontsnappen van mijngas. Het kan zijn dat van een bepaalde pijler gemakkelijk 1000 ton kolen per dag kunnen worden getrokken, terwijl men er toch niet meer dan 500 ton uithaalt, omdat anders het mijngasgehalte in de luchtstroom boven het voorgeschreven peil stijgt. Wordt de betreffende kolenlaag voor of tijdens de exploitatie ontgast, dan kan men zonder bezwaar de productie opvoeren. Dat is een kwestie van meer mensen. Dit voordeel noemt men: een grotere concentratie van de ontginning. Deze doet de kostprijs per ton dalen. Ten slotte is daar f N De moderne gestroomlijnde schacht van Staatsmijn Emma, waardoor per aur 500 ton kool van 546 meter dieptt naar boven wordt gebracht -\"* -•-•■ \\'•■ _■ _,*<£_&_ ■■■■■-■ \\'fTÜlfc\\'in •■:-\\' ■\\' ...*......->...'"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "df_remaining.iloc[56][\"text\"]"
   ]
  },
  {
   "source": [
    "# For sentiment analysis system"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                text   type\n0                               Aardgas bij Oostzaan  title\n1  Op een diepte van 220 meter is Dinsdagmiddag b...      p\n2  Vier nieuwe boringen in het Duitse petro leumg...      p\n3  &#39;s-GRAVENHAGE, 16 Nov. — Aan de Naamloze Venno...      p\n4  Onmiddellijk na deze toespraken werd weer inge...      p",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Aardgas bij Oostzaan</td>\n      <td>title</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Op een diepte van 220 meter is Dinsdagmiddag b...</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Vier nieuwe boringen in het Duitse petro leumg...</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>'s-GRAVENHAGE, 16 Nov. — Aan de Naamloze Venno...</td>\n      <td>p</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Onmiddellijk na deze toespraken werd weer inge...</td>\n      <td>p</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/selected_articles/2020-10-07_aardgas.csv\")\n",
    "# This step is necessary to modify the classes into a `list` column\n",
    "# To be used downstream\n",
    "#df[\"list\"] = df[df.columns[\"text\", \"type\"]].values.tolist()\n",
    "new_df = df[[\"text\", \"type\"]].copy()\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = \"max_length\"\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "MODEL = \"wietsedv/bert-base-dutch-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model - Bertje\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
    "model = BertModel.from_pretrained(MODEL)"
   ]
  },
  {
   "source": [
    "Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.type\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "FULL Dataset: (2565, 2)\nTRAIN Dataset: (2052, 2)\nTEST Dataset: (513, 2)\n"
    }
   ],
   "source": [
    "\n",
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "            'shuffle': True,\n",
    "            'num_workers': 0\n",
    "            }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "            'shuffle': True,\n",
    "            'num_workers': 0\n",
    "            }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name &#39;batch_sentences&#39; is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m&lt;ipython-input-31-21da5e5961b4&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[0;32m----&gt; 1\u001b[0;31m encoded_input = tokenizer(batch_sentences,\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     return_tensors=&quot;pt&quot;)\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m&quot;input_ids&quot;\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name &#39;batch_sentences&#39; is not defined"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch_sentences,\n",
    "    # Pad to the longest allowed length by the model \n",
    "    padding=\"max_length\",\n",
    "    # Truncate to maximum length\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\")\n",
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Correct OCR words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nl_core_news_lg\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import enchant\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from enchant.checker import SpellChecker\n",
    "from difflib import SequenceMatcher\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_original = a\n",
    "text = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+ Op [MASK] Heren Hemelvaartsdag overleed tot onze diepe droefheid   ,    na een langdurig lijden op [MASK] jarige leeft   ,    voorzien van de laatste H   .   H   .    Sacramenten der Stervenden   ,    en in volledige overgave aan Gods H   .    [MASK] [MASK] [MASK] weduwnaar van Mextildns [MASK] Wij bevelen [MASK] ziel in Uwe [MASK]   .    [MASK] aan   .    De bedroefde [MASK] Hnbert [MASK] [MASK] en kinderen Jan [MASK] Wetsels en kinderen Peter Wetsels en kinderen Jan Paas Wetsels en kinderen Charel [MASK] Wetsels en kind Jac   .    Wetsels Brands en kind De plechtige [MASK]   .   dienst gevolgd door de begrafenis zal plaats hebben Maandag [MASK] Mei om 9   .   [MASK] uur in de par   .    kerk te [MASK]   .    [MASK]   .    ten sterf [MASK] om 9 uur   .    [MASK] Zondagavond om 7 uur   .    i &gt; i Voor Uwe [MASK] en vele blijken Van deelneming   ,    betoond bij de ziekte   ,    het overlijden en de begrafenis van onze dierbare broer   ,    zwager   ,    oom en neef   ,    de [MASK]   .    Heer [MASK] [MASK] [MASK] betuigen wij onze hartelijke dank   .    Familie Roex Schinveld   ,    Wilhelminaplein   .    De plechtige [MASK]   .    zal plaats hebben in de par   .    kerk van de H   .    Elifius te Schinveld [MASK] [MASK] Mei a   .   s   .    om 8   .   [MASK] uur   .    — Beleefd verzocht deze te willen bijwonen   .    De plechtige jaardienst voor onze dierbare echtgenoten [MASK] [MASK] [MASK]   .    [MASK] en [MASK] [MASK] [MASK] MARIA [MASK] [MASK]   .    [MASK] zal plaats hebben Donderdag [MASK] Mei a s   .    in de par   .    kerk van de H   .    [MASK] te Weiten om 7   .   [MASK] uur   .    M   .    [MASK] en A   .    [MASK]\n"
    }
   ],
   "source": [
    "rep = { '\\n': ' ', '\\\\': ' ', '\\\"': '\"', '-': ' ', '\"': ' \" ', \n",
    "        '\"': ' \" ', '\"': ' \" ', ',':' , ', '.':' . ', '!':' ! ', \n",
    "        '?':' ? ', \"n't\": \" not\" , \"'ll\": \" will\", '*':' * ', \n",
    "        '(': ' ( ', ')': ' ) ', \"s'\": \"s '\"}\n",
    "rep = dict((re.escape(k), v) for k, v in rep.items()) \n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n",
    "\n",
    "def get_personslist(text):\n",
    "    personslist=[]\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if isinstance(chunk, nltk.tree.Tree) and chunk.label() == 'PERSON':\n",
    "                personslist.insert(0, (chunk.leaves()[0][0]))\n",
    "    return list(set(personslist))\n",
    "\n",
    "personslist = get_personslist(text)\n",
    "\n",
    "ignorewords = personslist + [\"!\", \",\", \".\", \"\\\"\", \"?\", '(', ')', '*', \"'\"]\n",
    "#using enchant.checker.SpellChecker, identify incorrect words\n",
    "d = SpellChecker(\"nl_NL\")\n",
    "words = text.split()\n",
    "incorrectwords = [w for w in words if not d.check(w) and w not in ignorewords]\n",
    "# using enchant.checker.SpellChecker, get suggested replacements\n",
    "suggestedwords = [d.suggest(w) for w in incorrectwords]\n",
    "# replace incorrect words with [MASK]\n",
    "for w in incorrectwords:\n",
    "    text = text.replace(w, '[MASK]')\n",
    "    text_original = text_original.replace(w, '[MASK]')\n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load, train and predict using pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../data/models/bertje\")\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "MASKIDS = [i for i, e in enumerate(tokenized_text) if e == '[MASK]']\n",
    "# Create the segments tensors\n",
    "segs = [i for i, e in enumerate(tokenized_text) if e == \".\"]\n",
    "segments_ids=[]\n",
    "prev=-1\n",
    "for k, s in enumerate(segs):\n",
    "    segments_ids = segments_ids + [k] * (s-prev)\n",
    "    prev=s\n",
    "segments_ids = segments_ids + [len(segs)] * (len(tokenized_text) - len(segments_ids))\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "# prepare Torch inputs \n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "# Load pre-trained model\n",
    "model = AutoModel.from_pretrained(\"../data/models/bertje\")\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m&lt;ipython-input-169-bc8e413fed4a&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Predict words for mask using BERT;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----&gt; 2\u001b[0;31m \u001b[0mtext_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMASKIDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m&lt;ipython-input-161-870f1396a09c&gt;\u001b[0m in \u001b[0;36mpredict_word\u001b[0;34m(text_original, predictions, maskids)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpred_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMASKIDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----&gt; 6\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMASKIDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "\n",
    "#Predict words for mask using BERT; \n",
    "def predict_word(text_original, predictions, maskids):\n",
    "    pred_words=[]\n",
    "    for i in range(len(MASKIDS)):\n",
    "        preds = torch.topk(predictions[0, MASKIDS[i]], k=50) \n",
    "        indices = preds.indices.tolist()\n",
    "        list1 = tokenizer.convert_ids_to_tokens(indices)\n",
    "        list2 = suggestedwords[i]\n",
    "        simmax=0\n",
    "        predicted_token=''\n",
    "        for word1 in list1:\n",
    "            for word2 in list2:\n",
    "                s = SequenceMatcher(None, word1, word2).ratio()\n",
    "                if s is not None and s > simmax:\n",
    "                    simmax = s\n",
    "                    predicted_token = word1\n",
    "        text_original = text_original.replace('[MASK]', predicted_token, 1)\n",
    "    return text_original\n",
    "\n",
    "text_original = predict_word(text_original, predictions, MASKIDS)\n",
    "print (text_original)"
   ]
  },
  {
   "source": [
    "# LDA model and TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = nl_core_news_lg.load()\n",
    "\n",
    "# My list of stop words.\n",
    "#stop_list = [\"Mrs.\",\"Ms.\",\"say\",\"WASHINGTON\",\"'s\",\"Mr.\",]\n",
    "\n",
    "# Updates spaCy's default stop words list with my additional words. \n",
    "#nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "# Iterates over the words in the stop words list and resets the \"is_stop\" flag.\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2565.0), HTML(value=&#39;&#39;)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53a6908f93d84394b0423d4624147210"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    }
   ],
   "source": [
    "doc_list = []\n",
    "# Iterates through each article in the corpus.\n",
    "for d in tqdm(doc):\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(d)\n",
    "    doc_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates, which is a mapping of word IDs to words.\n",
    "words = corpora.Dictionary(doc_list)\n",
    "\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [words.doc2bow(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(0,\n  &#39;0.007*&quot;3&quot; + 0.007*&quot;b&quot; + 0.006*&quot;u.&quot; + 0.006*&quot;ned&quot; + 0.005*&quot;100&quot; + 0.005*&quot;4&quot; + 0.005*&quot;n&quot; + 0.005*&quot;1&quot; + 0.005*&quot;10&quot; + 0.005*&quot;eva&quot;&#39;),\n (1,\n  &#39;0.008*&quot;komen&quot; + 0.007*&quot;twee&quot; + 0.006*&quot;rust&quot; + 0.006*&quot;goed&quot; + 0.006*&quot;minuut&quot; + 0.006*&quot;doelpunt&quot; + 0.005*&quot;gast&quot; + 0.005*&quot;brengen&quot; + 0.005*&quot;weten&quot; + 0.004*&quot;gaan&quot;&#39;),\n (2,\n  &#39;0.016*&quot;ƒ&quot; + 0.015*&quot;1&quot; + 0.012*&quot;2&quot; + 0.011*&quot;■&quot; + 0.010*&quot;tel&quot; + 0.010*&quot;f&quot; + 0.009*&quot;koop&quot; + 0.009*&quot;ét&quot; + 0.009*&quot;no.&quot; + 0.008*&quot;br&quot;&#39;),\n (3,\n  &#39;0.007*&quot;komen&quot; + 0.006*&quot;goed&quot; + 0.006*&quot;groot&quot; + 0.005*&quot;één&quot; + 0.005*&quot;gaan&quot; + 0.005*&quot;jaar&quot; + 0.004*&quot;zien&quot; + 0.004*&quot;zeggen&quot; + 0.004*&quot;maken&quot; + 0.004*&quot;staan&quot;&#39;),\n (4,\n  &#39;0.016*&quot;g&quot; + 0.012*&quot;j.&quot; + 0.011*&quot;1950&quot; + 0.010*&quot;mei&quot; + 0.009*&quot;h.&quot; + 0.006*&quot;a.&quot; + 0.005*&quot;uur&quot; + 0.005*&quot;overlijden&quot; + 0.004*&quot;b&quot; + 0.004*&quot;fan&quot;&#39;),\n (5,\n  &#39;0.018*&quot;mrs&quot; + 0.011*&quot;mr&quot; + 0.007*&quot;j&quot; + 0.004*&quot;afgel&quot; + 0.004*&quot;|&quot; + 0.003*&quot;john&quot; + 0.003*&quot;ballon&quot; + 0.003*&quot;bezoeken&quot; + 0.002*&quot;mr.&quot; + 0.002*&quot;kerk&quot;&#39;),\n (6,\n  &#39;0.044*&quot;v&quot; + 0.019*&quot;n&quot; + 0.009*&quot;■&quot; + 0.007*&quot;17&quot; + 0.007*&quot;28&quot; + 0.007*&quot;25&quot; + 0.006*&quot;18&quot; + 0.006*&quot;mei&quot; + 0.006*&quot;p&quot; + 0.006*&quot;26&quot;&#39;),\n (7,\n  &#39;0.006*&quot;krant&quot; + 0.006*&quot;gast&quot; + 0.005*&quot;heer&quot; + 0.004*&quot;nederlands&quot; + 0.004*&quot;minister&quot; + 0.004*&quot;regering&quot; + 0.004*&quot;amsterdam&quot; + 0.004*&quot;land&quot; + 0.003*&quot;wensen&quot; + 0.003*&quot;groot&quot;&#39;),\n (8,\n  &#39;0.032*&quot;■&quot; + 0.009*&quot;&gt;&quot; + 0.006*&quot;1&quot; + 0.006*&quot;j&quot; + 0.005*&quot;n&quot; + 0.005*&quot;v&quot; + 0.004*&quot;&lt;&quot; + 0.004*&quot;k&quot; + 0.004*&quot;f&quot; + 0.004*&quot;m&quot;&#39;),\n (9,\n  &#39;0.035*&quot;3&quot; + 0.018*&quot;100&quot; + 0.015*&quot;4&quot; + 0.015*&quot;31&quot; + 0.008*&quot;101&quot; + 0.006*&quot;102&quot; + 0.006*&quot;96&quot; + 0.005*&quot;99&quot; + 0.005*&quot;34&quot; + 0.005*&quot;97&quot;&#39;)]"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}