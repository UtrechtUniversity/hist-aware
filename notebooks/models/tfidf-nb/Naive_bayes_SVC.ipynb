{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline for BERT: Tf-idf + Naive Bayes or SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"~/dev/hist-aware/notebooks/data/labeled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"labeled_energy_1960_1990.csv\"))\n",
    "oil = pd.read_csv(os.path.join(DATA_DIR, \"labeled_oil_1960_1990.csv\"))\n",
    "gas = pd.read_csv(os.path.join(DATA_DIR, \"labeled_gas_1960_1990.csv\"))\n",
    "coal = pd.read_csv(os.path.join(DATA_DIR, \"labeled_coal_1960_1990.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.text_clean.values\n",
    "y = df.labels.values\n",
    "#y = label_binarize(df.labels.values, classes=[0, 1, 2])\n",
    "\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set GPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"dutch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(s):\n",
    "    \"\"\"\n",
    "    - Lowercase the sentence\n",
    "    - Isolate and remove punctuations except \"?\"\n",
    "    - Remove other special characters\n",
    "    - Remove trailing whitespace\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Isolate and remove punctuations except '?'\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
    "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Remove stopwords except 'not' and 'can'\n",
    "    s = \" \".join([word for word in s.split()\n",
    "                  if word not in stopwords])\n",
    "    # Remove trailing whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess already cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.81 s, sys: 26.5 ms, total: 1.83 s\n",
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocess text\n",
    "X_train = np.array([text_preprocessing(text) for text in X_train])\n",
    "X_val = np.array([text_preprocessing(text) for text in X_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Pipeline: TF-IDF vectorizer and MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6460176991150443"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_mnb = Pipeline([\n",
    "    (\"tf-idf\", TfidfVectorizer()),\n",
    "    (\"clf\",  MultinomialNB())\n",
    "])\n",
    "text_clf_mnb.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf_mnb.predict(X_val)\n",
    "np.mean(predicted == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline with SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7103781174577635"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_sdg = Pipeline([\n",
    "    ('tf-idf', TfidfVectorizer(ngram_range=(1, 2), binary=True, smooth_idf=False)),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])\n",
    "text_clf_sdg.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf_sdg.predict(X_val)\n",
    "np.mean(predicted == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [285, 528]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-f9228f4bf627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/histaware-fgpHC0Ao-py3.8/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/histaware-fgpHC0Ao-py3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1927\u001b[0m     \"\"\"\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/histaware-fgpHC0Ao-py3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/histaware-fgpHC0Ao-py3.8/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [285, 528]"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val, predicted))\n",
    "metrics.confusion_matrix(y_val, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'tf-idf__use_idf': (True, False),\n",
    "    'tf-idf__norm': ('l1', 'l2', None),\n",
    "    'tf-idf__max_df': (0.5, 0.75, 1.0),\n",
    "    'tf-idf__max_features': (None, 5000, 10000, 50000),\n",
    "    'tf-idf__ngram_range': ((1, 1), (1, 2), (1,3)),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf_mnb, parameters, cv=10, n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 432 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:   35.3s\n",
      "[Parallel(n_jobs=-1)]: Done 222 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 245 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done 293 tasks      | elapsed:   48.0s\n",
      "[Parallel(n_jobs=-1)]: Done 318 tasks      | elapsed:   54.1s\n",
      "[Parallel(n_jobs=-1)]: Done 345 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 372 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 401 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 430 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 461 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 492 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 525 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 558 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 593 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 628 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 665 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 702 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 741 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 780 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 821 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 862 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 905 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 948 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 993 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1038 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1085 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1132 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1181 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1230 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1281 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1332 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1385 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1438 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1493 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1548 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1605 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1662 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1721 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1780 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1841 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1902 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1965 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2028 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2093 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2158 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2225 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2292 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2361 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2430 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2501 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2572 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2645 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2718 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2793 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2868 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2945 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3022 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3101 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3180 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3261 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3342 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3425 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3508 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3593 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3678 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3765 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3852 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3941 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4030 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4121 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4212 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4320 out of 4320 | elapsed: 12.9min finished\n"
     ]
    }
   ],
   "source": [
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 1e-06\n",
      "tf-idf__max_df: 0.5\n",
      "tf-idf__max_features: None\n",
      "tf-idf__ngram_range: (1, 3)\n",
      "tf-idf__norm: 'l1'\n",
      "tf-idf__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "gs_clf.best_score_\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6338406445837064"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_sdg = Pipeline([\n",
    "    (\"tf-idf\", TfidfVectorizer(max_df = 0.5, max_features=None, ngram_range = (1, 3), norm = 'l1', use_idf = True)),\n",
    "    (\"clf\",  MultinomialNB(alpha = 1e-06))\n",
    "])\n",
    "text_clf_sdg.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf_sdg.predict(X_val)\n",
    "np.mean(predicted == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leonardovida/Dropbox/work/1_projects/2_histaware/data/raw/raw_selected/1960s/to_label_oil.csv\n",
      "/Users/leonardovida/Dropbox/work/1_projects/2_histaware/data/raw/raw_selected/1970s/to_label_oil.csv\n"
     ]
    }
   ],
   "source": [
    "DIR = \"/Users/leonardovida/Dropbox/work/1_projects/2_histaware/data/raw/raw_selected\"\n",
    "import os\n",
    "import imblearn\n",
    "\n",
    "df = []\n",
    "for root, dirs, files in os.walk(DIR):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            if \"oil\" in file:\n",
    "                df.append(pd.read_csv(os.path.join(root, file)))\n",
    "                print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.84      0.68      0.93      0.75      0.80      0.62       159\n",
      "          1       0.75      0.92      0.71      0.83      0.80      0.66       230\n",
      "          2       0.82      0.60      0.97      0.69      0.76      0.56        75\n",
      "\n",
      "avg / total       0.79      0.78      0.83      0.78      0.79      0.63       464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "\n",
    "# Load data\n",
    "oil = pd.read_csv(os.path.join(DATA_DIR, \"labeled_oil_1960_1990.csv\"))\n",
    "df = oil.copy()\n",
    "\n",
    "# Copy vectors\n",
    "X = df.text_clean.values\n",
    "y = df.labels.values\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=2020)\n",
    "\n",
    "# Preprocess text\n",
    "X_train = np.array([text_preprocessing(text) for text in X_train])\n",
    "X_val = np.array([text_preprocessing(text) for text in X_val])\n",
    "\n",
    "# Pipeline\n",
    "model_oil = make_pipeline_imb(\n",
    "    TfidfVectorizer(max_df = 0.5, max_features=None, ngram_range = (1, 3), norm = 'l1', use_idf = True),\n",
    "    RandomOverSampler(sampling_strategy='minority'),\n",
    "    MultinomialNB(alpha = 1e-06))\n",
    "model_oil.fit(X_train, y_train)\n",
    "y_pred = model_oil.predict(X_val)\n",
    "\n",
    "print(classification_report_imbalanced(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil = pd.read_csv(\"/Users/leonardovida/Dropbox/work/1_projects/2_histaware/data/raw/raw_selected/1970s/to_label_oil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil.dropna(subset=[\"text_clean\"], inplace=True)\n",
    "preds = model_oil.predict(df_oil[\"text_clean\"])\n",
    "df_oil[\"prediction_sentiment_nb\"] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.72      0.33      0.96      0.46      0.57      0.30        87\n",
      "          1       0.57      0.88      0.32      0.69      0.53      0.30       186\n",
      "          2       0.66      0.29      0.95      0.40      0.52      0.26        93\n",
      "\n",
      "avg / total       0.63      0.60      0.63      0.56      0.54      0.29       366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "gas = pd.read_csv(os.path.join(DATA_DIR, \"labeled_gas_1960_1990.csv\"))\n",
    "df = gas.copy()\n",
    "\n",
    "# Copy vectors\n",
    "X = df.text_clean.values\n",
    "y = df.labels.values\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=2020)\n",
    "\n",
    "# Preprocess text\n",
    "X_train = np.array([text_preprocessing(text) for text in X_train])\n",
    "X_val = np.array([text_preprocessing(text) for text in X_val])\n",
    "\n",
    "# Pipeline\n",
    "model_gas = make_pipeline_imb(\n",
    "    TfidfVectorizer(max_df = 0.5, max_features=None, ngram_range = (1, 3), norm = 'l1', use_idf = True),\n",
    "    RandomOverSampler(sampling_strategy='minority'),\n",
    "    MultinomialNB(alpha = 1e-06))\n",
    "model_gas.fit(X_train, y_train)\n",
    "y_pred = model_gas.predict(X_val)\n",
    "\n",
    "print(classification_report_imbalanced(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gas = pd.read_csv(\"/Users/leonardovida/Dropbox/work/1_projects/2_histaware/data/raw/raw_selected/1970s/to_label_gas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gas.dropna(subset=[\"text_clean\"], inplace=True)\n",
    "preds = model_gas.predict(df_gas[\"text_clean\"])\n",
    "df_gas[\"prediction_sentiment_nb\"] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.75      0.51      0.93      0.61      0.69      0.46        82\n",
      "          1       0.55      0.67      0.66      0.60      0.66      0.44       111\n",
      "          2       0.56      0.57      0.78      0.56      0.66      0.43        95\n",
      "\n",
      "avg / total       0.61      0.59      0.77      0.59      0.67      0.44       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "coal = pd.read_csv(os.path.join(DATA_DIR, \"labeled_coal_1960_1990.csv\"))\n",
    "df = coal.copy()\n",
    "\n",
    "# Copy vectors\n",
    "X = df.text_clean.values\n",
    "y = df.labels.values\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=2020)\n",
    "\n",
    "# Preprocess text\n",
    "X_train = np.array([text_preprocessing(text) for text in X_train])\n",
    "X_val = np.array([text_preprocessing(text) for text in X_val])\n",
    "\n",
    "# Pipeline\n",
    "model_coal = make_pipeline_imb(\n",
    "    TfidfVectorizer(max_df = 0.5, max_features=None, ngram_range = (1, 3), norm = 'l1', use_idf = True),\n",
    "    RandomOverSampler(sampling_strategy='minority'),\n",
    "    MultinomialNB(alpha = 1e-06))\n",
    "model_coal.fit(X_train, y_train)\n",
    "y_pred = model_coal.predict(X_val)\n",
    "\n",
    "print(classification_report_imbalanced(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coal = pd.read_csv(\"/Users/leonardovida/Dropbox/work/1_projects/2_histaware/data/raw/raw_selected/1970s/to_label_coal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coal.dropna(subset=[\"text_clean\"], inplace=True)\n",
    "preds = model_coal.predict(df_coal[\"text_clean\"])\n",
    "df_coal[\"prediction_sentiment_nb\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coal.to_csv(\"/Users/leonardovida/Desktop/df_coal_nb.csv\")\n",
    "df_oil.to_csv(\"/Users/leonardovida/Desktop/df_oil_.csv\")\n",
    "df_gas.to_csv(\"/Users/leonardovida/Desktop/df_gas_nb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
