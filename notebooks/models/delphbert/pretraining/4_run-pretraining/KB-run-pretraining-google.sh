python bert/run_pretraining.py \
    --bert_config_file=/home/leonardovida/dev/delphbert/config/bert_config.json \
    --input_file="/home/leonardovida/data/volume_1/data-histaware/pretrain-data/pretraining-data-shuffled/tf_examples_*.tfrecord" \
    --output_dir=/home/leonardovida/data/volume_1/data-histaware/pretraining_output \
    --max_seq_length=512 \
    --max_predictions_per_seq=20 \
    --do_train=True \
    --do_eval=True \
    --train_batch_size=2 \
    --eval_batch_size=4 \
    --learning_rate=1e-4 \
    --num_train_steps=1000000 \
    --num_warmup_steps=10000 \
    --save_checkpoints_steps=10000 \
    --iterations_per_loop=10000 \
    --max_eval_steps=10000