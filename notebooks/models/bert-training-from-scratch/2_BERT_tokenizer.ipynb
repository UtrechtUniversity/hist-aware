{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f9d1d7",
   "metadata": {},
   "source": [
    "# DelphBERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9598ec7-80f9-4645-8f8e-af2f569c0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "338efff0-a72b-4d90-a533-e18e037f4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_FILES = \"/home/leonardovida/data/volume_1/data-histaware/merged_articles/1970s\"\n",
    "PATH_TOKENIZER_DIR = \"/home/leonardovida/data/volume_1/data-histaware/tokenizer\"\n",
    "PATH_DATASET_DIR = \"/home/leonardovida/data/volume_1/data-histaware/dataset\"\n",
    "\n",
    "dataset = load_from_disk(PATH_DATASET_DIR)\n",
    "#!mkdir PATH_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e0f52b-26cd-4a3d-a8f0-cb7426fca28a",
   "metadata": {},
   "source": [
    "## Train WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca67a716-7f83-4884-9d6a-83d5f79e391d",
   "metadata": {},
   "source": [
    "### From text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "176852f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [str(x) for x in Path(\"/home/leonardovida/data/volume_1/data-histaware/dataset/\").glob(\"**/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ba90db9-76c5-45ae-a6b7-bec3bf478bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=52000,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    min_frequency=3, \n",
    ")\n",
    "\n",
    "bert_tokenizer.train(files=paths, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb831b3f-0de4-401e-af9b-9391b5cd930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.save(f\"{PATH_TOKENIZER_DIR}/1970\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0d3e0-3a2e-403f-b695-d70a04568850",
   "metadata": {},
   "source": [
    "## Train BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a28a12d4-2b81-417e-8c37-679004d60174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/leonardovida/data/volume_1/data-histaware/tokenizer/1970_new/bert-wordpiece-vocab.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    lowercase=False,\n",
    "    strip_accents=False,\n",
    "    clean_text=True\n",
    ")\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[f\"{PATH_DATASET_DIR}/data.1970.txt\"],\n",
    "    vocab_size=52000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\n",
    "        \"[PAD]\",\n",
    "        \"[UNK]\",\n",
    "        \"[CLS]\",\n",
    "        \"[SEP]\",\n",
    "        \"[MASK]\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cab30444-773e-4640-995a-a1c70ad59236",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertWordPieceTokenizer' object has no attribute 'save_vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6f5ca42ed09c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{PATH_TOKENIZER_DIR}/bert-wordpiece.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# create a BERT tokenizer with trained vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{PATH_TOKENIZER_DIR}/bert-wordpiece-vocab.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertWordPieceTokenizer' object has no attribute 'save_vocabulary'"
     ]
    }
   ],
   "source": [
    "tokenizer.save(f\"{PATH_TOKENIZER_DIR}/bert-wordpiece.json\")\n",
    "# create a BERT tokenizer with trained vocab\n",
    "tokenizer.save_vocabulary(f\"{PATH_TOKENIZER_DIR}/bert-wordpiece-vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05a7fcdb-2760-4233-9d7d-7648f54d9dd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertWordPieceTokenizer' object has no attribute 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-58faf26733ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertWordPieceTokenizer' object has no attribute 'max_length'"
     ]
    }
   ],
   "source": [
    "tokenizer.max_length(max_length = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44341a1-1481-472c-9945-2e465c3cad50",
   "metadata": {},
   "source": [
    "## Train BPE Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d4b427-109a-4e26-a85a-3d61508d3373",
   "metadata": {},
   "source": [
    "### From dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d91f475-86a4-4d3b-be2f-c9d56d47fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tokenizer\n",
    "import datasets\n",
    "from tokenizers import normalizers, pre_tokenizers, Tokenizer, models, trainers\n",
    "\n",
    "# Build a tokenizer\n",
    "bpe_tokenizer = Tokenizer(models.BPE())\n",
    "bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "bpe_tokenizer.normalizer = normalizers.Lowercase()\n",
    "\n",
    "# Build an iterator over this dataset\n",
    "def batch_iterator():\n",
    "    batch_length = 1000\n",
    "    for i in range(0, len(dataset[\"train\"]), batch_length):\n",
    "        yield dataset[\"train\"][i : i + batch_length][\"p_clean\"]\n",
    "\n",
    "\n",
    "# And finally train\n",
    "bpe_tokenizer.train_from_iterator(batch_iterator(), length=len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d06013-d875-412e-a8eb-5d3929dcbe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_model(f\"{PATH_TOKENIZER_DIR}/1970/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd43fc-b284-4812-83d8-ac8ff3c901ce",
   "metadata": {},
   "source": [
    "### From text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea097e6-467f-470e-8e6f-189e20b98ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=3, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b59fac-66af-45ef-91f1-bec0daba05b1",
   "metadata": {},
   "source": [
    "## Pre trained tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f477dec7-fc7e-4245-867d-41b1bb01b31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4d0f5201b74c629a5790376b1fe64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=241441.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18075658071430db0308727e3b1a7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e219bca1a342e2a7282496fc1c8c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=254.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('GroNLP/bert-base-dutch-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ea71a09-dec0-4778-92e8-26a79a43425e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='GroNLP/bert-base-dutch-cased', vocab_size=30000, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c906c2-ec8d-455f-a0ec-4316bd523bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histaware-NidRwJ64-py3.8",
   "language": "python",
   "name": "histaware-nidrwj64-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
