{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f9d1d7",
   "metadata": {},
   "source": [
    "# DelphBERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e714ea-3814-48a2-a840-63c2242d88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338efff0-a72b-4d90-a533-e18e037f4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH_RAW_FILES = \"/home/leonardovida/data-histaware/raw/raw_merged/\"\n",
    "PATH_RAW_FILES = \"/home/leonardovida/data/volume_1/data-histaware/merged_articles/1970s\"\n",
    "#PATH_TOKENIZER_DIR = \"/home/leonardovida/dev/hist-aware/notebooks/models/bert-training-from-scratch/tokenizer\"\n",
    "PATH_TOKENIZER_DIR = \"/home/leonardovida/data/volume_1/data-histaware/tokenizer\"\n",
    "#PATH_DATASET_DIR = \"/home/leonardovida/dev/hist-aware/notebooks/models/bert-training-from-scratch/dataset\"\n",
    "PATH_DATASET_DIR = \"/home/leonardovida/data/volume_1/data-histaware/dataset\"\n",
    "\n",
    "dataset = load_from_disk(PATH_DATASET_DIR)\n",
    "#!mkdir {tokenizer_dir}\n",
    "# For tokenizer\n",
    "#PRE_TRAINED_MODEL_NAME = 'wietsedv/bert-base-dutch-cased'\n",
    "#tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#!mkdir PATH_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c9d87",
   "metadata": {},
   "source": [
    "## Create tokenizer from pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47ce4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When dataset is ready\n",
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"sentences\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264b46a",
   "metadata": {},
   "source": [
    "## Create tokenizer from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "176852f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-069a8c386c35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#bert_tokenizer.train(dataset, trainer=trainer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-a168f39c8a06>\u001b[0m in \u001b[0;36mbatch_iterator\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentences\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=52_000,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    min_frequency=2, \n",
    ")\n",
    "\n",
    "#bert_tokenizer.train(dataset, trainer=trainer)\n",
    "\n",
    "bert_tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe63d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.save(PATH_TOKENIZER_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a115e",
   "metadata": {},
   "source": [
    "## Create tokenizer using BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f986f",
   "metadata": {},
   "source": [
    "Using byte-level BPE makes it possible to learn a subword vocabulary of modest size that can encode any input without getting “unknown” tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94381b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_20.txt', '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_140.txt', '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_40.txt']\n",
      "CPU times: user 19min 45s, sys: 25.1 s, total: 20min 10s\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "paths = [str(x) for x in Path(PATH_RAW_FILES).glob(\"*.txt\")]\n",
    "print(f\"Found {len(print(paths))} text files from which a tokenizer will be trained\")\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths,\n",
    "                vocab_size=52_000,\n",
    "                min_frequency=2,\n",
    "                special_tokens=[\n",
    "                    \"<s>\",\n",
    "                    \"<pad>\",\n",
    "                    \"</s>\",\n",
    "                    \"<unk>\",\n",
    "                    \"<mask>\",\n",
    "                ])\n",
    "\n",
    "# Save just json\n",
    "tokenizer.save_model(tokenizer_dir)\n",
    "# Save both vocab and merges\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "#tokenizer.save(f\"{tokenizer_dir}/vocab.json\", f\"{tokenizer_dir}/merges.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b6dfd",
   "metadata": {},
   "source": [
    "### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ac75d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(f\"{tokenizer_dir}/delphbert.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histaware-NidRwJ64-py3.8",
   "language": "python",
   "name": "histaware-nidrwj64-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
