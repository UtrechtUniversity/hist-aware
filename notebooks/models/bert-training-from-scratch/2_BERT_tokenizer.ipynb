{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f9d1d7",
   "metadata": {},
   "source": [
    "# DelphBERT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e263302",
   "metadata": {},
   "source": [
    "### Train tokenizer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdaf77c",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "reloaded_encoded_dataset = load_from_disk(PATH_DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0498667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir {tokenizer_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c9d87",
   "metadata": {},
   "source": [
    "## Create tokenizer from pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce4292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When dataset is ready\n",
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264b46a",
   "metadata": {},
   "source": [
    "## Create tokenizer from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "176852f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe63d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=50010, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "bert_tokenizer.train(text_files, trainer)\n",
    "\n",
    "bert_tokenizer.save(f\"{tokenizer_dir}/delphbert.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a115e",
   "metadata": {},
   "source": [
    "## Create tokenizer using BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f986f",
   "metadata": {},
   "source": [
    "Using byte-level BPE makes it possible to learn a subword vocabulary of modest size that can encode any input without getting “unknown” tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94381b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_20.txt', '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_140.txt', '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_40.txt']\n",
      "CPU times: user 19min 45s, sys: 25.1 s, total: 20min 10s\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "paths = [str(x) for x in Path(PATH_RAW_FILES).glob(\"*.txt\")]\n",
    "print(f\"Found {len(print(paths))} text files from which a tokenizer will be trained\")\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths,\n",
    "                vocab_size=52_000,\n",
    "                min_frequency=2,\n",
    "                special_tokens=[\n",
    "                    \"<s>\",\n",
    "                    \"<pad>\",\n",
    "                    \"</s>\",\n",
    "                    \"<unk>\",\n",
    "                    \"<mask>\",\n",
    "                ])\n",
    "\n",
    "# Save just json\n",
    "tokenizer.save_model(tokenizer_dir)\n",
    "# Save both vocab and merges\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "#tokenizer.save(f\"{tokenizer_dir}/vocab.json\", f\"{tokenizer_dir}/merges.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b6dfd",
   "metadata": {},
   "source": [
    "### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ac75d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(f\"{tokenizer_dir}/delphbert.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
