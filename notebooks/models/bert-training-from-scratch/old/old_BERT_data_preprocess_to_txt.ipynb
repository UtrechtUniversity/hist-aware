{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "879ca4db",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a8f40",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a6a36",
   "metadata": {},
   "source": [
    "### Preprocess from .csv into .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d8d3971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en_US', 'en', 'en_AU', 'en_CA', 'en_GB']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enchant\n",
    "broker = enchant.Broker()\n",
    "broker.describe()\n",
    "broker.list_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9150e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import enchant\n",
    "import nltk\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        #self.d = enchant.Dict(\"nl_NL\")\n",
    "        self.stopword_list = nltk.corpus.stopwords.words(\"dutch\")\n",
    "        self.STOPWORDS = set(self.stopword_list)\n",
    "        \n",
    "    def get_words(self):\n",
    "        self.text = \" \".join([c for c in nltk.word_tokenize(self.text)])\n",
    "        return self\n",
    "\n",
    "    def lower(self):\n",
    "        \"\"\"Transform to lower case.\"\"\"\n",
    "        self.text = \"\".join([t.lower() for t in self.text])\n",
    "        return self\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        \"\"\"Remove the stopwords.\"\"\"\n",
    "        self.text = \"\".join([t for t in self.text if t not in self.STOPWORDS])\n",
    "        return self\n",
    "\n",
    "    def remove_numeric(self):\n",
    "        \"\"\"Remove numbers.\"\"\"\n",
    "        self.text = \"\".join([c for c in self.text if not c.isdigit()])\n",
    "        return self\n",
    "\n",
    "    def remove_non_ascii(self):\n",
    "        \"\"\"Remove non ASCII chars.\"\"\"\n",
    "        self.text = \"\".join([re.sub(r\"[^\\x00-\\x7f]\", r\" \", c) for c in self.text])\n",
    "        return self\n",
    "\n",
    "    def remove_extra_whitespace_tabs(self):\n",
    "        \"\"\"Remove extra whitespaces and tabs.\"\"\"\n",
    "        self.text = re.sub(r\"^\\s*|\\s\\s*\", \" \", self.text).strip()\n",
    "        return self\n",
    "\n",
    "    def remove_one_char(self):\n",
    "        self.text = \" \".join([w for w in self.text.split() if len(w) > 1])\n",
    "        return self\n",
    "\n",
    "    def remove_non_words(self):\n",
    "        \"\"\"Remove rare words.\"\"\"\n",
    "        self.text = \" \".join(\n",
    "            [word for word in str(self.text).split() if self.d.check(word)]\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def keep_standard_chars(self):\n",
    "        self.text = \"\".join([re.sub(r\"[^-0-9\\w,. ?!()%/]\", r\"\", c) for c in self.text])\n",
    "        return self\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        self.text = text\n",
    "        self = self.get_words()\n",
    "        self = self.lower()\n",
    "        self = self.remove_stopwords()\n",
    "        self = self.remove_numeric()\n",
    "        self = self.remove_extra_whitespace_tabs()\n",
    "        self = self.remove_one_char()\n",
    "        self = self.remove_non_words()\n",
    "        return self.text\n",
    "\n",
    "    def clean(self, text):\n",
    "        self.text = text\n",
    "        self = self.get_words()\n",
    "        self = self.keep_standard_chars()\n",
    "        self = self.remove_extra_whitespace_tabs()\n",
    "        return self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9aea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, tqdm_notebook\n",
    "from loguru import logger\n",
    "\n",
    "def process_selected_articles(path):\n",
    "    tqdm_notebook().pandas()\n",
    "    csv_temp = []\n",
    "    # Create preprocessing class\n",
    "    tc = TextCleaner()\n",
    "\n",
    "    # Load merged articles for selected topic in nlp_pipeline\n",
    "    df = pd.read_csv(path)\n",
    "         \n",
    "    # Initial clean\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(\n",
    "        columns={\n",
    "            \"index\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Split p into original paragraphs\n",
    "    logger.debug(\n",
    "        f\"Articles before selecting 'articles': {df.shape[0]}\"\n",
    "    )\n",
    "    df = df[df[\"subject\"] == \"artikel\"]\n",
    "    df[\"p\"] = df.apply(lambda row: repr(row[\"p\"]).split(\"\\\\',\"), axis=1)\n",
    "    logger.debug(\n",
    "        f\"Articles after selecting 'articles': {df.shape[0]}\"\n",
    "    )\n",
    "    df = df.explode(\"p\")\n",
    "    logger.debug(\n",
    "        f\"Articles after splitting into paragraphs: {df.shape[0]}\"\n",
    "    )\n",
    "\n",
    "    # Preprocess p to cleaner p for Tokenizer and transformers\n",
    "    res = df[\"p\"].progress_apply(tc.clean)\n",
    "\n",
    "    # Eliminate paragraphs that do not contain anything\n",
    "    res.dropna(inplace=True)\n",
    "    \n",
    "    # Save to .txt  \n",
    "    base = os.path.basename(path)\n",
    "    name = os.path.splitext(base)[0]\n",
    "    res.to_csv(f'/home/leonardovida/data-histaware/raw/raw_merged/{name}.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    \n",
    "    logger.debug(f\"Completed: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2702f59",
   "metadata": {},
   "source": [
    "### Convert from _.csv_ to _.txt_ - Do it just once though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e32064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_40.csv',\n",
       " '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_60.csv',\n",
       " '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_80.csv',\n",
       " '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_120.csv',\n",
       " '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_100.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find path to csv files with processed data\n",
    "#Path().parent.absolute()\n",
    "paths = [str(x) for x in Path(PATH_RAW_FILES).glob(\"*.csv\")]\n",
    "paths = paths[2:]\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb235d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7fd6f84a604e4d90588b0ff7ca4fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b906627d484d96917f53f250849f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-10 14:31:52.863 | DEBUG    | __main__:process_selected_articles:23 - Articles before selecting 'articles': 957008\n",
      "2021-04-10 14:32:03.730 | DEBUG    | __main__:process_selected_articles:28 - Articles after selecting 'articles': 567544\n",
      "2021-04-10 14:32:07.279 | DEBUG    | __main__:process_selected_articles:32 - Articles after splitting into paragraphs: 1225204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ab81f569854f26a5ae6176d4f6019e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1225204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create txt files for \"processed\" data\n",
    "for path in tqdm(paths, total=len(paths)):\n",
    "    process_selected_articles(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd14ed",
   "metadata": {},
   "source": [
    "### Load .txt files into one (to be changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716010fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = [str(x) for x in Path(PATH_RAW_FILES).glob(\"*.txt\")]\n",
    "df = pd.DataFrame()\n",
    "for file in text_files:\n",
    "    temp = pd.read_csv(f\"{PATH_RAW_FILES}/merged_1970s_20.txt\", delimiter = \"\\t\", header=None)\n",
    "    df = pd.concat([df, temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
