{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bc4ed4",
   "metadata": {},
   "source": [
    "# DelphBERT: Create pre-training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b79a8",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f57f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import load_from_disk, Sequence, Features, Value\n",
    "\n",
    "from transformers import (PreTrainedTokenizer, HfArgumentParser, AutoTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "897fe0f2-5700-433b-aee5-8e6db8024023",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_FILES = \"/home/leonardovida/data/volume_1/data-histaware/merged_articles/1970s\"\n",
    "PATH_TOKENIZER_DIR = \"/home/leonardovida/data/volume_1/data-histaware/tokenizer\"\n",
    "PATH_DATASET_DIR = \"/home/leonardovida/data/volume_1/data-histaware/dataset\"\n",
    "PATH_MODEL_DIR = \"/home/leonardovida/data/volume_1/data-histaware/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500ea683-d645-492e-a7de-b8504826645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(PATH_DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b8cd1f-e65f-4a02-891e-6a88818cdd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['p', 'recordIdentifier', 'sentences', 'sents_len', 'subject'])\n",
    "\n",
    "#train_dataset, test_dataset = dataset(split=['train', 'test'])\n",
    "#test_dataset = test_dataset.shuffle().select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec49954-b223-4ae4-a3bf-1c31a0ab7f1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-93e23eefbf2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcontents_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcontents_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcontents_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "a_file = open(\"/home/leonardovida/data/volume_1/data-histaware/dataset/data.1970.txt\")\n",
    "file_contents = a_file.read()\n",
    "contents_split = file_contents.splitlines()\n",
    "contents_split = contents_split[1:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ca3f0b3-fcbc-481b-adcc-b95d93a31f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "contents_split = pd.DataFrame(contents_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f99ca9d8-c8b6-4d1e-84b4-c96c1a39b0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Het pleidooi van de groep komt nadat onlangs een \"\"groep zigeuners, onder leiding van Koka Petaio, in geen enkele Rijnmondgemeente welkom bleek te zijn.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_split.loc[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8320f8b9-bcab-432e-8564-e66b94693afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"\"\"Het pleidooi van de groep komt nadat onlangs een \"\"\"\"groep zigeuners, onder leiding van Koka Petaio, in geen enkele Rijnmondgemeente welkom bleek te zijn.\"\"\"\\n', 'Er moet een gemeente bereid worden gevonden, bijvoorbeeld Rotterdam, om een stuk grond een tijdelijke bestemming te geven als zigeuenerkampement. Dat Rijnmond deze bestemming voorlopig in het streekplan opnemen en GS van ZuidHolland dit goed7 keuren..\\n', \".' .'' . 718.00 IX draaischijf, 18.35 Plezier met Charlie Chaplin. 19.10 Plaatselijke tijd, 19.45 Journaal. .  ..'.'20.15 EEN GEVAL VOOR GORON 21.40 TOT IN HET LAATSTE DORP 22.25 DAGBOEK R.K. KERK 22.40 JOURNAAL 23.00 APROPOS FILM Actualiteiten uil de filmbranch 23.45 JOURNAAL SHBWJ\\n\", \"APOTHEKEN Voor spoedgevallen telefoon 166845. .' TANDARTSEN L.. A. v. d. Berg, Kastanjestrafet 1012, Spijkenisse (van 17 17.30 uur).\\n\", \"Twee ministers uit het kabinetDen Uyl hebben met wisselend succes hun eerste confrontatie met de NAVO achter de rug Vredeling (defensie) in Brussel en Van der. Stoel (buitenlandse zaken) in Kopenhagen. Binnen hun PvdA, veruit de grootste regeringspartij, is aan 'de hand van het discussiestuk Vrede en Veiligheid een nieuwe. gedachtenwisseling over de NAVO op gang gebracht die tot in het voorjaar van '74 zal\\n\", \"De vraag lijkt gewettigd wat de op n na kleinste regerringspartij de PPR, op het punt van de NAVO aan initiatieven heeft ontwikkeld. Al was het alleen maar vanwege de vele compromissen die de radicalen juist op dit stuk buitenlands (defensie)beleid hebben moeten slikken tijdens de geboorte van het progressieve regeerakkoord Keerpunt '72.  f i\\n\", \"Het antwoord op die vraag geeft dr. H. J. (Henk) Waltmans, sinds de verkiezingen van eind november '72 Tweede Kamerlid voor de PPR, voordien directeur van het Europahuis in Neerlands kleinste gemeente, het Zuidlimburgse Bemelen. Henk Waltmans, die samen met fractieleider Bas de Gaay Fortman het.buitenlands beleid voor zijn rekening neemt,, heeft sinds die gedenkwaardige verkiezingsoverwinning van de PPR. niet stilgezeten.\\n\", \"Reeds in het vroege voorjaar van '73 ging hij op pad om een belofte aan het PPRverkiezingscongres in Wageningen in te lossen door te trachten ook andere radicale groeperingen in Europa warm te maken voor de vredesinitiatieven van de PPR. Zijn Europese toernee bracht hem tot nu toe in Oostenrijk, Duitsland, BelgiDenemarken en Zweden. Bezoeken aan Noorwegen en lerland staan nog op het programma. Fractieleider De Gaay Fortman zal in GrootBritanni op onderzoek uitgaan.\\n\", '\"\"\"Dr. H. J Waltmans ..\"\"\"\"eruit geen oplossing\"\"\"\". vV, \\'..\\'v\\'r\"\"\"\\n']\n"
     ]
    }
   ],
   "source": [
    "contents_split.to_csv(\"/home/leonardovida/data/volume_1/data-histaware/dataset/data.1970_test.txt\", header=None, index=None, sep='\\n', mode='a')\n",
    "with open(\"/home/leonardovida/data/volume_1/data-histaware/dataset/data.1970_test.txt\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f19c133c-f8a8-4fd6-b813-f2be0439204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Het pleidooi van de groep komt nadat onlangs een \"\"\"\"groep zigeuners, onder leiding van Koka Petaio, in geen enkele Rijnmondgemeente welkom bleek te zijn.\"\"\"\n",
      "\n",
      "Er moet een gemeente bereid worden gevonden, bijvoorbeeld Rotterdam, om een stuk grond een tijdelijke bestemming te geven als zigeuenerkampement. Dat Rijnmond deze bestemming voorlopig in het streekplan opnemen en GS van ZuidHolland dit goed7 keuren..\n",
      "\n",
      ".' .'' . 718.00 IX draaischijf, 18.35 Plezier met Charlie Chaplin. 19.10 Plaatselijke tijd, 19.45 Journaal. .  ..'.'20.15 EEN GEVAL VOOR GORON 21.40 TOT IN HET LAATSTE DORP 22.25 DAGBOEK R.K. KERK 22.40 JOURNAAL 23.00 APROPOS FILM Actualiteiten uil de filmbranch 23.45 JOURNAAL SHBWJ\n",
      "\n",
      "APOTHEKEN Voor spoedgevallen telefoon 166845. .' TANDARTSEN L.. A. v. d. Berg, Kastanjestrafet 1012, Spijkenisse (van 17 17.30 uur).\n",
      "\n",
      "Twee ministers uit het kabinetDen Uyl hebben met wisselend succes hun eerste confrontatie met de NAVO achter de rug Vredeling (defensie) in Brussel en Van der. Stoel (buitenlandse zaken) in Kopenhagen. Binnen hun PvdA, veruit de grootste regeringspartij, is aan 'de hand van het discussiestuk Vrede en Veiligheid een nieuwe. gedachtenwisseling over de NAVO op gang gebracht die tot in het voorjaar van '74 zal\n",
      "\n",
      "De vraag lijkt gewettigd wat de op n na kleinste regerringspartij de PPR, op het punt van de NAVO aan initiatieven heeft ontwikkeld. Al was het alleen maar vanwege de vele compromissen die de radicalen juist op dit stuk buitenlands (defensie)beleid hebben moeten slikken tijdens de geboorte van het progressieve regeerakkoord Keerpunt '72.  f i\n",
      "\n",
      "Het antwoord op die vraag geeft dr. H. J. (Henk) Waltmans, sinds de verkiezingen van eind november '72 Tweede Kamerlid voor de PPR, voordien directeur van het Europahuis in Neerlands kleinste gemeente, het Zuidlimburgse Bemelen. Henk Waltmans, die samen met fractieleider Bas de Gaay Fortman het.buitenlands beleid voor zijn rekening neemt,, heeft sinds die gedenkwaardige verkiezingsoverwinning van de PPR. niet stilgezeten.\n",
      "\n",
      "Reeds in het vroege voorjaar van '73 ging hij op pad om een belofte aan het PPRverkiezingscongres in Wageningen in te lossen door te trachten ook andere radicale groeperingen in Europa warm te maken voor de vredesinitiatieven van de PPR. Zijn Europese toernee bracht hem tot nu toe in Oostenrijk, Duitsland, BelgiDenemarken en Zweden. Bezoeken aan Noorwegen en lerland staan nog op het programma. Fractieleider De Gaay Fortman zal in GrootBritanni op onderzoek uitgaan.\n",
      "\n",
      "\"\"\"Dr. H. J Waltmans ..\"\"\"\"eruit geen oplossing\"\"\"\". vV, '..'v'r\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in range(1, 10, 1):\n",
    "    print(lines[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6f581-315a-406a-99ab-12fb2355b1ac",
   "metadata": {},
   "source": [
    "Remove not useful columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a360a0-30e4-498b-ac5e-2c7d1f76b261",
   "metadata": {},
   "source": [
    "## HuggingFace way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f03fa144-eaa4-46a7-a76a-42d9204f779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    f\"{PATH_TOKENIZER_DIR}/1970/vocab.json\",\n",
    "    f\"{PATH_TOKENIZER_DIR}/1970/merges.txt\",\n",
    ")\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(f\"{PATH_TOKENIZER_DIR}/1970\", max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a6a2e-a18a-436c-9392-46b85a3197ac",
   "metadata": {},
   "source": [
    "## MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df3c82d7-b10d-4d15-850a-633632ddf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "                          CONFIG_MAPPING,\n",
    "                          MODEL_FOR_MASKED_LM_MAPPING,\n",
    "                          MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "                          PreTrainedTokenizer,\n",
    "                          TrainingArguments,\n",
    "                          AutoConfig,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelWithLMHead,\n",
    "                          AutoModelForCausalLM,\n",
    "                          AutoModelForMaskedLM,\n",
    "                          LineByLineTextDataset,\n",
    "                          TextDataset,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForWholeWordMask,\n",
    "                          DataCollatorForPermutationLanguageModeling,\n",
    "                          PretrainedConfig,\n",
    "                          Trainer,\n",
    "                          set_seed,\n",
    "                          )\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version\n",
    "\n",
    "# Set seed for reproducibility,\n",
    "set_seed(123)\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc7e7d17-d5c9-427e-aeb4-b82cd9a9c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDataArguments(object):\n",
    "    \"\"\"Define model and data configuration needed to perform pretraining.\n",
    "\n",
    "    Eve though all arguments are optional there still needs to be a certain \n",
    "    number of arguments that require values attributed.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    train_data_file (:obj:`str`, `optional`): \n",
    "      Path to your .txt file dataset. If you have an example on each line of \n",
    "      the file make sure to use line_by_line=True. If the data file contains \n",
    "      all text data without any special grouping use line_by_line=False to move \n",
    "      a block_size window across the text file.\n",
    "      This argument is optional and it will have a `None` value attributed \n",
    "      inside the function.\n",
    "\n",
    "    eval_data_file (:obj:`str`, `optional`): \n",
    "      Path to evaluation .txt file. It has the same format as train_data_file.\n",
    "      This argument is optional and it will have a `None` value attributed \n",
    "      inside the function.\n",
    "\n",
    "    line_by_line (:obj:`bool`, `optional`, defaults to :obj:`False`): \n",
    "      If the train_data_file and eval_data_file contains separate examples on \n",
    "      each line then line_by_line=True. If there is no separation between \n",
    "      examples and train_data_file and eval_data_file contains continuous text \n",
    "      then line_by_line=False and a window of block_size will be moved across \n",
    "      the files to acquire examples.\n",
    "      This argument is optional and it has a default value.\n",
    "\n",
    "    mlm (:obj:`bool`, `optional`, defaults to :obj:`False`): \n",
    "      Is a flag that changes loss function depending on model architecture. \n",
    "      This variable needs to be set to True when working with masked language \n",
    "      models like bert or roberta and set to False otherwise. There are \n",
    "      functions that will raise ValueError if this argument is \n",
    "      not set accordingly.\n",
    "      This argument is optional and it has a default value.\n",
    "\n",
    "    whole_word_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "      Used as flag to determine if we decide to use whole word masking or not. \n",
    "      Whole word masking means that whole words will be masked during training \n",
    "      instead of tokens which can be chunks of words.\n",
    "      This argument is optional and it has a default value.\n",
    "\n",
    "    mlm_probability(:obj:`float`, `optional`, defaults to :obj:`0.15`): \n",
    "      Used when training masked language models. Needs to have mlm set to True. \n",
    "      It represents the probability of masking tokens when training model.\n",
    "      This argument is optional and it has a default value.\n",
    "\n",
    "    plm_probability (:obj:`float`, `optional`, defaults to :obj:`float(1/6)`): \n",
    "      Flag to define the ratio of length of a span of masked tokens to \n",
    "      surrounding context length for permutation language modeling. \n",
    "      Used for XLNet.\n",
    "      This argument is optional and it has a default value.\n",
    "\n",
    "    max_span_length (:obj:`int`, `optional`, defaults to :obj:`5`): \n",
    "      Flag may also be used to limit the length of a span of masked tokens used \n",
    "      for permutation language modeling. Used for XLNet.\n",
    "      This argument is optional and it has a default value.\n",
    "\n",
    "    block_size (:obj:`int`, `optional`, defaults to :obj:`-1`): \n",
    "      It refers to the windows size that is moved across the text file. \n",
    "      Set to -1 to use maximum allowed length.\n",
    "      This argument is optional and it has a default value.\n",
    "\n",
    "    overwrite_cache (:obj:`bool`, `optional`, defaults to :obj:`False`): \n",
    "      If there are any cached files, overwrite them.\n",
    "      This argument is optional and it has a default value.\n",
    "\n",
    "    model_type (:obj:`str`, `optional`): \n",
    "      Type of model used: bert, roberta, gpt2. \n",
    "      More details: https://huggingface.co/transformers/pretrained_models.html\n",
    "      This argument is optional and it will have a `None` value attributed \n",
    "      inside the function.\n",
    "\n",
    "    model_config_name (:obj:`str`, `optional`):\n",
    "      Config of model used: bert, roberta, gpt2. \n",
    "      More details: https://huggingface.co/transformers/pretrained_models.html\n",
    "      This argument is optional and it will have a `None` value attributed \n",
    "      inside the function.\n",
    "\n",
    "    tokenizer_name: (:obj:`str`, `optional`)\n",
    "      Tokenizer used to process data for training the model. \n",
    "      It usually has same name as model_name_or_path: bert-base-cased, \n",
    "      roberta-base, gpt2 etc.\n",
    "      This argument is optional and it will have a `None` value attributed \n",
    "      inside the function.\n",
    "\n",
    "    model_name_or_path (:obj:`str`, `optional`): \n",
    "      Path to existing transformers model or name of \n",
    "      transformer model to be used: bert-base-cased, roberta-base, gpt2 etc. \n",
    "      More details: https://huggingface.co/transformers/pretrained_models.html\n",
    "      This argument is optional and it will have a `None` value attributed \n",
    "      inside the function.\n",
    "\n",
    "    model_cache_dir (:obj:`str`, `optional`): \n",
    "      Path to cache files to save time when re-running code.\n",
    "      This argument is optional and it will have a `None` value attributed \n",
    "      inside the function.\n",
    "\n",
    "    Raises:\n",
    "\n",
    "        ValueError: If `CONFIG_MAPPING` is not loaded in global variables.\n",
    "\n",
    "        ValueError: If `model_type` is not present in `CONFIG_MAPPING.keys()`.\n",
    "\n",
    "        ValueError: If `model_type`, `model_config_name` and \n",
    "          `model_name_or_path` variables are all `None`. At least one of them \n",
    "          needs to be set.\n",
    "\n",
    "        warnings: If `model_config_name` and `model_name_or_path` are both \n",
    "          `None`, the model will be trained from scratch.\n",
    "\n",
    "        ValueError: If `tokenizer_name` and `model_name_or_path` are both \n",
    "          `None`. We need at least one of them set to load tokenizer.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, train_data_file=None, eval_data_file=None, \n",
    "               line_by_line=False, mlm=False, mlm_probability=0.15, \n",
    "               whole_word_mask=False, plm_probability=float(1/6), \n",
    "               max_span_length=5, block_size=-1, overwrite_cache=False, \n",
    "               model_type=None, model_config_name=None, tokenizer_name=None, \n",
    "               model_name_or_path=None, model_cache_dir=None):\n",
    "     \n",
    "    # Make sure CONFIG_MAPPING is imported from transformers module.\n",
    "        if 'CONFIG_MAPPING' not in globals():\n",
    "            raise ValueError('Could not find `CONFIG_MAPPING` imported! Make sure' \\\n",
    "                           ' to import it from `transformers` module!')\n",
    "\n",
    "        # Make sure model_type is valid.\n",
    "        if (model_type is not None) and (model_type not in CONFIG_MAPPING.keys()):\n",
    "            raise ValueError('Invalid `model_type`! Use one of the following: %s' %\n",
    "                           (str(list(CONFIG_MAPPING.keys()))))\n",
    "\n",
    "        # Make sure that model_type, model_config_name and model_name_or_path \n",
    "        # variables are not all `None`.\n",
    "        if not any([model_type, model_config_name, model_name_or_path]):\n",
    "            raise ValueError('You can`t have all `model_type`, `model_config_name`,' \\\n",
    "                           ' `model_name_or_path` be `None`! You need to have' \\\n",
    "                           'at least one of them set!')\n",
    "\n",
    "        # Check if a new model will be loaded from scratch.\n",
    "        if not any([model_config_name, model_name_or_path]):\n",
    "          # Setup warning to show pretty. This is an overkill\n",
    "          warnings.formatwarning = lambda message,category,*args,**kwargs: \\\n",
    "                                   '%s: %s\\n' % (category.__name__, message)\n",
    "          # Display warning.\n",
    "          warnings.warn('You are planning to train a model from scratch! 🙀')\n",
    "\n",
    "        # Check if a new tokenizer wants to be loaded.\n",
    "        # This feature is not supported!\n",
    "        if not any([tokenizer_name, model_name_or_path]):\n",
    "          # Can't train tokenizer from scratch here! Raise error.\n",
    "          raise ValueError('You want to train tokenizer from scratch! ' \\\n",
    "                        'That is not possible yet! You can train your own ' \\\n",
    "                        'tokenizer separately and use path here to load it!')\n",
    "\n",
    "        # Set all data related arguments.\n",
    "        self.train_data_file = train_data_file\n",
    "        self.eval_data_file = eval_data_file\n",
    "        self.line_by_line = line_by_line\n",
    "        self.mlm = mlm\n",
    "        self.whole_word_mask = whole_word_mask\n",
    "        self.mlm_probability = mlm_probability\n",
    "        self.plm_probability = plm_probability\n",
    "        self.max_span_length = max_span_length\n",
    "        self.block_size = block_size\n",
    "        self.overwrite_cache = overwrite_cache\n",
    "\n",
    "        # Set all model and tokenizer arguments.\n",
    "        self.model_type = model_type\n",
    "        self.model_config_name = model_config_name\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.model_cache_dir = model_cache_dir\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c24a0688-2f3f-45cf-b8d8-d7a320de2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_config(args: ModelDataArguments):\n",
    "    \"\"\"\n",
    "    Get model configuration.\n",
    "\n",
    "    Using the ModelDataArguments return the model configuration.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    args (:obj:`ModelDataArguments`):\n",
    "      Model and data configuration arguments needed to perform pretraining.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    :obj:`PretrainedConfig`: Model transformers configuration.\n",
    "\n",
    "    Raises:\n",
    "\n",
    "    ValueError: If `mlm=True` and `model_type` is NOT in [\"bert\", \n",
    "          \"roberta\", \"distilbert\", \"camembert\"]. We need to use a masked \n",
    "          language model in order to set `mlm=True`.\n",
    "    \"\"\"\n",
    " \n",
    "    # Check model configuration.\n",
    "    if args.model_config_name is not None:\n",
    "        # Use model configure name if defined.\n",
    "        model_config = AutoConfig.from_pretrained(args.model_config_name, \n",
    "                                                  cache_dir=args.model_cache_dir)\n",
    "\n",
    "    elif args.model_name_or_path is not None:\n",
    "        # Use model name or path if defined.\n",
    "        model_config = AutoConfig.from_pretrained(args.model_name_or_path, \n",
    "                                                  cache_dir=args.model_cache_dir)\n",
    "\n",
    "    else:\n",
    "    # Use config mapping if building model from scratch.\n",
    "        model_config = CONFIG_MAPPING[args.model_type]()\n",
    "\n",
    "    # Make sure `mlm` flag is set for Masked Language Models (MLM).\n",
    "    if (model_config.model_type in [\"bert\", \"roberta\", \"distilbert\", \n",
    "                                  \"camembert\"]) and (args.mlm is False):\n",
    "        raise ValueError('BERT and RoBERTa-like models do not have LM heads')\n",
    "\n",
    "    # Adjust block size for xlnet.\n",
    "    if model_config.model_type == \"xlnet\":\n",
    "    # xlnet used 512 tokens when training.\n",
    "        args.block_size = 512\n",
    "        # setup memory length\n",
    "        model_config.mem_len = 1024\n",
    "\n",
    "    return model_config\n",
    "\n",
    "def get_model_config(args: ModelDataArguments):\n",
    "    \"\"\"Get model configuration.\n",
    "\n",
    "    Using the ModelDataArguments return the model configuration.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    args (:obj:`ModelDataArguments`):\n",
    "      Model and data configuration arguments needed to perform pretraining.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    :obj:`PretrainedConfig`: Model transformers configuration.\n",
    "\n",
    "    Raises:\n",
    "\n",
    "    ValueError: If `mlm=True` and `model_type` is NOT in [\"bert\", \n",
    "          \"roberta\", \"distilbert\", \"camembert\"]. We need to use a masked \n",
    "          language model in order to set `mlm=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check model configuration.\n",
    "    if args.model_config_name is not None:\n",
    "    # Use model configure name if defined.\n",
    "        model_config = AutoConfig.from_pretrained(args.model_config_name, \n",
    "                                          cache_dir=args.model_cache_dir)\n",
    "\n",
    "    elif args.model_name_or_path is not None:\n",
    "    # Use model name or path if defined.\n",
    "        model_config = AutoConfig.from_pretrained(args.model_name_or_path, \n",
    "                                          cache_dir=args.model_cache_dir)\n",
    "\n",
    "    else:\n",
    "    # Use config mapping if building model from scratch.\n",
    "        model_config = CONFIG_MAPPING[args.model_type]()\n",
    "\n",
    "    # Make sure `mlm` flag is set for Masked Language Models (MLM).\n",
    "    if (model_config.model_type in [\"bert\", \"roberta\", \"distilbert\", \n",
    "                                  \"camembert\"]) and (args.mlm is False):\n",
    "        raise ValueError('BERT and RoBERTa-like models do not have LM heads')\n",
    "\n",
    "    # Adjust block size for xlnet.\n",
    "    if model_config.model_type == \"xlnet\":\n",
    "    # xlnet used 512 tokens when training.\n",
    "        args.block_size = 512\n",
    "        # setup memory length\n",
    "        model_config.mem_len = 1024\n",
    "\n",
    "    return model_config\n",
    "\n",
    "\n",
    "def get_tokenizer(args: ModelDataArguments):\n",
    "    \"\"\"\n",
    "    Get model tokenizer.\n",
    "\n",
    "    Using the ModelDataArguments return the model tokenizer and change \n",
    "    `block_size` form `args` if needed.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    args (:obj:`ModelDataArguments`):\n",
    "      Model and data configuration arguments needed to perform pretraining.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    :obj:`PreTrainedTokenizer`: Model transformers tokenizer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Check tokenizer configuration.\n",
    "    if args.tokenizer_name:\n",
    "    # Use tokenizer name if define.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, \n",
    "                                                  cache_dir=args.model_cache_dir)\n",
    "\n",
    "    elif args.model_name_or_path:\n",
    "    # Use tokenizer name of path if defined.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, \n",
    "                                                  cache_dir=args.model_cache_dir)\n",
    "\n",
    "    # Setp data block size.\n",
    "    if args.block_size <= 0:\n",
    "    # Set block size to maximum length of tokenizer.\n",
    "    # Input block size will be the max possible for the model.\n",
    "    # Some max lengths are very large and will cause a\n",
    "        args.block_size = tokenizer.model_max_length\n",
    "    else:\n",
    "    # Never go beyond tokenizer maximum length.\n",
    "        args.block_size = min(args.block_size, tokenizer.model_max_length)\n",
    "\n",
    "    return tokenizer\n",
    "  \n",
    "\n",
    "def get_model(args: ModelDataArguments, model_config):\n",
    "    \"\"\"\n",
    "    Get model.\n",
    "\n",
    "    Using the ModelDataArguments return the actual model.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    args (:obj:`ModelDataArguments`):\n",
    "      Model and data configuration arguments needed to perform pretraining.\n",
    "\n",
    "    model_config (:obj:`PretrainedConfig`):\n",
    "      Model transformers configuration.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    :obj:`torch.nn.Module`: PyTorch model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure MODEL_FOR_MASKED_LM_MAPPING and MODEL_FOR_CAUSAL_LM_MAPPING are \n",
    "    # imported from transformers module.\n",
    "    if ('MODEL_FOR_MASKED_LM_MAPPING' not in globals()) and \\\n",
    "        ('MODEL_FOR_CAUSAL_LM_MAPPING' not in globals()):\n",
    "        raise ValueError('Could not find `MODEL_FOR_MASKED_LM_MAPPING`')\n",
    "\n",
    "    # Check if using pre-trained model or train from scratch.\n",
    "    if args.model_name_or_path:\n",
    "    # Use pre-trained model.\n",
    "        if type(model_config) in MODEL_FOR_MASKED_LM_MAPPING.keys():\n",
    "            return AutoModelForMaskedLM.from_pretrained(\n",
    "                            args.model_name_or_path,\n",
    "                            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "                            config=model_config,\n",
    "                            cache_dir=args.model_cache_dir,\n",
    "                            )\n",
    "        elif type(model_config) in MODEL_FOR_CAUSAL_LM_MAPPING.keys():\n",
    "            # Causal language modeling head.\n",
    "            return AutoModelForCausalLM.from_pretrained(\n",
    "                                              args.model_name_or_path, \n",
    "                                              from_tf=bool(\".ckpt\" in \n",
    "                                                            args.model_name_or_path),\n",
    "                                              config=model_config, \n",
    "                                              cache_dir=args.model_cache_dir)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Invalid `model_name_or_path`! It should be in %s or %s!' % \n",
    "                (str(MODEL_FOR_MASKED_LM_MAPPING.keys()), \n",
    "                 str(MODEL_FOR_CAUSAL_LM_MAPPING.keys())))\n",
    "    else:\n",
    "        # Use model from configuration - train from scratch.\n",
    "        print(\"Training new model from scratch!\")\n",
    "        return AutoModelWithLMHead.from_config(config)\n",
    "\n",
    "\n",
    "def get_dataset(dataset: Dataset, text_column_name: str, tokenizer: PreTrainedTokenizer, \n",
    "                evaluate: bool=False):\n",
    "    \"\"\"\n",
    "    Process dataset file into PyTorch Dataset.\n",
    "\n",
    "    Using the ModelDataArguments return the actual model.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    args (:obj:`ModelDataArguments`):\n",
    "    Model and data configuration arguments needed to perform pretraining.\n",
    "\n",
    "    tokenizer (:obj:`PreTrainedTokenizer`):\n",
    "    Model transformers tokenizer.\n",
    "\n",
    "    evaluate (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "    If set to `True` the test / validation file is being handled.\n",
    "    If set to `False` the train file is being handled.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    :obj:`Dataset`: PyTorch Dataset that contains file's data.\n",
    "\n",
    "    \"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
    "\n",
    "    #Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
    "    # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n",
    "    # efficient when it receives the `special_tokens_mask`.\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=9\n",
    "    )\n",
    "\n",
    "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "    # max_seq_length.\n",
    "    #def group_texts(examples):\n",
    "    #    # Concatenate all texts.\n",
    "    #    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    #    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    #    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    #    # customize this part to your needs.\n",
    "    #    total_length = (total_length // max_seq_length) * max_seq_length\n",
    "    #    # Split by chunks of max_len.\n",
    "    #    result = {\n",
    "    #        k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "    #        for k, t in concatenated_examples.items()\n",
    "    #    }\n",
    "    #    return result\n",
    "\n",
    "    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "    # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "    # might be slower to preprocess.\n",
    "    #\n",
    "    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "    tokenized_dataset = tokenized_dataset.map(\n",
    "        #group_texts,\n",
    "        #batched=True,\n",
    "        num_proc=9,\n",
    "    )\n",
    "\n",
    "    train_dataset = tokenized_dataset[\"train\"]\n",
    "\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def get_collator(args: ModelDataArguments, model_config: PretrainedConfig, \n",
    "                 tokenizer: PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "    Get appropriate collator function.\n",
    "\n",
    "    Collator function will be used to collate a PyTorch Dataset object.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    args (:obj:`ModelDataArguments`):\n",
    "      Model and data configuration arguments needed to perform pretraining.\n",
    "\n",
    "    model_config (:obj:`PretrainedConfig`):\n",
    "      Model transformers configuration.\n",
    "\n",
    "    tokenizer (:obj:`PreTrainedTokenizer`):\n",
    "      Model transformers tokenizer.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    :obj:`data_collator`: Transformers specific data collator.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Configure data for rest of model types.\n",
    "    if args.mlm and args.whole_word_mask:\n",
    "      # Use whole word masking.\n",
    "      return DataCollatorForWholeWordMask(\n",
    "                                          tokenizer=tokenizer, \n",
    "                                          mlm_probability=args.mlm_probability,\n",
    "                                          )\n",
    "    else:\n",
    "        return DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, \n",
    "            mlm=args.mlm, \n",
    "            mlm_probability=args.mlm_probability,\n",
    "            pad_to_multiple_of=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c53affba-8903-4caa-8804-92e91c538113",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_FILES = \"/home/leonardovida/data/volume_1/data-histaware/merged_articles/1970s\"\n",
    "PATH_TOKENIZER_DIR = \"/home/leonardovida/data/volume_1/data-histaware/tokenizer\"\n",
    "PATH_DATASET_DIR = \"/home/leonardovida/data/volume_1/data-histaware/dataset\"\n",
    "PATH_MODEL_DIR = \"/home/leonardovida/data/volume_1/data-histaware/model\"\n",
    "\n",
    "# See comments in `ModelDataArguments` class.\n",
    "model_data_args = ModelDataArguments(\n",
    "                                    train_data_file=f'{PATH_DATASET_DIR}/data.1970.txt', \n",
    "                                    #eval_data_file='/content/test.txt', \n",
    "                                    line_by_line=True, \n",
    "                                    mlm=True,\n",
    "                                    whole_word_mask=True,\n",
    "                                    mlm_probability=0.15,\n",
    "                                    plm_probability=float(1/6), \n",
    "                                    max_span_length=5,\n",
    "                                    block_size=50, \n",
    "                                    overwrite_cache=False, \n",
    "                                    model_type='bert', \n",
    "                                    model_config_name='bert-base-cased', \n",
    "                                    tokenizer_name='bert-base-cased', \n",
    "                                    model_name_or_path='bert-base-cased', \n",
    "                                    model_cache_dir='/home/leonardovida/data/volume_1/huggingface_cache/',\n",
    ")\n",
    "\n",
    "# Define arguments for training\n",
    "# Note: I only used the arguments I care about. `TrainingArguments` contains\n",
    "# a lot more arguments. For more details check the awesome documentation:\n",
    "# https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments\n",
    "training_args = TrainingArguments(\n",
    "                          # The output directory where the model predictions \n",
    "                          # and checkpoints will be written.\n",
    "                          output_dir=PATH_MODEL_DIR,\n",
    "\n",
    "                          # Overwrite the content of the output directory.\n",
    "                          overwrite_output_dir=True,\n",
    "\n",
    "                          # Whether to run training or not.\n",
    "                          do_train=True, \n",
    "                          \n",
    "                          # Whether to run evaluation on the dev or not.\n",
    "                          do_eval=False,\n",
    "                          \n",
    "                          # Batch size GPU/TPU core/CPU training.\n",
    "                          per_device_train_batch_size=4,\n",
    "                          \n",
    "                          # Batch size  GPU/TPU core/CPU for evaluation.\n",
    "                          per_device_eval_batch_size=100,\n",
    "\n",
    "                          # evaluation strategy to adopt during training\n",
    "                          # `no`: No evaluation during training.\n",
    "                          # `steps`: Evaluate every `eval_steps`.\n",
    "                          # `epoch`: Evaluate every end of epoch.\n",
    "                          evaluation_strategy='steps',\n",
    "\n",
    "                          # How often to show logs. I will se this to \n",
    "                          # plot history loss and calculate perplexity.\n",
    "                          logging_steps=700,\n",
    "\n",
    "                          # Number of update steps between two \n",
    "                          # evaluations if evaluation_strategy=\"steps\".\n",
    "                          # Will default to the same value as l\n",
    "                          # logging_steps if not set.\n",
    "                          eval_steps = None,\n",
    "                          \n",
    "                          # Set prediction loss to `True` in order to \n",
    "                          # return loss for perplexity calculation.\n",
    "                          prediction_loss_only=True,\n",
    "\n",
    "                          # The initial learning rate for Adam. \n",
    "                          # Defaults to 5e-5.\n",
    "                          learning_rate = 5e-5,\n",
    "\n",
    "                          # The weight decay to apply (if not zero).\n",
    "                          weight_decay=0,\n",
    "\n",
    "                          # Epsilon for the Adam optimizer. \n",
    "                          # Defaults to 1e-8\n",
    "                          adam_epsilon = 1e-8,\n",
    "\n",
    "                          # Maximum gradient norm (for gradient \n",
    "                          # clipping). Defaults to 0.\n",
    "                          max_grad_norm = 1.0,\n",
    "                          # Total number of training epochs to perform \n",
    "                          # (if not an integer, will perform the \n",
    "                          # decimal part percents of\n",
    "                          # the last epoch before stopping training).\n",
    "                          num_train_epochs = 2,\n",
    "\n",
    "                          # Number of updates steps before two checkpoint saves. \n",
    "                          # Defaults to 500\n",
    "                          save_steps = -1,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e88db93b-024c-471b-b09b-7b2022c49479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model configuration...\n",
      "Loading model`s tokenizer...\n",
      "Loading actual model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(28996, 768, padding_idx=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model configuration.\n",
    "print('Loading model configuration...')\n",
    "config = get_model_config(model_data_args)\n",
    "\n",
    "# Load model tokenizer.\n",
    "print('Loading model`s tokenizer...')\n",
    "tokenizer = get_tokenizer(model_data_args)\n",
    "\n",
    "# Loading model.\n",
    "print('Loading actual model...')\n",
    "model = get_model(model_data_args, config)\n",
    "\n",
    "# Resize model to fit all tokens in tokenizer.\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc23579-b326-4f25-bdef-f933286d9021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad966f5ea494ec68cf19ac1695aafd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#8', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455310210071430d9ea5c88610048157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#3', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811e670777e942d28304e5312a89696d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#7', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230ac6a49bda4882b3c83f0bb9040563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#1', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b3470bb11b4f1c8b4b7083c6c60d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#0', max=465135.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"p_clean\"],\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     max_length=200,\n",
    "                     return_special_tokens_mask=True)\n",
    "\n",
    "dataset = dataset.map(tokenize_function, num_proc=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a13194-d73e-4258-bdc1-10f7bed822e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup train dataset if `do_train` is set.\n",
    "print('Creating train dataset...')\n",
    "#train_dataset = get_dataset(\n",
    "#    dataset = dataset,\n",
    "#    text_column_name = \"p_clean\",\n",
    "#    #ModelDataArguments = model_data_args,\n",
    "#    tokenizer = tokenizer, \n",
    "#    evaluate = False) if training_args.do_train else None\n",
    "\n",
    "# Setup evaluation dataset if `do_eval` is set.\n",
    "#print('Creating evaluate dataset...')\n",
    "#eval_dataset = get_dataset(model_data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None\n",
    "\n",
    "# Get data collator to modify data format depending on type of model used.\n",
    "data_collator = get_collator(model_data_args, config, tokenizer)\n",
    "\n",
    "# Check how many logging prints you'll have. This is to avoid overflowing the \n",
    "# notebook with a lot of prints. Display warning to user if the logging steps \n",
    "# that will be displayed is larger than 100.\n",
    "if (len(train_dataset) // training_args.per_device_train_batch_size \\\n",
    "    // training_args.logging_steps * training_args.num_train_epochs) > 100:\n",
    "  # Display warning.\n",
    "  warnings.warn('Your `logging_steps` value will will do a lot of printing!' \\\n",
    "                ' Consider increasing `logging_steps` to avoid overflowing' \\\n",
    "                ' the notebook with a lot of prints!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf700652-70c9-4fbc-bdee-7dbcb8ecb5b7",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37111ab1-1f6f-4dfe-8038-6c2043881ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer.\n",
    "print('Loading `trainer`...')\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  #eval_dataset=eval_dataset,\n",
    "                  )\n",
    "\n",
    "\n",
    "# Check model path to save.\n",
    "if training_args.do_train:\n",
    "    print('Start training...')\n",
    "\n",
    "    # Setup model path if the model to train loaded from a local path.\n",
    "    model_path = (model_data_args.model_name_or_path \n",
    "                if model_data_args.model_name_or_path is not None and \n",
    "                os.path.isdir(model_data_args.model_name_or_path) \n",
    "                else None\n",
    "                )\n",
    "    # Run training.\n",
    "    trainer.train(model_path=model_path)\n",
    "    # Save model.\n",
    "    trainer.save_model()\n",
    "\n",
    "    # For convenience, we also re-save the tokenizer to the same directory,\n",
    "    # so that you can share your model easily on huggingface.co/models =).\n",
    "    if trainer.is_world_process_zero():\n",
    "    tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d8bf7-5027-4eaa-b403-d20339890945",
   "metadata": {},
   "source": [
    "## Plot Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea84ccf-ae43-4d5e-bd9a-da228e5a7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of train and evaluate loss.\n",
    "loss_history = {'train_loss':[], 'eval_loss':[]}\n",
    "\n",
    "# Keep track of train and evaluate perplexity.\n",
    "# This is a metric useful to track for language models.\n",
    "perplexity_history = {'train_perplexity':[], 'eval_perplexity':[]}\n",
    "\n",
    "# Loop through each log history.\n",
    "for log_history in trainer.state.log_history:\n",
    "\n",
    "    if 'loss' in log_history.keys():\n",
    "        # Deal with trianing loss.\n",
    "        loss_history['train_loss'].append(log_history['loss'])\n",
    "        perplexity_history['train_perplexity'].append(math.exp(log_history['loss']))\n",
    "\n",
    "    elif 'eval_loss' in log_history.keys():\n",
    "        # Deal with eval loss.\n",
    "        loss_history['eval_loss'].append(log_history['eval_loss'])\n",
    "        perplexity_history['eval_perplexity'].append(math.exp(log_history['eval_loss']))\n",
    "\n",
    "# Plot Losses.\n",
    "plot_dict(loss_history,\n",
    "          start_step=training_args.logging_steps, \n",
    "          step_size=training_args.logging_steps,\n",
    "          use_title='Loss', \n",
    "          use_xlabel='Train Steps',\n",
    "          use_ylabel='Values',\n",
    "          magnify=2)\n",
    "print()\n",
    "\n",
    "# Plot Perplexities.\n",
    "plot_dict(perplexity_history, start_step=training_args.logging_steps, \n",
    "          step_size=training_args.logging_steps, use_title='Perplexity', \n",
    "          use_xlabel='Train Steps', use_ylabel='Values', magnify=2)\n",
    "\n",
    "# check if `do_eval` flag is set.\n",
    "if training_args.do_eval:\n",
    "    # capture output if trainer evaluate.\n",
    "    eval_output = trainer.evaluate()\n",
    "    # compute perplexity from model loss.\n",
    "    perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "    print('\\nEvaluate Perplexity: {:10,.2f}'.format(perplexity))\n",
    "else:\n",
    "    print('No evaluation needed. No evaluation data provided, `do_eval=False`!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e35d0-e86f-4142-b516-d38546263cb8",
   "metadata": {},
   "source": [
    "# Semi Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8623de3-3942-4f80-b2b3-2393cbf5fd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "configuration = RobertaConfig() # standard\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0670234e-39b5-4e59-bb97-21df1ca4d8df",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-04efc01ad6c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m dataset = LineByLineTextDataset(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{PATH_DATASET_DIR}/data.1970.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, file_path, block_size)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mbatch_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.5/lib/python3.8/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=f\"{PATH_DATASET_DIR}/data.1970.txt\",\n",
    "    block_size=4,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{PATH_MODEL_DIR}\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_gpu_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8ef5d-03a5-4015-babb-f697491421e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{PATH_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c17e0f81-cfc4-41a6-aef9-48670726bf19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in /home/leonardovida/data/volume_1/data-histaware/tokenizer/1970.json. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: speech_to_text, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-41b2e809c173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m }\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0muse_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_fast\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mconfig_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Unrecognized model in {}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m\"Should have a `model_type` key in its config.json, or contain one of the following strings \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized model in /home/leonardovida/data/volume_1/data-histaware/tokenizer/1970.json. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: speech_to_text, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas"
     ]
    }
   ],
   "source": [
    "model_name_or_path = None\n",
    "model_type = \"wietsedv/bert-base-dutch-cased\" # BertForMaskedLM) # (RobertaConfig, RobertaForMaskedLM),\n",
    "tokenizer_name = f\"{PATH_TOKENIZER_DIR}/1970.json\"\n",
    "cache_dir = '/home/leonardovida/data/volume_1/huggingface_cache/'\n",
    "use_fast_tokenizer = True\n",
    "model_revision = \"main\"\n",
    "use_auth_token = False\n",
    "dataset_name = None\n",
    "dataset_config_name = None\n",
    "train_file = dataset[\"train\"]\n",
    "mlm_probability = 0.15\n",
    "#validation_file = dataset[\"validation\"]\n",
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(2021)\n",
    "\n",
    "# Load model config\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": cache_dir,\n",
    "    \"use_auth_token\": use_auth_token,\n",
    "}\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_type, **config_kwargs)\n",
    "#config = CONFIG_MAPPING[model_type]()\n",
    "\n",
    "# Load Tokenizer config\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": cache_dir,\n",
    "    \"use_fast\": use_fast_tokenizer,\n",
    "    \"use_auth_token\": True if use_auth_token else None,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, **tokenizer_kwargs)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForMaskedLM.from_config(\n",
    "    config,\n",
    "    cache_dir=cache_dir)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "\n",
    "column_names = datasets[\"train\"].column_names\n",
    "\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
    "\n",
    "#Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
    "# We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n",
    "# efficient when it receives the `special_tokens_mask`.\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    #batched=True,\n",
    "    num_proc=9,\n",
    "    remove_columns=column_names,\n",
    ")\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // max_seq_length) * max_seq_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=9,\n",
    ")\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "\n",
    "# Data collator\n",
    "# This one will take care of randomly masking the tokens.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=mlm_probability,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "\n",
    "# Initialize our Trainer for training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Training\n",
    "if True:\n",
    "    checkpoint = None\n",
    "    train_result = trainer.train()\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "    metrics = train_result.metrics\n",
    "\n",
    "    max_train_samples = len(train_dataset)\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0f88b-d857-4358-b9ee-09f4f63bdd43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histaware-NidRwJ64-py3.8",
   "language": "python",
   "name": "histaware-nidrwj64-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
