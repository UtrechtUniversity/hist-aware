{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bc4ed4",
   "metadata": {},
   "source": [
    "# DelphBERT: Create pre-training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b79a8",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f57f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from datasets import load_from_disk, Sequence, Features, Value\n",
    "\n",
    "from transformers import (PreTrainedTokenizer, HfArgumentParser, AutoTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6fca76",
   "metadata": {},
   "source": [
    "### Use pre-trained tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerLambda(object):\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, rng: random.Random, threshold_size: int,\n",
    "                 text_column: str = \"text\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.rng = rng\n",
    "        self.threshold_size = threshold_size\n",
    "        self.text_column: str = text_column\n",
    "\n",
    "    def __call__(self, documents: List[str]) -> Dict[str, Any]:\n",
    "        # batched version\n",
    "        remove_end_count = 2\n",
    "        outputs = []\n",
    "        for doc in documents[self.text_column]:\n",
    "            sentences = nltk.sent_tokenize(doc)\n",
    "            if len(sentences) > 1:\n",
    "                # remove category and references from the end of wikipedia document\n",
    "                # TODO cleaning dataset is a better approach\n",
    "                for i in range(remove_end_count):\n",
    "                    sentences.pop()\n",
    "            if len(sentences) > self.threshold_size:\n",
    "                doc_encoded = self.tokenizer.batch_encode_plus(sentences, add_special_tokens=False,\n",
    "                                                               return_token_type_ids=False, return_attention_mask=False)\n",
    "                outputs.append(doc_encoded[\"input_ids\"])\n",
    "        self.rng.shuffle(outputs)\n",
    "        \n",
    "        return {'tokens': outputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be111f8",
   "metadata": {},
   "source": [
    "### Main function to create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstanceFactoryLambda(object):\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, rng: random.Random, rns: np.random.RandomState,\n",
    "                 max_seq_length: int, short_seq_prob: float, masked_lm_prob: float, max_predictions_per_seq: int,\n",
    "                 dupe_factor: int, text_column: str = \"tokens\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        # for some reason @dataclass instances are not pickable\n",
    "        # would use args as a constructor parameter\n",
    "        # self.args = args\n",
    "        self.rng = rng\n",
    "        self.rns = rns\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.short_seq_prob = short_seq_prob\n",
    "        self.masked_lm_prob = masked_lm_prob\n",
    "        self.max_predictions_per_seq = max_predictions_per_seq\n",
    "        self.dupe_factor = dupe_factor\n",
    "        self.text_column: str = text_column\n",
    "        self.MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                                       [\"index\", \"label\"])\n",
    "\n",
    "    def __call__(self, documents: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Create `TrainingInstance`s from documents.\"\"\"\n",
    "        vocab_words = list(self.tokenizer.get_vocab().keys())\n",
    "        all_tokens, all_segment_ids, all_is_random_next, all_masked_lm_positions, all_masked_lm_labels = [], [], [], [], []\n",
    "\n",
    "        for _ in range(self.dupe_factor):\n",
    "            for document_index in range(len(documents[self.text_column])):\n",
    "                tokens, segment_ids, is_random_next, masked_lm_positions, masked_lm_labels = self.create_instances_from_document(\n",
    "                    documents[self.text_column], document_index, self.max_seq_length, self.short_seq_prob,\n",
    "                    self.masked_lm_prob, self.max_predictions_per_seq, vocab_words, self.rng)\n",
    "\n",
    "                all_tokens.extend(tokens)\n",
    "                all_segment_ids.extend(segment_ids)\n",
    "                all_is_random_next.extend(is_random_next)\n",
    "                all_masked_lm_positions.extend(masked_lm_positions)\n",
    "                all_masked_lm_labels.extend(masked_lm_labels)\n",
    "\n",
    "        return self.convert_instances_to_dataset(all_tokens, all_segment_ids, all_is_random_next,\n",
    "                                                 all_masked_lm_positions, all_masked_lm_labels)\n",
    "\n",
    "    def convert_instances_to_dataset(self, all_tokens, all_segment_ids, all_is_random_next,\n",
    "                                     all_masked_lm_positions, all_masked_lm_labels):\n",
    "        \"\"\"Create new HF dataset from training instances\"\"\"\n",
    "\n",
    "        num_instances = len(all_tokens)\n",
    "        all_input_ids = np.zeros([num_instances, self.max_seq_length], dtype=\"int32\")\n",
    "        all_attention_mask = np.zeros([num_instances, self.max_seq_length], dtype=\"int8\")\n",
    "        all_token_type_ids = np.zeros([num_instances, self.max_seq_length], dtype=\"int8\")\n",
    "        all_masked_labels = np.full([num_instances, self.max_seq_length], fill_value=-100, dtype=\"int32\")\n",
    "        all_next_sentence_labels = np.zeros(num_instances, dtype=\"int8\")\n",
    "\n",
    "        for idx in range(num_instances):\n",
    "            input_ids = all_tokens[idx]\n",
    "            input_mask = [1] * len(input_ids)\n",
    "            masked_labels = np.full([self.max_seq_length, ], -100, dtype=\"int32\")\n",
    "            masked_labels[all_masked_lm_positions[idx]] = all_masked_lm_labels[idx]\n",
    "\n",
    "            # masked_lm_weights omitted for pytorch, needed for tf features\n",
    "            all_input_ids[idx][:len(input_ids)] = input_ids\n",
    "            all_attention_mask[idx][:len(input_ids)] = input_mask\n",
    "            all_token_type_ids[idx][:len(input_ids)] = all_segment_ids[idx]\n",
    "            all_masked_labels[idx] = masked_labels\n",
    "            all_next_sentence_labels[idx] = 1 if all_is_random_next[idx] else 0\n",
    "\n",
    "        p = self.rns.permutation(len(all_input_ids))\n",
    "        return {\"input_ids\": all_input_ids[p],\n",
    "                \"attention_mask\": all_attention_mask[p],\n",
    "                \"token_type_ids\": all_token_type_ids[p],\n",
    "                \"labels\": all_masked_labels[p],\n",
    "                \"next_sentence_label\": all_next_sentence_labels[p]}\n",
    "\n",
    "    def create_instances_from_document(self,\n",
    "                                       all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "                                       masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "        \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "        document = all_documents[document_index]\n",
    "\n",
    "        # Account for [CLS], [SEP], [SEP]\n",
    "        max_num_tokens = max_seq_length - 3\n",
    "\n",
    "        # We *usually* want to fill up the entire sequence since we are padding\n",
    "        # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "        # computation. However, we *sometimes*\n",
    "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "        # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "        # The `target_seq_length` is just a rough target however, whereas\n",
    "        # `max_seq_length` is a hard limit.\n",
    "        target_seq_length = max_num_tokens\n",
    "        if rng.random() < short_seq_prob:\n",
    "            target_seq_length = rng.randint(2, max_num_tokens)\n",
    "\n",
    "        # We DON'T just concatenate all of the tokens from a document into a long\n",
    "        # sequence and choose an arbitrary split point because this would make the\n",
    "        # next sentence prediction task too easy. Instead, we split the input into\n",
    "        # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "        # input.\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "        all_tokens, all_segment_ids, all_is_random_next, all_masked_lm_positions, all_masked_lm_labels = [], [], [], [], []\n",
    "        while i < len(document):\n",
    "            segment = document[i]\n",
    "            current_chunk.append(segment)\n",
    "            current_length += len(segment)\n",
    "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                    # (first) sentence.\n",
    "                    a_end = 1\n",
    "                    if len(current_chunk) >= 2:\n",
    "                        a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "                    tokens_a = []\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "\n",
    "                    tokens_b = []\n",
    "                    # Random next\n",
    "                    is_random_next = False\n",
    "                    if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        # This should rarely go for more than one iteration for large\n",
    "                        # corpora. However, just to be careful, we try to make sure that\n",
    "                        # the random document is not the same as the document\n",
    "                        # we're processing.\n",
    "                        random_document_index = None\n",
    "                        for _ in range(10):\n",
    "                            random_document_index = rng.randint(0, len(all_documents) - 1)\n",
    "                            if random_document_index != document_index:\n",
    "                                break\n",
    "\n",
    "                        # If picked random document is the same as the current document\n",
    "                        if random_document_index == document_index:\n",
    "                            is_random_next = False\n",
    "\n",
    "                        random_document = all_documents[random_document_index]\n",
    "                        random_start = rng.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "                        # We didn't actually use these segments so we \"put them back\" so\n",
    "                        # they don't go to waste.\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    # Actual next\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "                    self.truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "\n",
    "                    assert len(tokens_a) >= 1\n",
    "                    assert len(tokens_b) >= 1\n",
    "\n",
    "                    tokens = []\n",
    "                    segment_ids = []\n",
    "                    tokens.append(self.tokenizer.cls_token_id)\n",
    "                    segment_ids.append(0)\n",
    "                    for token in tokens_a:\n",
    "                        tokens.append(token)\n",
    "                        segment_ids.append(0)\n",
    "\n",
    "                    tokens.append(self.tokenizer.sep_token_id)\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "                    for token in tokens_b:\n",
    "                        tokens.append(token)\n",
    "                        segment_ids.append(1)\n",
    "                    tokens.append(self.tokenizer.sep_token_id)\n",
    "                    segment_ids.append(1)\n",
    "\n",
    "                    (tokens, masked_lm_positions,\n",
    "                     masked_lm_labels) = self.create_masked_lm_predictions(\n",
    "                        tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "\n",
    "                    all_tokens.append(tokens)\n",
    "                    all_segment_ids.append(segment_ids)\n",
    "                    all_is_random_next.append(is_random_next)\n",
    "                    all_masked_lm_positions.append(masked_lm_positions)\n",
    "                    all_masked_lm_labels.append(masked_lm_labels)\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "            i += 1\n",
    "\n",
    "        return all_tokens, all_segment_ids, all_is_random_next, all_masked_lm_positions, all_masked_lm_labels\n",
    "\n",
    "    def create_masked_lm_predictions(self, tokens, masked_lm_prob,\n",
    "                                     max_predictions_per_seq, vocab_words, rng):\n",
    "        \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "        cand_indexes = []\n",
    "        for (i, token) in enumerate(tokens):\n",
    "            if token == self.tokenizer.cls_token_id or token == self.tokenizer.sep_token_id:\n",
    "                continue\n",
    "            cand_indexes.append(i)\n",
    "\n",
    "        rng.shuffle(cand_indexes)\n",
    "\n",
    "        output_tokens = list(tokens)\n",
    "\n",
    "        num_to_predict = min(max_predictions_per_seq,\n",
    "                             max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "        masked_lms = []\n",
    "        covered_indexes = set()\n",
    "        for index in cand_indexes:\n",
    "            if len(masked_lms) >= num_to_predict:\n",
    "                break\n",
    "            if index in covered_indexes:\n",
    "                continue\n",
    "            covered_indexes.add(index)\n",
    "\n",
    "            masked_token = None\n",
    "            # 80% of the time, replace with [MASK]\n",
    "            if rng.random() < 0.8:\n",
    "                masked_token = self.tokenizer.mask_token_id\n",
    "            else:\n",
    "                # 10% of the time, keep original\n",
    "                if rng.random() < 0.5:\n",
    "                    masked_token = tokens[index]\n",
    "                # 10% of the time, replace with random word\n",
    "                else:\n",
    "                    random_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "                    masked_token = self.tokenizer.convert_tokens_to_ids(random_token)\n",
    "\n",
    "            output_tokens[index] = masked_token\n",
    "            masked_lms.append(self.MaskedLmInstance(index=index, label=tokens[index]))\n",
    "\n",
    "        masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "        masked_lm_positions = []\n",
    "        masked_lm_labels = []\n",
    "        for p in masked_lms:\n",
    "            masked_lm_positions.append(p.index)\n",
    "            masked_lm_labels.append(p.label)\n",
    "\n",
    "        return output_tokens, masked_lm_positions, masked_lm_labels\n",
    "\n",
    "    def truncate_seq_pair(self, tokens_a, tokens_b, max_num_tokens, rng):\n",
    "        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_num_tokens:\n",
    "                break\n",
    "\n",
    "            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "            assert len(trunc_tokens) >= 1\n",
    "\n",
    "            # We want to sometimes truncate from the front and sometimes from the\n",
    "            # back to add more randomness and avoid biases.\n",
    "            if rng.random() < 0.5:\n",
    "                del trunc_tokens[0]\n",
    "            else:\n",
    "                trunc_tokens.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dataset.map(TokenizerLambda(tokenizer, rng, args.document_size_threshold), batched=True,\n",
    "                            remove_columns=dataset.column_names, batch_size=args.batch_size,\n",
    "                            num_proc=cpu_count)\n",
    "\n",
    "print(f\"Creating training instances using {len(documents)} documents, please wait...\")\n",
    "\n",
    "f = Features({'input_ids': Sequence(feature=Value(dtype='int32')),\n",
    "              'attention_mask': Sequence(feature=Value(dtype='int8')),\n",
    "              'token_type_ids': Sequence(feature=Value(dtype='int8')),\n",
    "              'labels': Sequence(feature=Value(dtype='int32')),\n",
    "              'next_sentence_label': Value(dtype='int8'),\n",
    "              })\n",
    "\n",
    "for shard_i in range(args.num_shards):\n",
    "    documents_shard = documents.shard(num_shards=args.num_shards, index=shard_i)\n",
    "\n",
    "    print(f\"Processing shard {shard_i} with {len(documents_shard)} documents.\")\n",
    "    \n",
    "    pre_training_dataset = documents_shard.map(\n",
    "        TrainingInstanceFactoryLambda(tokenizer, rng=rng, rns=rns,\n",
    "                                      max_seq_length=args.max_seq_length,\n",
    "                                      short_seq_prob=args.short_seq_prob,\n",
    "                                      masked_lm_prob=args.masked_lm_prob,\n",
    "                                      dupe_factor=args.dupe_factor,\n",
    "                                      max_predictions_per_seq=args.max_predictions_per_seq),\n",
    "        batched=True, features=f, remove_columns=documents.column_names, batch_size=args.batch_size,\n",
    "        num_proc=cpu_count)\n",
    "    \n",
    "    shard_output_file = \"_\".join([args.output_dataset, str(shard_i)])\n",
    "    \n",
    "    print(f\"Saving dataset {shard_output_file} with {len(pre_training_dataset)} samples to disk.\")\n",
    "\n",
    "    pre_training_dataset.save_to_disk(shard_output_file)\n",
    "\n",
    "print(f\"Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
