{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5dbd8f",
   "metadata": {},
   "source": [
    "# Pre-training DelphBERT - A tiny Transformer model based on a large newspaper dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d20b8",
   "metadata": {},
   "source": [
    "* //TODO: Copy description of project from Github\n",
    "* //TODO: Description of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "665cd494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Any, Dict, List, NamedTuple, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import RobertaForMaskedLM\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c8889",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f327ea7",
   "metadata": {},
   "source": [
    "First, we need to load the entire \"processed\" library of Delpher newspaper on the transformer.\n",
    "\n",
    "* // TODO: even though we are loading clean and unclean text, we are training using the **unclean** text, as Transformers want complete sentences and the clean text does not have stopwords nor lower-upper cases.\n",
    "    * Think testing the performance training on semi-clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91b8a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_FILES = \"/home/leonardovida/data-histaware/raw/raw_merged/\"\n",
    "PATH_TOKENIZER_DIR = \"/home/leonardovida/dev/hist-aware/notebooks/models/bert-training-from-scratch/tokenizer\"\n",
    "PATH_DATASET_DIR = \"/home/leonardovida/dev/hist-aware/notebooks/models/bert-training-from-scratch/dataset\"\n",
    "#!mkdir PATH_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f35341b",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd44c7",
   "metadata": {},
   "source": [
    "### Preprocess from .csv into .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126cc90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en_US', 'en', 'en_AU', 'en_CA', 'en_GB']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enchant\n",
    "broker = enchant.Broker()\n",
    "broker.describe()\n",
    "broker.list_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff823268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import enchant\n",
    "import nltk\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        #self.d = enchant.Dict(\"nl_NL\")\n",
    "        self.stopword_list = nltk.corpus.stopwords.words(\"dutch\")\n",
    "        self.STOPWORDS = set(self.stopword_list)\n",
    "        \n",
    "    def get_words(self):\n",
    "        self.text = \" \".join([c for c in nltk.word_tokenize(self.text)])\n",
    "        return self\n",
    "\n",
    "    def lower(self):\n",
    "        \"\"\"Transform to lower case.\"\"\"\n",
    "        self.text = \"\".join([t.lower() for t in self.text])\n",
    "        return self\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        \"\"\"Remove the stopwords.\"\"\"\n",
    "        self.text = \"\".join([t for t in self.text if t not in self.STOPWORDS])\n",
    "        return self\n",
    "\n",
    "    def remove_numeric(self):\n",
    "        \"\"\"Remove numbers.\"\"\"\n",
    "        self.text = \"\".join([c for c in self.text if not c.isdigit()])\n",
    "        return self\n",
    "\n",
    "    def remove_non_ascii(self):\n",
    "        \"\"\"Remove non ASCII chars.\"\"\"\n",
    "        self.text = \"\".join([re.sub(r\"[^\\x00-\\x7f]\", r\" \", c) for c in self.text])\n",
    "        return self\n",
    "\n",
    "    def remove_extra_whitespace_tabs(self):\n",
    "        \"\"\"Remove extra whitespaces and tabs.\"\"\"\n",
    "        self.text = re.sub(r\"^\\s*|\\s\\s*\", \" \", self.text).strip()\n",
    "        return self\n",
    "\n",
    "    def remove_one_char(self):\n",
    "        self.text = \" \".join([w for w in self.text.split() if len(w) > 1])\n",
    "        return self\n",
    "\n",
    "    def remove_non_words(self):\n",
    "        \"\"\"Remove rare words.\"\"\"\n",
    "        self.text = \" \".join(\n",
    "            [word for word in str(self.text).split() if self.d.check(word)]\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def keep_standard_chars(self):\n",
    "        self.text = \"\".join([re.sub(r\"[^-0-9\\w,. ?!()%/]\", r\"\", c) for c in self.text])\n",
    "        return self\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        self.text = text\n",
    "        self = self.get_words()\n",
    "        self = self.lower()\n",
    "        self = self.remove_stopwords()\n",
    "        self = self.remove_numeric()\n",
    "        self = self.remove_extra_whitespace_tabs()\n",
    "        self = self.remove_one_char()\n",
    "        self = self.remove_non_words()\n",
    "        return self.text\n",
    "\n",
    "    def clean(self, text):\n",
    "        self.text = text\n",
    "        self = self.get_words()\n",
    "        self = self.keep_standard_chars()\n",
    "        self = self.remove_extra_whitespace_tabs()\n",
    "        return self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da32e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, tqdm_notebook\n",
    "from loguru import logger\n",
    "\n",
    "def process_selected_articles(path):\n",
    "    tqdm_notebook().pandas()\n",
    "    csv_temp = []\n",
    "    # Create preprocessing class\n",
    "    tc = TextCleaner()\n",
    "\n",
    "    # Load merged articles for selected topic in nlp_pipeline\n",
    "    df = pd.read_csv(path)\n",
    "         \n",
    "    # Initial clean\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(\n",
    "        columns={\n",
    "            \"index\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Split p into original paragraphs\n",
    "    logger.debug(\n",
    "        f\"Articles before selecting 'articles': {df.shape[0]}\"\n",
    "    )\n",
    "    df = df[df[\"subject\"] == \"artikel\"]\n",
    "    df[\"p\"] = df.apply(lambda row: repr(row[\"p\"]).split(\"\\\\',\"), axis=1)\n",
    "    logger.debug(\n",
    "        f\"Articles after selecting 'articles': {df.shape[0]}\"\n",
    "    )\n",
    "    df = df.explode(\"p\")\n",
    "    logger.debug(\n",
    "        f\"Articles after splitting into paragraphs: {df.shape[0]}\"\n",
    "    )\n",
    "\n",
    "    # Preprocess p to cleaner p for Tokenizer and transformers\n",
    "    res = df[\"p\"].progress_apply(tc.clean)\n",
    "\n",
    "    # Eliminate paragraphs that do not contain anything\n",
    "    res.dropna(inplace=True)\n",
    "    \n",
    "    # Save to .txt  \n",
    "    base = os.path.basename(path)\n",
    "    name = os.path.splitext(base)[0]\n",
    "    res.to_csv(f'/home/leonardovida/data-histaware/raw/raw_merged/{name}.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    \n",
    "    logger.debug(f\"Completed: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea28cf",
   "metadata": {},
   "source": [
    "### Convert from _.csv_ to _.txt_ - Do it just once though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782e1058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_40.csv',\n",
       " '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_60.csv',\n",
       " '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_80.csv',\n",
       " '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_120.csv',\n",
       " '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_100.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find path to csv files with processed data\n",
    "#Path().parent.absolute()\n",
    "paths = [str(x) for x in Path(PATH_RAW_FILES).glob(\"*.csv\")]\n",
    "paths = paths[2:]\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117ee91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7fd6f84a604e4d90588b0ff7ca4fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b906627d484d96917f53f250849f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-10 14:31:52.863 | DEBUG    | __main__:process_selected_articles:23 - Articles before selecting 'articles': 957008\n",
      "2021-04-10 14:32:03.730 | DEBUG    | __main__:process_selected_articles:28 - Articles after selecting 'articles': 567544\n",
      "2021-04-10 14:32:07.279 | DEBUG    | __main__:process_selected_articles:32 - Articles after splitting into paragraphs: 1225204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ab81f569854f26a5ae6176d4f6019e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1225204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create txt files for \"processed\" data\n",
    "for path in tqdm(paths, total=len(paths)):\n",
    "    process_selected_articles(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f1d834",
   "metadata": {},
   "source": [
    "### Load .txt files into one (to be changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = [str(x) for x in Path(PATH_RAW_FILES).glob(\"*.txt\")]\n",
    "df = pd.DataFrame()\n",
    "for file in text_files:\n",
    "    temp = pd.read_csv(f\"{PATH_RAW_FILES}/merged_1970s_20.txt\", delimiter = \"\\t\", header=None)\n",
    "    df = pd.concat([df, temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d4a385",
   "metadata": {},
   "source": [
    "## Load data into Dataset directly from .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8c0b426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1765ec5baa25fcb6\n",
      "Reusing dataset csv (/home/leonardovida/.cache/huggingface/datasets/csv/default-1765ec5baa25fcb6/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "csv_paths = [str(x) for x in Path(PATH_RAW_FILES).glob(\"*.csv\")]\n",
    "dataset = datasets.load_dataset(\n",
    "    \"csv\",\n",
    "    data_files = csv_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00720f33",
   "metadata": {},
   "source": [
    "Remove columns that are not necessary in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3e36c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['Unnamed: 0', 'article_name', 'date', 'index_article',\n",
    "                                  'article_filepath', 'dir', 'title', 'access_rights',\n",
    "                                  'identifier', 'metadata_title', 'index_metadata', 'metadata_filepath',\n",
    "                                  'newspaper_title', 'newspaper_date', 'newspaper_publisher', 'newspaper_source',\n",
    "                                  'newspaper_volume', 'newspaper_issuenumber', 'newspaper_recordIdentifier',\n",
    "                                  'transformedRecordIdentifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5a9a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/leonardovida/.cache/huggingface/datasets/csv/default-1765ec5baa25fcb6/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-8a85db9cb5d9cb6e.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda article: article['subject'] == \"artikel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][\"p\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b2dcbe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[ 'SNEEK â€” Wij , hier in Nederland , moeten een les trekken uit wat er nu in Chili is gebeurd , zei de heer Piet Reekman , leider van de Sjaloomgroep in Odijk , gisteravond in de Ichtuskerk in Sneek . Wij moeten uit al die dingen die zijn gebeurd om Allende en zijn volksbeweging tegen te werken , op opzij te zetten en uit te moorden , iets leren hier in Nederland . Want als we niet oppassen worden de democratische krachten die hier streven voor een rechtvaardiger samenleving ook opzij gezet , zei hij . ' , 'De heer Reekman was naar Sneek uitgenodigd om over Chili te praten voor leden van de Rotonde-gespreksgroepen . Aan de hand van zijn inleiding zullen de komende weken discussieavonden worden gehouden . Aan het einde van zijn inleiding trok Piet Reekman gisteravond een parallel tussen de aarzelende ( iiristen-dernocratische partijen , in Chili en de christen-democratische partijen in Nederland , die volgens hem ook nooit een keuze durven maken . ' , 'het volk kan weer in armoede vervallen . ' , 'Een voor onze begrippen heel merkwaardige organisatie , zo vertelde de heer Reekman , was de beweging â€ž Christenen voor het socialisme '' , die in Chili Allende steunde . Meer dan de helft van de pastores in het land was er lid van . â€ž Zoiets durven wij niet . Wij zitten met een valse gespletenheid , die in eeuwen ontwikkeld is . Alsof christendom niet voor het volk zou zijn . Als â€ž In Chili hebben die partijen onnoemelijk zitten scharrelen en sjoemelen en daarmee hebben ze de rechtse krachten , die nu al de levens van 30.000 Chilenen op hun geweten hebben , gedekt . Ze kozen nooit . En hier in Nederland scharrelen de confessionele partijen net zo hard . Uit de les van Chili moeten wij leren dat dat niet langer kan. ledereen , die in christen-democratische partijen zit en die zijn hart wil laten spreken , moet deze partij dwingen tot een keuze . Altijd hebben de rechtse krachten in deze partijen weer de overhand en de scheur die duidelijk door die partijen loopt wordt bedekt , wordt weggepoetst . Maar die scheur moet duidelijk worden . Er moet een keuze gemaakt worden , want ook zij vormen een stukje gezicht van de kerk '' . Aldus de heer Reekman . ' , 'De heer Reekman heeft vorig jaar een bezoek aan Chili gebracht en hij heeft ondermeer gesproken met Salvador Allende , tot de militaire staatsgreep president van het land . Chili is een ontzettend rijk land met een talentvol , rijk volk . De grond is rijk en in de bodem zitten onnoemelijk veel schatten . Een derde van de totale wereldkopervoorraad zit in de Chileense bodem . Ook de zee langs dit 4500 kilometer lange Zuidamerikaanse land is zeer rijk aan vis . En dit rijke land is stelselmatig leeggeplunderd door grote ondernemingen , in de meeste gevallen Amerikaanse . ' , 'Piet Reekman vertelde dat de Amerikaanse kopermaaitschappijen vijftig jaar geleden een investering in hun fabrieken hebben gedaan van twintig miljoen gulden . Sinds die tijd hebben ze voor ruim 4 % miljard gulden winst uit Chili naar de Verenigde Staten gehaald . â€ž Men spreekt dan over ontwikkelingshulp van rijke aan arme landen , maar het is in feite andersom . De arme landen helpen met hun schatten de ontwikkeling van de rijke landen , zodat ze nog rijker worden\\\\ ' 1 . In al die jaren , aldus Piet Reekman , hebben de Amerikanen nooit meer een cent in de koperindustrieÃ«n geÃ¯nvesteerd , want de machines , e.d . moesten door de Chilenen zelf worden betaald . Die truc hebben de Chilenen doorgekregen . Democratisch hebben ze Allende . die dat wou tegengaan , in het zadel geholpen . Maar drie jaar heeft hij zijn werk mogen doen ; toen was de maat voor de rechtse krachten , geholpen door de Amerikaanse CIA , vol . In een bloedige moordpartij , die nog voortduurt maken ze het land weer geschikt voor maatschappijen , die alleen op winst uit zijn . En Christus nu zou hebben geleefd , zou hij onmiddellijk worden opgepakt , omdat hij zich van geen autoriteit iets aantrok . De bek van leeuwen splijt ik open en wetten zal ik overtreden , heeft hij gezegd . Waarom zijn wij dan zo gespleten en waarom kiezen diristen-democraten in ons land uitemdelijk altijd weer voorrechts ? `` ' , 'De waarheid , zo sprak de heer Reekman , van de werkelijkheid is ontzettend eenvoudig , maar alles wordt ingewikkeld gemaakt en je kunt haast als regel aannemen dat hoe hoger het inkomen is hoe ondoorzichtiger en ingewikkelder de mensen de zaken maken . Zij hebben er belang bij de dingen onbegrijpelijk te houden , zodat men niet in de gaten krijgt hoe de structuur precies werkt die hen aan een hoog inkomen helpt Terwijl alles in werkelijkheid zo vreselijk eenvoudig ligt '' . In Chili , aldus Piet Reekman , had men dat door . Toen het menens werd , grepen de rechtse krachten geholpen door Amerika in . Laten wij daar van leren . ' , 'PIET RECKMAN ' ]\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\" \".join([c for c in nltk.word_tokenize(dataset[\"train\"][\"p\"][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94256a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725f81b4ae2044be9c6084e20a56d4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4186207.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def paragraph_clean(article):\n",
    "    \"\"\"Basic cleaning of paragraphs.\n",
    "    \n",
    "    More can and will be done in the tokenizing step.\n",
    "    \"\"\"\n",
    "    text = \"\".join([re.sub(r\"[^-0-9\\w,. ?!()%/]\", r\"\", c) for c in article])\n",
    "    text = re.sub(r\"^\\s*|\\s\\s*\", \" \", text).strip()\n",
    "    return {\"p\": text}\n",
    "    \n",
    "dataset = dataset.map(paragraph_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5898b",
   "metadata": {},
   "source": [
    "Divide each paragraph into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef01001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sentence_tokenize(article):\n",
    "    \"\"\"Tokenize sentences from paragraphs.\"\"\"\n",
    "    sents = re.split('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)(\\s|[A-Z].*)', article[\"p\"])\n",
    "    return {\"sentences\": sents}\n",
    "\n",
    "dataset = dataset.map(sentence_tokenize, num_proc=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a04c2",
   "metadata": {},
   "source": [
    "Rebuild from sentences to shorter paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unite(sentences, n):\n",
    "    \"\"\"Unite sentences previously split using nltk.tokenize.\"\"\"\n",
    "    count = [0]\n",
    "    sents = []\n",
    "    for sent in sentences:\n",
    "        if sum(count) + len(sent.split()) > 400:\n",
    "            return sents\n",
    "        else:\n",
    "            sents.append(sent)\n",
    "            count.append(len(sent.split()))\n",
    "\n",
    "dataset = dataset.map(unite, num_proc=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229e9d3",
   "metadata": {},
   "source": [
    "Split into train, test and validation and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 90% train\n",
    "train_testvalid = dataset.train_test_split(test_size=0.1)\n",
    "# Split valid into 50% valid and 50% test\n",
    "test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5)\n",
    "# Gather everything into dataset\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_testvalid[\"train\"],\n",
    "    \"test\": test_valid[\"test\"],\n",
    "    \"valid\": test_valid[\"train\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(PATH_DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f66be",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9654cf8",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84814fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "reloaded_encoded_dataset = load_from_disk(PATH_DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cf396a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir {tokenizer_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb94f1",
   "metadata": {},
   "source": [
    "BERT uses WordPiece\n",
    "\n",
    "Here we normalize a LOT the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f60c82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "356fca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=50010, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "bert_tokenizer.train(text_files, trainer)\n",
    "\n",
    "bert_tokenizer.save(f\"{tokenizer_dir}/delphbert.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When dataset is ready\n",
    "#def batch_iterator(batch_size=1000):\n",
    "#    for i in range(0, len(dataset), batch_size):\n",
    "#        yield dataset[i : i + batch_size][\"text\"]\n",
    "#tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9969eb7",
   "metadata": {},
   "source": [
    "### Additional tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0989c163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_20.txt', '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_140.txt', '/home/leonardovida/data-histaware/raw/raw_merged/merged_1970s_40.txt']\n",
      "CPU times: user 19min 45s, sys: 25.1 s, total: 20min 10s\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "paths = [str(x) for x in Path(PATH_RAW_FILES).glob(\"*.txt\")]\n",
    "print(f\"Found {len(print(paths))} text files from which a tokenizer will be trained\")\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95f72d4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ByteLevelBPETokenizer' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f6693d3b4da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Save both vocab and merges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#tokenizer.save(f\"{tokenizer_dir}/vocab.json\", f\"{tokenizer_dir}/merges.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ByteLevelBPETokenizer' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "# Save just json\n",
    "tokenizer.save_model(tokenizer_dir)\n",
    "# Save both vocab and merges\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "#tokenizer.save(f\"{tokenizer_dir}/vocab.json\", f\"{tokenizer_dir}/merges.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf151316",
   "metadata": {},
   "source": [
    "### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59225c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(f\"{tokenizer_dir}/delphbert.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf635a6",
   "metadata": {},
   "source": [
    "### Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2a7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_dataset = dataset.map(lambda article: tokenizer(article['p']), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8cda5",
   "metadata": {},
   "source": [
    "### Find max length articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde44cb7",
   "metadata": {},
   "source": [
    "Go through the entire dataset and find the max lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in df.content:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e8d3a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1374043cd7c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer._tokenizer.post_processor = BertProcessing(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_truncation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa171097",
   "metadata": {},
   "source": [
    "Test tokenizer performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3fa635e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Waarom',\n",
       " 'Ä ',\n",
       " 'niet',\n",
       " 'Ä ',\n",
       " 'met',\n",
       " 'Ä ',\n",
       " 'd',\n",
       " 'Ãƒ',\n",
       " 'Â©',\n",
       " 'da',\n",
       " 'in',\n",
       " 'Ä ',\n",
       " 'over',\n",
       " 'Ä ',\n",
       " 'Schilder',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'zoals',\n",
       " 'Ä ',\n",
       " 'over',\n",
       " 'Ä ',\n",
       " 'zoveel',\n",
       " 'Ä ',\n",
       " 'anderen',\n",
       " '?',\n",
       " 'Ä ',\n",
       " 'Hij',\n",
       " 'Ä ',\n",
       " 'wist',\n",
       " 'Ä ',\n",
       " 'zich',\n",
       " 'Ä ',\n",
       " 'tegenstander',\n",
       " ':',\n",
       " 'Ä ',\n",
       " 'Barth',\n",
       " 'Ä ',\n",
       " 'stond',\n",
       " 'Ä ',\n",
       " 'tussen',\n",
       " 'Ä ',\n",
       " 'beiden',\n",
       " '.',\n",
       " 'Ä ',\n",
       " 'Maar',\n",
       " 'Ä ',\n",
       " 'Mis',\n",
       " 'k',\n",
       " 'otte',\n",
       " 'Ä ',\n",
       " 'wist',\n",
       " 'Ä ',\n",
       " 'zich',\n",
       " 'Ä ',\n",
       " 'in',\n",
       " 'Ä ',\n",
       " 'dezelfde',\n",
       " 'Ä ',\n",
       " 'tijd',\n",
       " 'Ä ',\n",
       " 'te',\n",
       " 'Ä ',\n",
       " 'staan',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'in',\n",
       " 'Ä ',\n",
       " 'dezelfde',\n",
       " 'Ä ',\n",
       " 'storm',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'die',\n",
       " 'Ä ',\n",
       " 'ook',\n",
       " 'Ä ',\n",
       " 'Schilder',\n",
       " 'Ä ',\n",
       " 'onderging',\n",
       " '.',\n",
       " 'Ä ',\n",
       " 'Soms',\n",
       " 'Ä ',\n",
       " 'is',\n",
       " 'Ä ',\n",
       " 'er',\n",
       " 'Ä ',\n",
       " 'opvallende',\n",
       " 'Ä ',\n",
       " 'affin',\n",
       " 'iteit',\n",
       " '.',\n",
       " 'Ä ',\n",
       " 'Hij',\n",
       " 'Ä ',\n",
       " 'kende',\n",
       " 'Ä ',\n",
       " 'de',\n",
       " 'Ä ',\n",
       " 'dichters',\n",
       " 'Ä ',\n",
       " 'van',\n",
       " 'Ä ',\n",
       " 'Nederland',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'zoals',\n",
       " 'Ä ',\n",
       " 'ook',\n",
       " 'Ä ',\n",
       " 'Schilder',\n",
       " ':',\n",
       " 'Ä ',\n",
       " 'D',\n",
       " 'Ãƒ',\n",
       " 'Â¨',\n",
       " 'r',\n",
       " 'Ä ',\n",
       " 'M',\n",
       " 'ouw',\n",
       " 'Ä ',\n",
       " '(',\n",
       " 'Ad',\n",
       " 'wa',\n",
       " 'ita',\n",
       " ')',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'Nij',\n",
       " 'hof',\n",
       " 'Ä ',\n",
       " 'f',\n",
       " 'Ä ',\n",
       " 'en',\n",
       " 'Ä ',\n",
       " 'Mar',\n",
       " 'sman',\n",
       " '.',\n",
       " 'Ä ',\n",
       " 'Hij',\n",
       " 'Ä ',\n",
       " 'stond',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'denk',\n",
       " 'Ä ',\n",
       " 'ik',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'geestelijk',\n",
       " 'Ä ',\n",
       " 'ook',\n",
       " 'Ä ',\n",
       " 'aanzienlijk',\n",
       " 'Ä ',\n",
       " 'dichter',\n",
       " 'Ä ',\n",
       " 'bij',\n",
       " 'Ä ',\n",
       " 'hen',\n",
       " '.',\n",
       " 'Ä ',\n",
       " 'Hij',\n",
       " 'Ä ',\n",
       " 'had',\n",
       " 'Ä ',\n",
       " 'de',\n",
       " 'Ä ',\n",
       " 'Nederlandse',\n",
       " 'Ä ',\n",
       " 'taal',\n",
       " 'Ä ',\n",
       " 'en',\n",
       " 'Ä ',\n",
       " 'het',\n",
       " 'Ä ',\n",
       " 'Nederlands',\n",
       " 'Ä ',\n",
       " 'land',\n",
       " 'Ä ',\n",
       " 'lief',\n",
       " ':',\n",
       " 'Ä ',\n",
       " 'Ã¢',\n",
       " 'Ä¢',\n",
       " 'Å€',\n",
       " 'De',\n",
       " 'Ä ',\n",
       " 'nederlandse',\n",
       " 'Ä ',\n",
       " 'taal',\n",
       " 'Ä ',\n",
       " 'en',\n",
       " 'Ä ',\n",
       " 'het',\n",
       " 'Ä ',\n",
       " 'gebo',\n",
       " 'om',\n",
       " 'te',\n",
       " 'Ä ',\n",
       " 'van',\n",
       " 'Ä ',\n",
       " 'dit',\n",
       " 'Ä ',\n",
       " 'land',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'deze',\n",
       " 'Ä ',\n",
       " 'twee',\n",
       " 'Ä ',\n",
       " 'gewassen',\n",
       " 'Ä ',\n",
       " 'zijn',\n",
       " 'Ä ',\n",
       " 'mijn',\n",
       " 'Ä ',\n",
       " 'aardse',\n",
       " 'Ä ',\n",
       " 'heerlijkheid',\n",
       " '\"',\n",
       " 'Ä ',\n",
       " '(',\n",
       " '329',\n",
       " ')',\n",
       " '.',\n",
       " 'Ä ',\n",
       " 'Al',\n",
       " 'Ä ',\n",
       " 'zou',\n",
       " 'Ä ',\n",
       " 'het',\n",
       " 'Ä ',\n",
       " 'all',\n",
       " 'Ãƒ',\n",
       " 'Â©',\n",
       " 'Ãƒ',\n",
       " 'Â©',\n",
       " 'n',\n",
       " 'Ä ',\n",
       " 'd',\n",
       " 'Ãƒ',\n",
       " 'Â©',\n",
       " 'ze',\n",
       " 'Ä ',\n",
       " 'zin',\n",
       " 'Ä ',\n",
       " 'zijn',\n",
       " 'Ä ',\n",
       " '-',\n",
       " 'Ä ',\n",
       " 'daarvoor',\n",
       " 'Ä ',\n",
       " 'is',\n",
       " 'Ä ',\n",
       " 'lectuur',\n",
       " 'Ä ',\n",
       " 'van',\n",
       " 'Ä ',\n",
       " 'meer',\n",
       " 'Ä ',\n",
       " 'dan',\n",
       " 'Ä ',\n",
       " '600',\n",
       " 'Ä ',\n",
       " 'bladzijden',\n",
       " 'Ä ',\n",
       " 'geen',\n",
       " 'Ä ',\n",
       " 'te',\n",
       " 'Ä ',\n",
       " 'hoge',\n",
       " 'Ä ',\n",
       " 'prijs',\n",
       " '!',\n",
       " 'Ä ',\n",
       " 'Dit',\n",
       " 'Ä ',\n",
       " 'Nederland',\n",
       " 'Ä ',\n",
       " 'werd',\n",
       " 'Ä ',\n",
       " 'plat',\n",
       " 'gedrukt',\n",
       " 'Ä ',\n",
       " 'door',\n",
       " 'Ä ',\n",
       " 'traditionele',\n",
       " 'Ä ',\n",
       " 'kerkelijk',\n",
       " 'heid',\n",
       " 'Ä ',\n",
       " 'en',\n",
       " 'Ä ',\n",
       " 'bloed',\n",
       " 'de',\n",
       " 'Ä ',\n",
       " 'weg',\n",
       " 'Ä ',\n",
       " 'in',\n",
       " 'Ä ',\n",
       " 'dood',\n",
       " 'Ä ',\n",
       " 'geloof',\n",
       " '.',\n",
       " 'Ä ',\n",
       " 'Dit',\n",
       " 'Ä ',\n",
       " 'Nederland',\n",
       " 'Ä ',\n",
       " 'werd',\n",
       " 'Ä ',\n",
       " 'besp',\n",
       " 'rongen',\n",
       " 'Ä ',\n",
       " 'door',\n",
       " 'Ä ',\n",
       " 'duistere',\n",
       " 'Ä ',\n",
       " 'machten',\n",
       " '.',\n",
       " 'Ä ',\n",
       " 'Zou',\n",
       " 'Ä ',\n",
       " 'het',\n",
       " 'Ä ',\n",
       " 'Woord',\n",
       " 'Ä ',\n",
       " '(',\n",
       " 'zoals',\n",
       " 'Ä ',\n",
       " 'Barth',\n",
       " 'Ä ',\n",
       " 'het',\n",
       " 'Ä ',\n",
       " 'ver',\n",
       " 'stond',\n",
       " ')',\n",
       " 'Ä ',\n",
       " 'geen',\n",
       " 'Ä ',\n",
       " 'nieuwe',\n",
       " 'Ä ',\n",
       " 'glans',\n",
       " 'Ä ',\n",
       " 'kunnen',\n",
       " 'Ä ',\n",
       " 'leggen',\n",
       " 'Ä ',\n",
       " 'op',\n",
       " 'Ä ',\n",
       " 'het',\n",
       " 'Ä ',\n",
       " 'eigen',\n",
       " 'Ä ',\n",
       " 'leven',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'op',\n",
       " 'Ä ',\n",
       " 'het',\n",
       " 'Ä ',\n",
       " 'volk',\n",
       " ',',\n",
       " 'Ä ',\n",
       " 'waaronder',\n",
       " 'Ä ',\n",
       " 'hij',\n",
       " 'Ä ',\n",
       " 'werkte',\n",
       " '?',\n",
       " '</s>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Waarom niet met dÃ©dain over Schilder, zoals over zoveel anderen? Hij wist zich tegenstander: Barth stond tussen beiden. Maar Miskotte wist zich in dezelfde tijd te staan, in dezelfde storm, die ook Schilder onderging. Soms is er opvallende affiniteit. Hij kende de dichters van Nederland, zoals ook Schilder: DÃ¨r Mouw (Adwaita), Nijhof f en Marsman. Hij stond, denk ik, geestelijk ook aanzienlijk dichter bij hen. Hij had de Nederlandse taal en het Nederlands land lief: â€žDe nederlandse taal en het geboomte van dit land, deze twee gewassen zijn mijn aardse heerlijkheid\" (329). Al zou het allÃ©Ã©n dÃ©ze zin zijn - daarvoor is lectuur van meer dan 600 bladzijden geen te hoge prijs! Dit Nederland werd platgedrukt door traditionele kerkelijkheid en bloedde weg in dood geloof. Dit Nederland werd besprongen door duistere machten. Zou het Woord (zoals Barth het verstond) geen nieuwe glans kunnen leggen op het eigen leven, op het volk, waaronder hij werkte?').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f1cbb5",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ea8823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 10 16:28:52 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:00:05.0 Off |                  N/A |\n",
      "| 32%   29C    P8     4W / 250W |      1MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  On   | 00000000:00:06.0 Off |                  N/A |\n",
      "| 31%   28C    P8     1W / 250W |      1MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f6287",
   "metadata": {},
   "source": [
    "Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90be95c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"train_path\"] = train_path\n",
    "#os.environ[\"eval_path\"] = eval_path\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]='1'  #Makes for easier debugging (just in case)\n",
    "weights_dir = \"/home/leonardovida/dev/hist-aware/notebooks/models/bert-training-from-scratch/weights\"\n",
    "#!mkdir {weights_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "def0e291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "430cd65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1, # The vocabulary size of the token_type_ids passed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c95f72",
   "metadata": {},
   "source": [
    "Paper BERTje\n",
    "--max_predictions_per_seq=20 \\\n",
    "  --train_batch_size=256 \\\n",
    "  --eval_batch_size=32 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --num_train_steps=1000000 \\\n",
    "  --num_warmup_steps=10000 \\\n",
    "  --save_checkpoints_steps=10000 \\\n",
    "  --iterations_per_loop=10000 \\\n",
    "  --max_eval_steps=10000 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "012c91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_dir, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbe395c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec21e686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83502880"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()\n",
    "# => 84 million parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680deeb5",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e4d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DelphDataset(Dataset):\n",
    "    def __init__(self, evaluate: bool = False):\n",
    "        tokenizer = ByteLevelBPETokenizer(\n",
    "            tokenizer_dir + \"/vocab.json\",\n",
    "            tokenizer_dir + \"/merges.txt\",\n",
    "        )\n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        tokenizer.enable_truncation(max_length=512)\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        #src_files = Path(PATH_RAW_FILES).glob(\"*-eval.txt\") if evaluate else Path(PATH_RAW_FILES).glob(\"*-train.txt\")\n",
    "        src_files = Path(PATH_RAW_FILES).glob(\"*.txt\")\n",
    "        for src_file in src_files:\n",
    "            print(\"ðŸ”¥\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Weâ€™ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadec06c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-aa4b77328b62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDelphDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-f561b921f563>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, evaluate)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         tokenizer = ByteLevelBPETokenizer(\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mtokenizer_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/vocab.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mtokenizer_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/merges.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer_dir' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = DelphDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1805bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./oscar.eo.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60b69c",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34ef68c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4720c1f9f258d4c1\n",
      "Reusing dataset text (/home/leonardovida/.cache/huggingface/datasets/text/default-4720c1f9f258d4c1/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b701365fc07f4dabb4bffc7232d6a529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3869.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [202] at entry 0 and [157] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-3846ad950bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_fields'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# namedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_fields'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# namedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-NidRwJ64-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [202] at entry 0 and [157] at entry 1"
     ]
    }
   ],
   "source": [
    "files = [str(x) for x in Path(PATH_RAW_FILES).glob(\"*.txt\")]\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "dataset = load_dataset('text', data_files=files, split='train')\n",
    "\n",
    "# Load it using transformers\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=f\"{tokenizer_dir}/byte-level-BPE.tokenizer.json\")\n",
    "\n",
    "def encode(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        # TODO: padding \n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0683c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_f: str,\n",
    "                 tokenizer: PreTrainedTokenizer,\n",
    "                 batch_size: int = 16,\n",
    "                 num_workers: int = 2) -> Dict[str, DataLoader]:\n",
    "    \"\"\"Given an input file, prepare the train, test, validation dataloaders.\n",
    "       The created datasets will be preprocessed and save to disk.\n",
    "    :param dataset_f: input file\n",
    "    :param tokenizer: pretrained tokenizer that will prepare the data, i.e. convert tokens into IDs\n",
    "    :param batch_size: batch size for the dataloaders\n",
    "    :param num_workers: number of CPU workers to use during dataloading. On Windows this must be zero\n",
    "    :return: a dictionary containing train, test, validation dataloaders\n",
    "    \"\"\"\n",
    "\n",
    "    def collate(batch: List[Dict[str, Tensor]]) -> Dict[str, Tensor]:\n",
    "        \"\"\"Collates gathered items to form a batch which is then used in training, evaluation, or testing.\n",
    "        :param batch: a list of samples from the dataset. Each sample is a dictionary with keys \"input_ids\".\n",
    "        :return: the created batch with keys \"input_ids\"\n",
    "        \"\"\"\n",
    "        all_input_ids = pad_sequence([item[\"input_ids\"] for item in batch]).to(torch.long)\n",
    "\n",
    "        return {\"input_ids\": all_input_ids}\n",
    "\n",
    "    def preprocess(sentences: List[str]) -> Dict[str, Union[list, Tensor]]:\n",
    "        \"\"\"Preprocess the raw input sentences from the text file.\n",
    "        :param sentences: a list of sentences (strings)\n",
    "        :return: a dictionary of \"input_ids\"\n",
    "        \"\"\"\n",
    "        tokens = [s.split() for s in sentences]\n",
    "\n",
    "        # The sequences are not padded here. we leave that to the dataloader in collate\n",
    "        # That means: a bit slower processing, but a smaller saved dataset size\n",
    "        return tokenizer(tokens,\n",
    "                         add_special_tokens=False,\n",
    "                         return_token_type_ids=False,\n",
    "                         return_attention_mask=False\n",
    "                        )\n",
    "    \n",
    "    dataset = Dataset.from_dict({\"text\": Path(dataset_f).read_text(encoding=\"utf-8\").splitlines()})\n",
    "\n",
    "    # Split the dataset into train, test, dev\n",
    "    # 90% (train), 10% (test + validation)\n",
    "    train_testvalid = dataset.train_test_split(test_size=0.1)\n",
    "    # 10% of total (test), 10% of total (validation)\n",
    "    test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "    dataset = DatasetDict({\"train\": train_testvalid[\"train\"],\n",
    "                           \"test\": test_valid[\"test\"],\n",
    "                           \"valid\": test_valid[\"train\"]})\n",
    "\n",
    "    dataset = dataset.map(preprocess, input_columns=[\"text\"], batched=True)\n",
    "    dataset.set_format(\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "    return {partition: DataLoader(ds,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  collate_fn=collate,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True) for partition, ds in dataset.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ca4633e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ae81820bd3f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m prepare_data(dataset_f = \"/home/leonardovida/data-histaware/articles0_50000.txt\",\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     num_workers = 2)\n",
      "\u001b[0;32m<ipython-input-37-dcdbbbc957d5>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(dataset_f, tokenizer, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     48\u001b[0m                            \"valid\": test_valid[\"train\"]})\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         return DatasetDict(\n\u001b[0;32m--> 432\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    433\u001b[0m                 k: dataset.map(\n\u001b[1;32m    434\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    431\u001b[0m         return DatasetDict(\n\u001b[1;32m    432\u001b[0m             {\n\u001b[0;32m--> 433\u001b[0;31m                 k: dataset.map(\n\u001b[0m\u001b[1;32m    434\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0mtest_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mtest_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mupdate_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoes_function_return_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing finished, running the mapping function on the dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mdoes_function_return_dict\u001b[0;34m(inputs, indices)\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mfn_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             processed_inputs = (\n\u001b[0;32m-> 1378\u001b[0;31m                 \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwith_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             )\n\u001b[1;32m   1380\u001b[0m             \u001b[0mdoes_return_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-dcdbbbc957d5>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# The sequences are not padded here. we leave that to the dataloader in collate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# That means: a bit slower processing, but a smaller saved dataset size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         return tokenizer(tokens,\n\u001b[0m\u001b[1;32m     33\u001b[0m                          \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                          \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_batched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2248\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2249\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2250\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2432\u001b[0m         )\n\u001b[1;32m   2433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2434\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2435\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2436\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    376\u001b[0m         )\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "prepare_data(dataset_f = \"/home/leonardovida/data-histaware/articles0_50000.txt\",\n",
    "    tokenizer = tokenizer,\n",
    "    batch_size = 16,\n",
    "    num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8c865",
   "metadata": {},
   "source": [
    "### Create Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f4d1c",
   "metadata": {},
   "source": [
    "To make backpropagation on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e22d8d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536c01a",
   "metadata": {},
   "source": [
    "### Initalize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aff81128",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-039345378aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"home/leonardovida/data-histaware/delphBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cdad02",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./EsperBERTo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20bdc33",
   "metadata": {},
   "source": [
    "### Verify learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0f68a",
   "metadata": {},
   "source": [
    "Create a fill-mask pipeline to check whether the model learned anything useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./EsperBERTo\",\n",
    "    tokenizer=\"./EsperBERTo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask(\"La suno <mask>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0dbb1",
   "metadata": {},
   "source": [
    "### Upload the model to HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1d10e",
   "metadata": {},
   "source": [
    "Write a README.md model card and add it to the repository under `model_cards/`. Your model card should ideally include:\n",
    "* a model description,\n",
    "* training params (dataset, preprocessing, hyperparameters), \n",
    "* evaluation results,\n",
    "* intended uses & limitations\n",
    "* whatever else is helpful! ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformers-cli upload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
