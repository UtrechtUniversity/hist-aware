{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "concerned-label",
   "metadata": {},
   "source": [
    "# Pre-training DelphBERT - A tiny Transformer model based on a large newspaper dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-cartoon",
   "metadata": {},
   "source": [
    "* //TODO: Copy description of project from Github\n",
    "* //TODO: Description of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "spectacular-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Any, Dict, List, NamedTuple, Optional, Sequence, Tuple, Union\n",
    "\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import RobertaForMaskedLM\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-amsterdam",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-sally",
   "metadata": {},
   "source": [
    "First, we need to load the entire \"processed\" library of Delpher newspaper on the transformer.\n",
    "\n",
    "* // TODO: even though we are loading clean and unclean text, we are training using the **unclean** text, as Transformers want complete sentences and the clean text does not have stopwords nor lower-upper cases.\n",
    "    * Think testing the performance training on semi-clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-finding",
   "metadata": {},
   "source": [
    "### Convert from _.csv_ to _.txt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "neither-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to csv files with processed data\n",
    "#Path().parent.absolute()\n",
    "paths = [str(x) for x in Path(\"/home/leonardovida/data-histaware/1990s/\").glob(\"*.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "proved-extraction",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbafbabfe1f94ba291e55c57b3a7793f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=78.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 63533",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b478204d9ba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/home/leonardovida/data-histaware/{name}.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2059\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 63533"
     ]
    }
   ],
   "source": [
    "# Create txt files for \"processed\" data\n",
    "for path in tqdm(paths, total=len(paths)):\n",
    "    base = os.path.basename(path)\n",
    "    name = os.path.splitext(base)[0]\n",
    "    df = pd.read_csv(path)\n",
    "    df.dropna(subset=[\"text\"], inplace=True)\n",
    "    df[\"text\"].to_csv(f'/home/leonardovida/data-histaware/{name}.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pretty-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 34s, sys: 35.3 s, total: 37min 9s\n",
      "Wall time: 6min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "paths = [str(x) for x in Path(\"/home/leonardovida/data-histaware\").glob(\"*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-roberts",
   "metadata": {},
   "source": [
    "### Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "animal-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dir = \"/home/leonardovida/dev/dev/hist-aware/notebooks/models/bert-training/tokenizer\"\n",
    "#!mkdir {tokenizer_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "homeless-wireless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/leonardovida/dev/dev/hist-aware/notebooks/models/bert-training/tokenizer/vocab.json',\n",
       " '/home/leonardovida/dev/dev/hist-aware/notebooks/models/bert-training/tokenizer/merges.txt']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-balloon",
   "metadata": {},
   "source": [
    "### Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "nearby-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    tokenizer_dir + \"/vocab.json\",\n",
    "    tokenizer_dir + \"/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "elegant-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-conflict",
   "metadata": {},
   "source": [
    "Test tokenizer performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "vocal-indiana",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Waarom',\n",
       " 'Ġ',\n",
       " 'niet',\n",
       " 'Ġ',\n",
       " 'met',\n",
       " 'Ġ',\n",
       " 'd',\n",
       " 'Ã',\n",
       " '©',\n",
       " 'da',\n",
       " 'in',\n",
       " 'Ġ',\n",
       " 'over',\n",
       " 'Ġ',\n",
       " 'Schilder',\n",
       " ',',\n",
       " 'Ġ',\n",
       " 'zoals',\n",
       " 'Ġ',\n",
       " 'over',\n",
       " 'Ġ',\n",
       " 'zoveel',\n",
       " 'Ġ',\n",
       " 'anderen',\n",
       " '?',\n",
       " 'Ġ',\n",
       " 'Hij',\n",
       " 'Ġ',\n",
       " 'wist',\n",
       " 'Ġ',\n",
       " 'zich',\n",
       " 'Ġ',\n",
       " 'tegenstander',\n",
       " ':',\n",
       " 'Ġ',\n",
       " 'Bart',\n",
       " 'h',\n",
       " 'Ġ',\n",
       " 'stond',\n",
       " 'Ġ',\n",
       " 'tussen',\n",
       " 'Ġ',\n",
       " 'beiden',\n",
       " '.',\n",
       " 'Ġ',\n",
       " 'Maar',\n",
       " 'Ġ',\n",
       " 'Mis',\n",
       " 'k',\n",
       " 'otte',\n",
       " 'Ġ',\n",
       " 'wist',\n",
       " 'Ġ',\n",
       " 'zich',\n",
       " 'Ġ',\n",
       " 'in',\n",
       " 'Ġ',\n",
       " 'dezelfde',\n",
       " 'Ġ',\n",
       " 'tijd',\n",
       " 'Ġ',\n",
       " 'te',\n",
       " 'Ġ',\n",
       " 'staan',\n",
       " ',',\n",
       " 'Ġ',\n",
       " 'in',\n",
       " 'Ġ',\n",
       " 'dezelfde',\n",
       " 'Ġ',\n",
       " 'storm',\n",
       " ',',\n",
       " 'Ġ',\n",
       " 'die',\n",
       " 'Ġ',\n",
       " 'ook',\n",
       " 'Ġ',\n",
       " 'Schilder',\n",
       " 'Ġ',\n",
       " 'onderging',\n",
       " '.',\n",
       " 'Ġ',\n",
       " 'Soms',\n",
       " 'Ġ',\n",
       " 'is',\n",
       " 'Ġ',\n",
       " 'er',\n",
       " 'Ġ',\n",
       " 'opvallende',\n",
       " 'Ġ',\n",
       " 'affiniteit',\n",
       " '.',\n",
       " 'Ġ',\n",
       " 'Hij',\n",
       " 'Ġ',\n",
       " 'kende',\n",
       " 'Ġ',\n",
       " 'de',\n",
       " 'Ġ',\n",
       " 'dichters',\n",
       " 'Ġ',\n",
       " 'van',\n",
       " 'Ġ',\n",
       " 'Nederland',\n",
       " ',',\n",
       " 'Ġ',\n",
       " 'zoals',\n",
       " 'Ġ',\n",
       " 'ook',\n",
       " 'Ġ',\n",
       " 'Schilder',\n",
       " ':',\n",
       " 'Ġ',\n",
       " 'D',\n",
       " 'Ã',\n",
       " '¨',\n",
       " 'r',\n",
       " 'Ġ',\n",
       " 'M',\n",
       " 'ouw',\n",
       " 'Ġ',\n",
       " '(',\n",
       " 'Ad',\n",
       " 'wa',\n",
       " 'ita',\n",
       " '),',\n",
       " 'Ġ',\n",
       " 'Nij',\n",
       " 'hof',\n",
       " 'Ġ',\n",
       " 'f',\n",
       " 'Ġ',\n",
       " 'en',\n",
       " 'Ġ',\n",
       " 'Mar',\n",
       " 'sman',\n",
       " '.',\n",
       " 'Ġ',\n",
       " 'Hij',\n",
       " 'Ġ',\n",
       " 'stond',\n",
       " ',',\n",
       " 'Ġ',\n",
       " 'denk',\n",
       " 'Ġ',\n",
       " 'ik',\n",
       " ',',\n",
       " 'Ġ',\n",
       " 'geestelijk',\n",
       " 'Ġ',\n",
       " 'ook',\n",
       " 'Ġ',\n",
       " 'aanzienlijk',\n",
       " 'Ġ',\n",
       " 'dichter',\n",
       " 'Ġ',\n",
       " 'bij',\n",
       " 'Ġ',\n",
       " 'hen',\n",
       " '.',\n",
       " 'Ġ',\n",
       " 'Hij',\n",
       " 'Ġ',\n",
       " 'had',\n",
       " 'Ġ',\n",
       " 'de',\n",
       " 'Ġ',\n",
       " 'Nederlandse',\n",
       " 'Ġ',\n",
       " 'taal',\n",
       " 'Ġ',\n",
       " 'en',\n",
       " 'Ġ',\n",
       " 'het',\n",
       " 'Ġ',\n",
       " 'Nederlands',\n",
       " 'Ġ',\n",
       " 'land',\n",
       " 'Ġ',\n",
       " 'lief',\n",
       " ':',\n",
       " 'Ġ',\n",
       " 'â',\n",
       " 'Ģ',\n",
       " 'ŀ',\n",
       " 'De',\n",
       " 'Ġ',\n",
       " 'nederlandse',\n",
       " 'Ġ',\n",
       " 'taal',\n",
       " 'Ġ',\n",
       " 'en',\n",
       " 'Ġ',\n",
       " 'het',\n",
       " 'Ġ',\n",
       " 'ge',\n",
       " 'boom',\n",
       " 'te',\n",
       " 'Ġ',\n",
       " 'van',\n",
       " 'Ġ',\n",
       " 'dit',\n",
       " 'Ġ',\n",
       " 'land',\n",
       " ',',\n",
       " 'Ġ',\n",
       " 'deze',\n",
       " 'Ġ',\n",
       " 'twee',\n",
       " 'Ġ',\n",
       " 'gewassen',\n",
       " 'Ġ',\n",
       " 'zijn',\n",
       " 'Ġ',\n",
       " 'mijn',\n",
       " 'Ġ',\n",
       " 'aardse',\n",
       " 'Ġ',\n",
       " 'heerlijkheid',\n",
       " '\"',\n",
       " 'Ġ',\n",
       " '(',\n",
       " '329',\n",
       " ').',\n",
       " 'Ġ',\n",
       " 'Al',\n",
       " 'Ġ',\n",
       " 'zou',\n",
       " 'Ġ',\n",
       " 'het',\n",
       " 'Ġ',\n",
       " 'all',\n",
       " 'Ã',\n",
       " '©',\n",
       " 'Ã',\n",
       " '©',\n",
       " 'n',\n",
       " 'Ġ',\n",
       " 'd',\n",
       " 'Ã',\n",
       " '©',\n",
       " 'ze',\n",
       " 'Ġ',\n",
       " 'zin',\n",
       " 'Ġ',\n",
       " 'zijn',\n",
       " 'Ġ',\n",
       " '-',\n",
       " 'Ġ',\n",
       " 'daarvoor',\n",
       " 'Ġ',\n",
       " 'is',\n",
       " 'Ġ',\n",
       " 'lectuur',\n",
       " 'Ġ',\n",
       " 'van',\n",
       " 'Ġ',\n",
       " 'meer',\n",
       " 'Ġ',\n",
       " 'dan',\n",
       " 'Ġ',\n",
       " '600',\n",
       " 'Ġ',\n",
       " 'bladzijden',\n",
       " 'Ġ',\n",
       " 'geen',\n",
       " 'Ġ',\n",
       " 'te',\n",
       " 'Ġ',\n",
       " 'hoge',\n",
       " 'Ġ',\n",
       " 'prijs',\n",
       " '!',\n",
       " 'Ġ',\n",
       " 'Dit',\n",
       " 'Ġ',\n",
       " 'Nederland',\n",
       " 'Ġ',\n",
       " 'werd',\n",
       " 'Ġ',\n",
       " 'plat',\n",
       " 'gedrukt',\n",
       " 'Ġ',\n",
       " 'door',\n",
       " 'Ġ',\n",
       " 'traditionele',\n",
       " 'Ġ',\n",
       " 'kerkelijk',\n",
       " 'heid',\n",
       " 'Ġ',\n",
       " 'en',\n",
       " 'Ġ',\n",
       " 'bloed',\n",
       " 'de',\n",
       " 'Ġ',\n",
       " 'weg',\n",
       " 'Ġ',\n",
       " 'in',\n",
       " 'Ġ',\n",
       " 'dood',\n",
       " 'Ġ',\n",
       " 'geloof',\n",
       " '.',\n",
       " 'Ġ',\n",
       " 'Dit',\n",
       " 'Ġ',\n",
       " 'Nederland',\n",
       " 'Ġ',\n",
       " 'werd',\n",
       " 'Ġ',\n",
       " 'be',\n",
       " 'sprongen',\n",
       " 'Ġ',\n",
       " 'door',\n",
       " 'Ġ',\n",
       " 'duistere',\n",
       " 'Ġ',\n",
       " 'machten',\n",
       " '.',\n",
       " 'Ġ',\n",
       " 'Zou',\n",
       " 'Ġ',\n",
       " 'het',\n",
       " 'Ġ',\n",
       " 'Woord',\n",
       " 'Ġ',\n",
       " '(',\n",
       " 'zoals',\n",
       " 'Ġ',\n",
       " 'Bart',\n",
       " 'h',\n",
       " 'Ġ',\n",
       " 'het',\n",
       " 'Ġ',\n",
       " 'ver',\n",
       " 'stond',\n",
       " ')',\n",
       " 'Ġ',\n",
       " 'geen',\n",
       " 'Ġ',\n",
       " 'nieuwe',\n",
       " 'Ġ',\n",
       " 'glans',\n",
       " 'Ġ',\n",
       " 'kunnen',\n",
       " 'Ġ',\n",
       " 'leggen',\n",
       " 'Ġ',\n",
       " 'op',\n",
       " 'Ġ',\n",
       " 'het',\n",
       " 'Ġ',\n",
       " 'eigen',\n",
       " 'Ġ',\n",
       " 'leven',\n",
       " ',',\n",
       " 'Ġ',\n",
       " 'op',\n",
       " 'Ġ',\n",
       " 'het',\n",
       " 'Ġ',\n",
       " 'volk',\n",
       " ',',\n",
       " 'Ġ',\n",
       " 'waaronder',\n",
       " 'Ġ',\n",
       " 'hij',\n",
       " 'Ġ',\n",
       " 'werkte',\n",
       " '?',\n",
       " '</s>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Waarom niet met dédain over Schilder, zoals over zoveel anderen? Hij wist zich tegenstander: Barth stond tussen beiden. Maar Miskotte wist zich in dezelfde tijd te staan, in dezelfde storm, die ook Schilder onderging. Soms is er opvallende affiniteit. Hij kende de dichters van Nederland, zoals ook Schilder: Dèr Mouw (Adwaita), Nijhof f en Marsman. Hij stond, denk ik, geestelijk ook aanzienlijk dichter bij hen. Hij had de Nederlandse taal en het Nederlands land lief: „De nederlandse taal en het geboomte van dit land, deze twee gewassen zijn mijn aardse heerlijkheid\" (329). Al zou het alléén déze zin zijn - daarvoor is lectuur van meer dan 600 bladzijden geen te hoge prijs! Dit Nederland werd platgedrukt door traditionele kerkelijkheid en bloedde weg in dood geloof. Dit Nederland werd besprongen door duistere machten. Zou het Woord (zoals Barth het verstond) geen nieuwe glans kunnen leggen op het eigen leven, op het volk, waaronder hij werkte?').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-clothing",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "geological-violation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 24 12:34:26 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:00:05.0 Off |                  N/A |\n",
      "| 30%   30C    P8    11W / 250W |      1MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  On   | 00000000:00:06.0 Off |                  N/A |\n",
      "| 30%   31C    P8    11W / 250W |      1MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-pastor",
   "metadata": {},
   "source": [
    "Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "official-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"train_path\"] = train_path\n",
    "#os.environ[\"eval_path\"] = eval_path\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]='1'  #Makes for easier debugging (just in case)\n",
    "weights_dir = \"/home/leonardovida/dev/dev/hist-aware/notebooks/models/bert-training/weights\"\n",
    "#!mkdir {weights_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "virtual-simple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "satisfactory-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6, \n",
    "    type_vocab_size=1, # The vocabulary size of the token_type_ids passed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-teddy",
   "metadata": {},
   "source": [
    "Paper BERTje\n",
    "--max_predictions_per_seq=20 \\\n",
    "  --train_batch_size=256 \\\n",
    "  --eval_batch_size=32 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --num_train_steps=1000000 \\\n",
    "  --num_warmup_steps=10000 \\\n",
    "  --save_checkpoints_steps=10000 \\\n",
    "  --iterations_per_loop=10000 \\\n",
    "  --max_eval_steps=10000 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "honest-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_dir, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "freelance-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "recreational-choir",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83504416"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()\n",
    "# => 84 million parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-modeling",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./oscar.eo.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "joined-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_f: str,\n",
    "                 tokenizer: PreTrainedTokenizer,\n",
    "                 batch_size: int = 16,\n",
    "                 num_workers: int = 2) -> Dict[str, DataLoader]:\n",
    "    \"\"\"Given an input file, prepare the train, test, validation dataloaders.\n",
    "       The created datasets will be preprocessed and save to disk.\n",
    "    :param dataset_f: input file\n",
    "    :param tokenizer: pretrained tokenizer that will prepare the data, i.e. convert tokens into IDs\n",
    "    :param batch_size: batch size for the dataloaders\n",
    "    :param num_workers: number of CPU workers to use during dataloading. On Windows this must be zero\n",
    "    :return: a dictionary containing train, test, validation dataloaders\n",
    "    \"\"\"\n",
    "\n",
    "    def collate(batch: List[Dict[str, Tensor]]) -> Dict[str, Tensor]:\n",
    "        \"\"\"Collates gathered items to form a batch which is then used in training, evaluation, or testing.\n",
    "        :param batch: a list of samples from the dataset. Each sample is a dictionary with keys \"input_ids\".\n",
    "        :return: the created batch with keys \"input_ids\"\n",
    "        \"\"\"\n",
    "        all_input_ids = pad_sequence([item[\"input_ids\"] for item in batch]).to(torch.long)\n",
    "\n",
    "        return {\"input_ids\": all_input_ids}\n",
    "\n",
    "    def preprocess(sentences: List[str]) -> Dict[str, Union[list, Tensor]]:\n",
    "        \"\"\"Preprocess the raw input sentences from the text file.\n",
    "        :param sentences: a list of sentences (strings)\n",
    "        :return: a dictionary of \"input_ids\"\n",
    "        \"\"\"\n",
    "        tokens = [s.split() for s in sentences]\n",
    "\n",
    "        # The sequences are not padded here. we leave that to the dataloader in collate\n",
    "        # That means: a bit slower processing, but a smaller saved dataset size\n",
    "        return tokenizer(tokens,\n",
    "                         add_special_tokens=False,\n",
    "                         return_token_type_ids=False,\n",
    "                         return_attention_mask=False\n",
    "                        )\n",
    "    \n",
    "    dataset = Dataset.from_dict({\"text\": Path(dataset_f).read_text(encoding=\"utf-8\").splitlines()})\n",
    "\n",
    "    # Split the dataset into train, test, dev\n",
    "    # 90% (train), 10% (test + validation)\n",
    "    train_testvalid = dataset.train_test_split(test_size=0.1)\n",
    "    # 10% of total (test), 10% of total (validation)\n",
    "    test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "    dataset = DatasetDict({\"train\": train_testvalid[\"train\"],\n",
    "                           \"test\": test_valid[\"test\"],\n",
    "                           \"valid\": test_valid[\"train\"]})\n",
    "\n",
    "    dataset = dataset.map(preprocess, input_columns=[\"text\"], batched=True)\n",
    "    dataset.set_format(\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "    return {partition: DataLoader(ds,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  collate_fn=collate,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True) for partition, ds in dataset.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "helpful-bangkok",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ae81820bd3f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m prepare_data(dataset_f = \"/home/leonardovida/data-histaware/articles0_50000.txt\",\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     num_workers = 2)\n",
      "\u001b[0;32m<ipython-input-37-dcdbbbc957d5>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(dataset_f, tokenizer, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     48\u001b[0m                            \"valid\": test_valid[\"train\"]})\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         return DatasetDict(\n\u001b[0;32m--> 432\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    433\u001b[0m                 k: dataset.map(\n\u001b[1;32m    434\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    431\u001b[0m         return DatasetDict(\n\u001b[1;32m    432\u001b[0m             {\n\u001b[0;32m--> 433\u001b[0;31m                 k: dataset.map(\n\u001b[0m\u001b[1;32m    434\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0mtest_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mtest_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mupdate_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoes_function_return_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing finished, running the mapping function on the dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mdoes_function_return_dict\u001b[0;34m(inputs, indices)\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mfn_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             processed_inputs = (\n\u001b[0;32m-> 1378\u001b[0;31m                 \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwith_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m             )\n\u001b[1;32m   1380\u001b[0m             \u001b[0mdoes_return_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-dcdbbbc957d5>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# The sequences are not padded here. we leave that to the dataloader in collate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# That means: a bit slower processing, but a smaller saved dataset size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         return tokenizer(tokens,\n\u001b[0m\u001b[1;32m     33\u001b[0m                          \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                          \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_batched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2248\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2249\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2250\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2432\u001b[0m         )\n\u001b[1;32m   2433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2434\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2435\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2436\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/histaware-YEVqYXZ--py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    376\u001b[0m         )\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "prepare_data(dataset_f = \"/home/leonardovida/data-histaware/articles0_50000.txt\",\n",
    "    tokenizer = tokenizer,\n",
    "    batch_size = 16,\n",
    "    num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-player",
   "metadata": {},
   "source": [
    "### Create Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-review",
   "metadata": {},
   "source": [
    "To make backpropagation on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-lunch",
   "metadata": {},
   "source": [
    "### Initalize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"home/leonardovida/data-histaware/delphBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-given",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./EsperBERTo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-things",
   "metadata": {},
   "source": [
    "### Verify learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-henry",
   "metadata": {},
   "source": [
    "Create a fill-mask pipeline to check whether the model learned anything useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./EsperBERTo\",\n",
    "    tokenizer=\"./EsperBERTo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask(\"La suno <mask>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-exhaust",
   "metadata": {},
   "source": [
    "### Upload the model to HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-irish",
   "metadata": {},
   "source": [
    "Write a README.md model card and add it to the repository under `model_cards/`. Your model card should ideally include:\n",
    "* a model description,\n",
    "* training params (dataset, preprocessing, hyperparameters), \n",
    "* evaluation results,\n",
    "* intended uses & limitations\n",
    "* whatever else is helpful! 🤓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformers-cli upload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histaware-YEVqYXZ--py3.8",
   "language": "python",
   "name": "histaware-yevqyxz--py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
