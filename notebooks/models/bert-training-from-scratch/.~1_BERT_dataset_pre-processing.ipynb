{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56a8f58",
   "metadata": {},
   "source": [
    "# Pre-training DelphBERT - A tiny Transformer model based on a large newspaper dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f1d1f",
   "metadata": {},
   "source": [
    "* //TODO: Copy description of project from Github\n",
    "* //TODO: Description of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf53f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Any, Dict, List, NamedTuple, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import BertTokenizer\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import RobertaForMaskedLM\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb440b3",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49449126",
   "metadata": {},
   "source": [
    "First, we need to load the entire \"processed\" library of Delpher newspaper on the transformer.\n",
    "\n",
    "* // TODO: even though we are loading clean and unclean text, we are training using the **unclean** text, as Transformers want complete sentences and the clean text does not have stopwords nor lower-upper cases.\n",
    "    * Think testing the performance training on semi-clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f42000",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_FILES = \"/home/leonardovida/data-histaware/raw/raw_merged/\"\n",
    "PATH_TOKENIZER_DIR = \"/home/leonardovida/dev/hist-aware/notebooks/models/bert-training-from-scratch/tokenizer\"\n",
    "PATH_DATASET_DIR = \"/home/leonardovida/dev/hist-aware/notebooks/models/bert-training-from-scratch/dataset\"\n",
    "\n",
    "# For tokenizer\n",
    "PRE_TRAINED_MODEL_NAME = 'wietsedv/bert-base-dutch-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#!mkdir PATH_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d0587",
   "metadata": {},
   "source": [
    "## Load data into Dataset directly from .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "813a8b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1765ec5baa25fcb6\n",
      "Reusing dataset csv (/home/leonardovida/.cache/huggingface/datasets/csv/default-1765ec5baa25fcb6/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "csv_paths = [str(x) for x in Path(PATH_RAW_FILES).glob(\"*.csv\")]\n",
    "dataset = datasets.load_dataset(\n",
    "    \"csv\",\n",
    "    data_files = csv_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6524f50",
   "metadata": {},
   "source": [
    "Remove columns that are not necessary in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0550120",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['Unnamed: 0', 'article_name', 'date', 'index_article',\n",
    "                                  'article_filepath', 'dir', 'title', 'access_rights',\n",
    "                                  'identifier', 'metadata_title', 'index_metadata', 'metadata_filepath',\n",
    "                                  'newspaper_title', 'newspaper_date', 'newspaper_publisher', 'newspaper_source',\n",
    "                                  'newspaper_volume', 'newspaper_issuenumber', 'newspaper_recordIdentifier',\n",
    "                                  'transformedRecordIdentifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4629394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/leonardovida/.cache/huggingface/datasets/csv/default-1765ec5baa25fcb6/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-8a85db9cb5d9cb6e.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda article: article['subject'] == \"artikel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c31f92",
   "metadata": {},
   "source": [
    "### Look at one example\n",
    "\n",
    "Always visually check to understand if dataset is actually what you expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a31c07d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\'TIMMERMAN Arme Feenstra hat it op it heden moai drok. Dy drokte hat syn moaije kanten, mar ek syn listige. Men kin troch de drokte de klanten net sa gau helpe as men wol woe en .de klanten wurde wolris hwat üngedurich en komme to freegjen: „Hwa.nnear sil it nou wêze? Komme jo nou hast ek ris?\" Frou Heerschop fan Tsjalhuzum forwachtte de timmerman ek al in skoft. Okkerwyks lei der by him in brief fan har troch de bus. Dêr stie yn: Mattheüs 11:3. De famylje Feenstra fansels hastich de tekst yn \\\\\\'e bibel opsykje en hwat stie dêr: „Zijt gij het, die komen zou, of hebben wij een ander te verwachten?\\\\\\'\\\\\\' Der is in brief werom gien mei in sitaet fan Don Herold: „Werken is het mooiste wat er is. \\\\\\'Laten we dus altijd zorgen, dat we iets voor de volgende dag overlaten. („Nijslan\" — Nijland). *\\']'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"p\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf588e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372bfa6f4a154a26812ceff32ff7b13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#0', max=465135.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f348feaa0fde471b984ee60ca0fee87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#1', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c0efd283fc4bfe996f6ae19f0ebd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#2', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a106239f3dc948a1992cef00ce169efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#3', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05823846795c4cc5a3df8d68b975d330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#4', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f23f8ca13d647e198301352ea530480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#5', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe97032899c94c0b84c27d37bbcc3b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#6', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d687a65c49e64fff83da13fdc564f74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#7', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01aa6898e4574c68bbe398525c3a1040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='#8', max=465134.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "def paragraph_clean(article):\n",
    "    \"\"\"Basic cleaning of paragraphs.\n",
    "    \n",
    "    More can and will be done in the tokenizing step.\n",
    "    \"\"\"\n",
    "    words = \" \".join([w for w in nltk.word_tokenize(article[\"p\"])])\n",
    "    words = \"\".join([re.sub(r\"[^-0-9\\w,. ?!()%/]\", r\"\", w) for w in words])\n",
    "    words = re.sub(r\"^\\s*|\\s\\s*\", \" \", words).strip()\n",
    "    return {\"p\": words}\n",
    "    \n",
    "dataset = dataset.map(paragraph_clean, num_proc=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72317b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TIMMERMAN Arme Feenstra hat it op it heden moai drok . Dy drokte hat syn moaije kanten , mar ek syn listige . Men kin troch de drokte de klanten net sa gau helpe as men wol woe en .de klanten wurde wolris hwat üngedurich