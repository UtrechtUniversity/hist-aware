{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from text_manager import TextManager\n",
    "from embedding import Word2VecEmbedding\n",
    "from lstm_model import LSTM_Model\n",
    "from cnn_model import CNN_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_DATA_DIR = '../../data/tmp'\n",
    "EMBEDDING_PATH = \"../../data/320/combined-320.txt\"\n",
    "OUTPUT_DIR = '../../data/output'\n",
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "\n",
    "data_fp = \"../../data/labeled/labeled_energy_1970_1990.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(data_fp):\n",
    "    \"\"\"Load dataset \n",
    "    \"\"\"\n",
    "\n",
    "    # read the data of the file location given as argument to this function\n",
    "    df = pd.read_csv(data_fp)\n",
    "\n",
    "    # make texts and labels\n",
    "    texts = df['text'].fillna('')\n",
    "    labels = df[\"labels\"]\n",
    "\n",
    "    return texts.values, labels.values\n",
    "\n",
    "\n",
    "def data_prep(data_fp):\n",
    "\n",
    "\n",
    "    # Dataset, labels and embedding layer are stored to disk in pickle file. \n",
    "    if not os.path.exists(TEMP_DATA_DIR):\n",
    "        os.makedirs(TEMP_DATA_DIR)\n",
    "        \n",
    "    pickle_fp = os.path.join(TEMP_DATA_DIR, 'hist_aware_pickle.pickle')\n",
    "\n",
    "    # load the dataset from disk\n",
    "    texts, lbls = load_dataset(data_fp)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get the texts and their corresponding labels\n",
    "\n",
    "    textManager = TextManager(\n",
    "        max_num_words = MAX_NUM_WORDS,\n",
    "        max_sequence_length = MAX_SEQUENCE_LENGTH\n",
    "    )\n",
    "    \n",
    "    texts = textManager.clean_text(texts)\n",
    "    print('max length of all texts', len(max(texts, key=len)))\n",
    "    \n",
    "    data, labels, word_index = textManager.sequence_maker(texts, lbls)\n",
    "\n",
    "    if not os.path.exists(TEMP_DATA_DIR):\n",
    "        os.makedirs(TEMP_DATA_DIR)\n",
    "\n",
    "    embedding = Word2VecEmbedding(word_index, MAX_NUM_WORDS,\n",
    "                                  MAX_SEQUENCE_LENGTH)\n",
    "    embedding.load_word2vec_data(EMBEDDING_PATH)\n",
    "    embedding_layer = embedding.build_embedding()\n",
    "\n",
    "    with open(pickle_fp, 'wb') as f:\n",
    "        pickle.dump((data, labels, embedding_layer), f)\n",
    "\n",
    "def train_model(model, dropout=0):\n",
    "\n",
    "        \"\"\" Read dataset, labels and embedding layer from pickle file. \"\"\"\n",
    "        pickle_fp = os.path.join(TEMP_DATA_DIR, 'hist_aware_pickle.pickle')\n",
    "\n",
    "        with open(pickle_fp, 'rb') as f:\n",
    "            data, labels, embedding_layer = pickle.load(f)\n",
    "    \n",
    "\n",
    "        \"\"\" Split dataset to train and test \"\"\"\n",
    "        x_train, x_val, y_train, y_val = train_test_split(data, labels,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=labels)\n",
    "            \n",
    "        print(\"x_train shape:\", x_train.shape, \", x_val shape:\", x_val.shape)\n",
    "        print(\"y_train shape:\", y_train.shape, \", y_val shape:\", y_val.shape)\n",
    "       \n",
    "        if model == 'lstm':\n",
    "            \"\"\" Make a lstm model \"\"\"\n",
    "            deep_model = LSTM_Model\n",
    "            args_model = {\n",
    "                'backwards': True,\n",
    "                'dropout': dropout,\n",
    "                'optimizer': 'rmsprop',\n",
    "                'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "                'embedding_layer': embedding_layer\n",
    "            }\n",
    "        elif model == 'cnn':\n",
    "            \"\"\" Make a cnn model \"\"\"\n",
    "            deep_model = CNN_Model\n",
    "            args_model = {\n",
    "                'optimizer': 'rmsprop',\n",
    "                'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "                'embedding_layer': embedding_layer\n",
    "            }\n",
    "            \n",
    "        \"\"\" Train model, calculate scores\"\"\"\n",
    "        model = deep_model(**args_model)\n",
    "        model.train(x_train, y_train, x_val, y_val)\n",
    "\n",
    "        \n",
    "\n",
    "        pred = model.predict(x_val)\n",
    "        \n",
    "         # store result in dataframe\n",
    "        df_y = pd.DataFrame({'sent_0': y_val[:,0],'sent_1': y_val[:,1],'sent_2': y_val[:,2]})\n",
    "        df_pred = pd.DataFrame({'sent_0': pred[:,0],'sent_1': pred[:,1],'sent_2': pred[:,2]})\n",
    "        \n",
    "        result_df = pd.concat([df_y.idxmax(axis=1),df_pred.idxmax(axis=1)], axis=1)\n",
    "        result_df.columns =['y_val','pred']\n",
    "        \n",
    "\n",
    "        \"\"\"Save the result to a file\"\"\"\n",
    "        \n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "                os.makedirs(OUTPUT_DIR)\n",
    "        export_path = os.path.join(OUTPUT_DIR, 'dropout{}.csv'.format(dropout))\n",
    "        result_df.to_csv(export_path,index=False)\n",
    "        df_pred.to_csv(os.path.join(OUTPUT_DIR,'preds_prob.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search():\n",
    "    # create model\n",
    "    model = Keras Classifier(build_fn=create_model, verbose=0)\n",
    "    # define the grid search parameters\n",
    "    batch_size = [10, 20, 40, 60, 80, 100]\n",
    "    epochs = [10, 50, 100]\n",
    "    param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "    grid_result = grid.fit(X, Y)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of all texts 11644\n",
      "Found 31517 unique tokens.\n",
      "Shape of data tensor: (6214, 10000)\n",
      "Shape of label tensor: (6214, 3)\n",
      "Indexing word vectors.\n",
      "Found 1442951 word vectors.\n",
      "Shape of embedding matrix:  (20000, 320)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dropout = 0.2\n",
    "data_prep(data_fp)\n",
    "#train_lstm(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4163, 10000) , x_val shape: (2051, 10000)\n",
      "y_train shape: (4163, 3) , y_val shape: (2051, 3)\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10000)]           0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 10000, 320)        6400000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10)                13240     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 6,415,035\n",
      "Trainable params: 15,035\n",
      "Non-trainable params: 6,400,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.2\n",
    "train_model('lstm',dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.0\n",
    "train_model('cnn',dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[490,  86,  41],\n",
       "       [163, 789,  97],\n",
       "       [102, 104, 179]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "export_path = os.path.join(OUTPUT_DIR, 'dropout{}.csv'.format(dropout))\n",
    "results = pd.read_csv(export_path)\n",
    "\n",
    "confusion_matrix(results['y_val'], results['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, accuracy_score\n",
    "\n",
    "def get_classification_report(y_test, preds):\n",
    "    cr = classification_report(y_test, preds , output_dict=True)\n",
    "    return pd.DataFrame(cr).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent_0</th>\n",
       "      <td>0.649007</td>\n",
       "      <td>0.794165</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>617.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_1</th>\n",
       "      <td>0.805924</td>\n",
       "      <td>0.752145</td>\n",
       "      <td>0.778107</td>\n",
       "      <td>1049.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_2</th>\n",
       "      <td>0.564669</td>\n",
       "      <td>0.464935</td>\n",
       "      <td>0.509972</td>\n",
       "      <td>385.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.710873</td>\n",
       "      <td>0.710873</td>\n",
       "      <td>0.710873</td>\n",
       "      <td>0.710873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.673200</td>\n",
       "      <td>0.670415</td>\n",
       "      <td>0.667455</td>\n",
       "      <td>2051.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.713432</td>\n",
       "      <td>0.710873</td>\n",
       "      <td>0.708575</td>\n",
       "      <td>2051.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "sent_0         0.649007  0.794165  0.714286   617.000000\n",
       "sent_1         0.805924  0.752145  0.778107  1049.000000\n",
       "sent_2         0.564669  0.464935  0.509972   385.000000\n",
       "accuracy       0.710873  0.710873  0.710873     0.710873\n",
       "macro avg      0.673200  0.670415  0.667455  2051.000000\n",
       "weighted avg   0.713432  0.710873  0.708575  2051.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_report(results['y_val'], results['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3179\n",
       "0    1868\n",
       "2    1167\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_fp)\n",
    "data.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-56c01a4706c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexport_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'preds_prob.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "export_path = os.path.join(OUTPUT_DIR, 'preds_prob.csv'.format(dropout))\n",
    "probs = pd.read_csv(export_path)\n",
    "\n",
    "probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array(['hello','world','!','Oooh gaaah booo gaah?'])\n",
    "max(a, key=len)\n",
    "len(max(a, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histaware-VpTDj7tf-py3.8",
   "language": "python",
   "name": "histaware-vptdj7tf-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
