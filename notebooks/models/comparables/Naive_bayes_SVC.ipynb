{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline for BERT: Tf-idf + Naive Bayes or SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"~/dev/hist-aware/notebooks/data/labeled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, \"labeled_energy_1970_1990.csv\"))\n",
    "oil = pd.read_csv(os.path.join(DATA_DIR, \"labeled_oil_1970_1990.csv\"))\n",
    "gas = pd.read_csv(os.path.join(DATA_DIR, \"labeled_gas_1970_1990.csv\"))\n",
    "coal = pd.read_csv(os.path.join(DATA_DIR, \"labeled_coal_1970_1990.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.text_clean.values\n",
    "y = df.labels.values\n",
    "#y = label_binarize(df.labels.values, classes=[0, 1, 2])\n",
    "\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set GPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "Device name: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"dutch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(s):\n",
    "    \"\"\"\n",
    "    - Lowercase the sentence\n",
    "    - Change \"'t\" to \"not\"\n",
    "    - Isolate and remove punctuations except \"?\"\n",
    "    - Remove other special characters\n",
    "    - Remove stop words except \"not\" and \"can\"\n",
    "    - Remove trailing whitespace\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Isolate and remove punctuations except '?'\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
    "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Remove stopwords except 'not' and 'can'\n",
    "    s = \" \".join([word for word in s.split()\n",
    "                  if word not in stopwords])\n",
    "    # Remove trailing whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess already cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.68 s, sys: 35 ms, total: 2.72 s\n",
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocess text\n",
    "X_train = np.array([text_preprocessing(text) for text in X_train])\n",
    "X_val = np.array([text_preprocessing(text) for text in X_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Pipeline: TF-IDF vectorizer and MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7256140350877193"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_mnb = Pipeline([\n",
    "    (\"tf-idf\", TfidfVectorizer()),\n",
    "    (\"clf\",  MultinomialNB())\n",
    "])\n",
    "text_clf_mnb.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf_mnb.predict(X_val)\n",
    "np.mean(predicted == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline with SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7656140350877193"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_sdg = Pipeline([\n",
    "    ('tf-idf', TfidfVectorizer(ngram_range=(1, 2), binary=True, smooth_idf=False)),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])\n",
    "text_clf_sdg.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf_sdg.predict(X_val)\n",
    "np.mean(predicted == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.70      0.77       473\n",
      "           1       0.72      0.96      0.82       719\n",
      "           2       0.83      0.30      0.45       233\n",
      "\n",
      "    accuracy                           0.77      1425\n",
      "   macro avg       0.80      0.65      0.68      1425\n",
      "weighted avg       0.79      0.77      0.74      1425\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[331, 132,  10],\n",
       "       [ 25, 689,   5],\n",
       "       [ 27, 135,  71]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val, predicted))\n",
    "metrics.confusion_matrix(y_val, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'tf-idf__use_idf': (True, False),\n",
    "    'tf-idf__norm': ('l1', 'l2', None),\n",
    "    'tf-idf__max_df': (0.5, 0.75, 1.0),\n",
    "    'tf-idf__max_features': (None, 5000, 10000, 50000),\n",
    "    'tf-idf__ngram_range': ((1, 1), (1, 2), (1,3)),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf_mnb, parameters, cv=10, n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 432 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:   35.3s\n",
      "[Parallel(n_jobs=-1)]: Done 222 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 245 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done 293 tasks      | elapsed:   48.0s\n",
      "[Parallel(n_jobs=-1)]: Done 318 tasks      | elapsed:   54.1s\n",
      "[Parallel(n_jobs=-1)]: Done 345 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 372 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 401 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 430 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 461 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 492 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 525 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 558 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 593 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 628 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 665 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 702 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 741 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 780 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 821 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 862 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 905 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 948 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 993 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1038 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1085 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1132 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1181 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1230 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1281 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1332 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1385 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1438 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1493 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1548 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1605 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1662 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1721 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1780 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1841 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1902 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1965 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2028 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2093 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2158 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2225 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2292 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2361 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2430 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2501 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2572 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2645 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2718 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2793 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2868 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2945 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3022 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3101 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3180 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3261 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3342 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3425 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3508 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3593 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3678 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3765 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3852 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3941 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4030 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4121 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4212 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4320 out of 4320 | elapsed: 12.9min finished\n"
     ]
    }
   ],
   "source": [
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 1e-06\n",
      "tf-idf__max_df: 0.5\n",
      "tf-idf__max_features: None\n",
      "tf-idf__ngram_range: (1, 3)\n",
      "tf-idf__norm: 'l1'\n",
      "tf-idf__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "gs_clf.best_score_\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8336842105263158"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_sdg = Pipeline([\n",
    "    (\"tf-idf\", TfidfVectorizer(max_df = 0.5, max_features=None, ngram_range = (1, 3), norm = 'l1', use_idf = True)),\n",
    "    (\"clf\",  MultinomialNB(alpha = 1e-06))\n",
    "])\n",
    "text_clf_sdg.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf_sdg.predict(X_val)\n",
    "np.mean(predicted == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leonardovida/hist-aware/data/to_label/1980s_olie_to_label_.csv\n",
      "/home/leonardovida/hist-aware/data/to_label/1980s_kool_to_label.csv\n",
      "/home/leonardovida/hist-aware/data/to_label/1990s_olie_to_label.csv\n",
      "/home/leonardovida/hist-aware/data/to_label/1980s_gas_to_label.csv\n",
      "/home/leonardovida/hist-aware/data/to_label/1990s_kool_to_label.csv\n",
      "/home/leonardovida/hist-aware/data/to_label/1970s_kool_to_label.csv\n",
      "/home/leonardovida/hist-aware/data/to_label/1990s_gas_to_label.csv\n",
      "/home/leonardovida/hist-aware/data/to_label/1970s_olie_to_label.csv\n",
      "/home/leonardovida/hist-aware/data/to_label/1970s_gas_to_label.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for file in os.listdir(\"/home/leonardovida/hist-aware/data/to_label\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(\"/home/leonardovida/hist-aware/data/to_label\", file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df[\"text_clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histaware-JJpORNNs-py3.8",
   "language": "python",
   "name": "histaware-jjpornns-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
